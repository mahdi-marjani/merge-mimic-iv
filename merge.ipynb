{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Imports & helper utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print(\"Ready — imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load admissions.csv (or use sample if file not present) and expand to one row per day (day_index)\n",
    "csv_path = Path(\"admissions.csv\")\n",
    "\n",
    "admissions = pd.read_csv(csv_path, low_memory=False,\n",
    "                            parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "print(\"Loaded admissions.csv from disk. Rows:\", len(admissions))\n",
    "\n",
    "# Ensure datetime types\n",
    "admissions['admittime'] = pd.to_datetime(admissions['admittime'], errors='coerce')\n",
    "admissions['dischtime'] = pd.to_datetime(admissions['dischtime'], errors='coerce')\n",
    "\n",
    "# If dischtime missing, fill with admittime (so at least one day is produced)\n",
    "admissions['dischtime'] = admissions['dischtime'].fillna(admissions['admittime'])\n",
    "\n",
    "# Function to expand a single admission row into day rows\n",
    "def expand_admission_row(row):\n",
    "    adm_date = row['admittime'].normalize().date()\n",
    "    dis_date = row['dischtime'].normalize().date()\n",
    "    n_days = (dis_date - adm_date).days + 1\n",
    "    if n_days <= 0:\n",
    "        n_days = 1\n",
    "    rows = []\n",
    "    for d in range(n_days):\n",
    "        new = {\n",
    "            'subject_id': row['subject_id'],\n",
    "            'hadm_id': row['hadm_id'],\n",
    "            'day_index': int(d),\n",
    "            'admittime': row['admittime'],\n",
    "            'dischtime': row['dischtime'],\n",
    "            'deathtime': row['deathtime'],\n",
    "            'admission_type': row.get('admission_type', np.nan),\n",
    "            'admit_provider_id': row.get('admit_provider_id', np.nan),\n",
    "            'admission_location': row.get('admission_location', np.nan),\n",
    "            'discharge_location': row.get('discharge_location', np.nan),\n",
    "            'insurance': row.get('insurance', np.nan),\n",
    "            'language': row.get('language', np.nan),\n",
    "            'marital_status': row.get('marital_status', np.nan),\n",
    "            'race': row.get('race', np.nan),\n",
    "            'edregtime': row.get('edregtime', pd.NaT),\n",
    "            'edouttime': row.get('edouttime', pd.NaT),\n",
    "            'hospital_expire_flag': row.get('hospital_expire_flag', np.nan)\n",
    "        }\n",
    "        rows.append(new)\n",
    "    return rows\n",
    "\n",
    "# Expand all admissions\n",
    "expanded = []\n",
    "for _, r in admissions.iterrows():\n",
    "    expanded.extend(expand_admission_row(r))\n",
    "\n",
    "merged_initial = pd.DataFrame(expanded)\n",
    "# Convert types\n",
    "merged_initial[['subject_id','hadm_id','day_index']] = merged_initial[['subject_id','hadm_id','day_index']].astype('Int64')\n",
    "\n",
    "print(\"Expanded admissions -> rows:\", merged_initial.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv(\"admissions_expanded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Attach icustays -> mark ICU presence per (subject_id, hadm_id, day_index)\n",
    "# - Preserves merged_initial unchanged\n",
    "# - Produces merged_with_icu with ICU columns filled when row_date falls inside an icu stay\n",
    "\n",
    "icu_path = Path(\"icustays.csv\")\n",
    "if not icu_path.exists():\n",
    "    raise FileNotFoundError(f\"icustays.csv not found at {icu_path.resolve()}  -- put the file next to admissions.csv\")\n",
    "\n",
    "# load icustays (parse datetimes)\n",
    "icustays = pd.read_csv(icu_path, low_memory=False, parse_dates=['intime','outtime'])\n",
    "\n",
    "# normalize / required columns (tolerant if some optional cols missing)\n",
    "for col in ['subject_id','hadm_id','intime','outtime']:\n",
    "    if col not in icustays.columns:\n",
    "        raise KeyError(f\"Expected column '{col}' in icustays.csv but it is missing.\")\n",
    "\n",
    "# optional helpful columns: stay_id, first_careunit, last_careunit, los\n",
    "optional_cols = ['stay_id','first_careunit','last_careunit','los']\n",
    "for c in optional_cols:\n",
    "    if c not in icustays.columns:\n",
    "        icustays[c] = pd.NA  # create if missing so downstream code is simpler\n",
    "\n",
    "# --- prepare merged copy (do not modify merged_initial in-place) ---\n",
    "merged_with_icu = merged_initial.copy().reset_index(drop=False).rename(columns={'index':'row_id'})\n",
    "# add ICU output columns (keeps original names minimal)\n",
    "for col in ['stay_id_icu','icustay_intime','icustay_outtime','first_careunit_icu','last_careunit_icu','los_icu']:\n",
    "    if col not in merged_with_icu.columns:\n",
    "        merged_with_icu[col] = pd.NA\n",
    "\n",
    "# compute the date of each merged row (admission day + day_index)\n",
    "# assume admittime exists and day_index present\n",
    "if 'admittime' not in merged_with_icu.columns:\n",
    "    raise KeyError(\"merged_initial must contain 'admittime' column\")\n",
    "\n",
    "# ensure admittime is datetime\n",
    "merged_with_icu['admittime'] = pd.to_datetime(merged_with_icu['admittime'], errors='coerce')\n",
    "merged_with_icu['day_index_int'] = merged_with_icu['day_index'].fillna(0).astype(int)\n",
    "merged_with_icu['row_date'] = merged_with_icu['admittime'].dt.normalize() + pd.to_timedelta(merged_with_icu['day_index_int'], unit='D')\n",
    "\n",
    "# normalize icu intime/outtime to dates (fill missing outtime with intime)\n",
    "icustays['intime'] = pd.to_datetime(icustays['intime'], errors='coerce')\n",
    "icustays['outtime'] = pd.to_datetime(icustays['outtime'], errors='coerce').fillna(icustays['intime'])\n",
    "icustays['intime_norm'] = icustays['intime'].dt.normalize()\n",
    "icustays['outtime_norm'] = icustays['outtime'].dt.normalize()\n",
    "\n",
    "# create a merged candidate set joining by subject_id & hadm_id (many-to-many)\n",
    "# keep the icu's key cols for matching and assignment\n",
    "icu_keep = ['subject_id','hadm_id','stay_id','intime','outtime','intime_norm','outtime_norm','first_careunit','last_careunit','los']\n",
    "candidate = merged_with_icu.merge(icustays[icu_keep], on=['subject_id','hadm_id'], how='left', suffixes=('','_icu'))\n",
    "\n",
    "# mark rows where row_date is within icu interval\n",
    "mask_in_icu = (candidate['row_date'] >= candidate['intime_norm']) & (candidate['row_date'] <= candidate['outtime_norm'])\n",
    "candidate['in_icu'] = mask_in_icu.fillna(False)\n",
    "\n",
    "# For rows that match multiple ICU stays, pick the earliest ICU (by intime)\n",
    "# keep only matches; then for duplicates keep the one with smallest intime\n",
    "matched = candidate[candidate['in_icu']].copy()\n",
    "if not matched.empty:\n",
    "    # sort so earliest ICU intime appears first for each original row\n",
    "    matched = matched.sort_values(by=['row_id','intime'])\n",
    "    # pick first match per row_id\n",
    "    first_matches = matched.groupby('row_id', as_index=False).first()\n",
    "    # map the ICU fields back into merged_with_icu by row_id\n",
    "    map_cols = {\n",
    "        'stay_id':'stay_id_icu',\n",
    "        'intime':'icustay_intime',\n",
    "        'outtime':'icustay_outtime',\n",
    "        'first_careunit':'first_careunit_icu',\n",
    "        'last_careunit':'last_careunit_icu',\n",
    "        'los':'los_icu'\n",
    "    }\n",
    "    for src, dst in map_cols.items():\n",
    "        # build mapping series\n",
    "        mapping = first_matches.set_index('row_id')[src]\n",
    "        merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), dst] = merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), 'row_id'].map(mapping)\n",
    "    assigned_count = len(first_matches)\n",
    "else:\n",
    "    assigned_count = 0\n",
    "\n",
    "# cleanup helper columns\n",
    "merged_with_icu = merged_with_icu.drop(columns=['day_index_int','row_date'])\n",
    "\n",
    "print(f\"ICU assignment complete. Rows where ICU info filled: {int(assigned_count)}\")\n",
    "# preview some assigned rows (if any)\n",
    "print(merged_with_icu[merged_with_icu['stay_id_icu'].notna()].head(20))\n",
    "\n",
    "# merged_with_icu is ready for next steps (icustays merged)\n",
    "# note: merged_initial remains unchanged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- فقط تغییر اسم و ترتیب ستون‌های ICU ---\n",
    "\n",
    "# تغییر نام ستون‌ها\n",
    "rename_map = {\n",
    "    'stay_id_icu': 'stay_id',\n",
    "    'first_careunit_icu': 'first_careunit',\n",
    "    'last_careunit_icu': 'last_careunit',\n",
    "    'icustay_intime': 'icustays_intime',\n",
    "    'icustay_outtime': 'icustays_outtime',\n",
    "    'los_icu': 'los'\n",
    "}\n",
    "merged_with_icu = merged_with_icu.rename(columns=rename_map)\n",
    "\n",
    "# جابجایی ترتیب ستون‌های ICU\n",
    "icu_cols_ordered = ['stay_id', 'first_careunit', 'last_careunit', 'icustays_intime', 'icustays_outtime', 'los']\n",
    "\n",
    "# بقیه ستون‌ها (به جز ICU)\n",
    "other_cols = [c for c in merged_with_icu.columns if c not in icu_cols_ordered]\n",
    "\n",
    "# بازآرایی نهایی\n",
    "merged_with_icu = merged_with_icu[other_cols + icu_cols_ordered]\n",
    "\n",
    "print(\"Renaming & reordering complete.\")\n",
    "print(merged_with_icu[icu_cols_ordered].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06434530",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e880f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f318e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv('merged_initial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.to_csv('merged_with_icu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.head(100).to_csv('merged_with_icu_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell X: merge all_vanco.csv into merged_with_icu (or merged_initial if icu-step not present)\n",
    "vanco_path = Path(\"all_vanco.csv\")\n",
    "if not vanco_path.exists():\n",
    "    raise FileNotFoundError(f\"all_vanco.csv not found at {vanco_path.resolve()}\")\n",
    "\n",
    "# load and parse\n",
    "all_vanco = pd.read_csv(vanco_path, low_memory=False, parse_dates=['charttime'])\n",
    "\n",
    "# normalize ids to integers (some hadm_id may have .0)\n",
    "all_vanco['subject_id'] = pd.to_numeric(all_vanco['subject_id'], errors='coerce').astype('Int64')\n",
    "all_vanco['hadm_id'] = pd.to_numeric(all_vanco['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "# helper: resolve numeric value for comparison\n",
    "def resolve_numeric(row):\n",
    "    v = row.get('value')\n",
    "    vn = row.get('valuenum')\n",
    "    # treat missing-like tokens as missing\n",
    "    if pd.isna(v) or str(v).strip() in ['', '___', 'NaN', 'nan']:\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    # try parse value (may contain commas, spaces)\n",
    "    s = str(v).strip().replace(',', '')\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        # fallback to valuenum\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "all_vanco['resolved_val'] = all_vanco.apply(resolve_numeric, axis=1)\n",
    "\n",
    "# Choose base merged DF to join into (prefer merged_with_icu if exists)\n",
    "\n",
    "\n",
    "# ensure admittime exists and is datetime\n",
    "merged_with_vanco = merged_with_icu.copy()\n",
    "if 'admittime' not in merged_with_vanco.columns:\n",
    "    raise KeyError(f\"merged_with_vanco must contain 'admittime' column before merging labs.\")\n",
    "merged_with_vanco['admittime'] = pd.to_datetime(merged_with_vanco['admittime'], errors='coerce')\n",
    "\n",
    "# build admit lookup (one admittime per subject_id,hadm_id)\n",
    "admit_map = merged_with_vanco.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time']).dt.normalize()\n",
    "\n",
    "# merge admit_date into all_vanco to compute day index of each lab\n",
    "all_vanco = all_vanco.merge(admit_map[['subject_id','hadm_id','admit_date']], on=['subject_id','hadm_id'], how='left')\n",
    "\n",
    "# if admit_date missing -> we cannot compute day_index -> drop those rows (or keep with NaN day)\n",
    "missing_admit = all_vanco['admit_date'].isna().sum()\n",
    "if missing_admit:\n",
    "    print(f\"Warning: {missing_admit} all_vanco rows have no matching admission (admit_date missing) and will be skipped.\")\n",
    "all_vanco = all_vanco[all_vanco['admit_date'].notna()].copy()\n",
    "\n",
    "# compute lab day index relative to admission day (day0 = admittime.normalize())\n",
    "all_vanco['chart_date'] = pd.to_datetime(all_vanco['charttime'], errors='coerce').dt.normalize()\n",
    "all_vanco['day_index_lab'] = (all_vanco['chart_date'] - all_vanco['admit_date']).dt.days.fillna(0).astype(int)\n",
    "# clamp negative days to 0\n",
    "all_vanco.loc[all_vanco['day_index_lab'] < 0, 'day_index_lab'] = 0\n",
    "\n",
    "# For each (subject, hadm, day_index_lab) pick the row with maximum resolved_val\n",
    "group_cols = ['subject_id','hadm_id','day_index_lab']\n",
    "# drop rows where resolved_val is NaN (no usable numeric) — they won't contribute to max\n",
    "usable = all_vanco[~all_vanco['resolved_val'].isna()].copy()\n",
    "if usable.empty:\n",
    "    print(\"No usable numeric vanco values found to aggregate.\")\n",
    "    # create empty daily_vanco with expected cols\n",
    "    daily_vanco = pd.DataFrame(columns=['subject_id','hadm_id','day_index_lab',\n",
    "                                       'charttime','value','valuenum','valueuom','flag','resolved_val'])\n",
    "else:\n",
    "    idx = usable.groupby(group_cols)['resolved_val'].idxmax()\n",
    "    daily_vanco = usable.loc[idx].copy()\n",
    "\n",
    "# rename columns to DBML names\n",
    "daily_vanco = daily_vanco.rename(columns={\n",
    "    'charttime':'all_vanco_charttime',\n",
    "    'value':'all_vanco_value',\n",
    "    'valuenum':'all_vanco_valuenum',\n",
    "    'valueuom':'all_vanco_valueuom',\n",
    "    'flag':'all_vanco_flag',\n",
    "    'day_index_lab':'day_index'\n",
    "})\n",
    "\n",
    "# keep only needed cols for merging\n",
    "merge_cols = ['subject_id','hadm_id','day_index',\n",
    "              'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']\n",
    "daily_vanco = daily_vanco[merge_cols]\n",
    "\n",
    "# ensure types align\n",
    "daily_vanco['subject_id'] = daily_vanco['subject_id'].astype('Int64')\n",
    "daily_vanco['hadm_id'] = daily_vanco['hadm_id'].astype('Int64')\n",
    "daily_vanco['day_index'] = daily_vanco['day_index'].astype('Int64')\n",
    "\n",
    "# merge into base (left join so all base rows remain); prefer existing base as left\n",
    "merged_with_vanco = merged_with_vanco.merge(daily_vanco, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "print(f\"all_vanco merged -> rows with vanco info: {int(merged_with_vanco['all_vanco_charttime'].notna().sum())}\")\n",
    "\n",
    "# quick preview (first 20 rows that got vanco info)\n",
    "preview = merged_with_vanco[merged_with_vanco['all_vanco_charttime'].notna()].head(20)\n",
    "print(preview[['subject_id','hadm_id','day_index',\n",
    "               'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_vanco.head(100).to_csv('merged_with_vanco_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "chartevents = pd.read_csv(chartevents_path, nrows=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e61e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(chartevents_path,nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)   # همه ردیف‌ها\n",
    "pd.set_option(\"display.max_columns\", None)  # همه ستون‌ها\n",
    "pd.set_option(\"display.width\", None)        # عرض رو محدود نکن\n",
    "pd.set_option(\"display.max_colwidth\", None) # طول رشته ستون رو کامل نشون بده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chartevents.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f99c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked missingness (FIXED)\n",
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(f\"chartevents.csv not found at {chartevents_path.resolve()}\")\n",
    "\n",
    "usecols = ['subject_id','hadm_id','itemid','charttime','value','valuenum']\n",
    "chunksize = 200_000\n",
    "\n",
    "total_counts = defaultdict(int)\n",
    "present_counts = defaultdict(int)\n",
    "\n",
    "reader = pd.read_csv(chartevents_path, usecols=usecols, chunksize=chunksize, low_memory=True)\n",
    "\n",
    "chunk_i = 0\n",
    "for chunk in reader:\n",
    "    chunk_i += 1\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # present mask: True if valuenum exists\n",
    "    present_mask = ~chunk['valuenum'].isna()\n",
    "\n",
    "    # rows that need textual check (valuenum is NaN)\n",
    "    need_check = chunk['valuenum'].isna()\n",
    "    if need_check.any():\n",
    "        vals = chunk.loc[need_check, 'value'].astype(str).str.strip()\n",
    "        good = ~vals.isin([\"\", \"___\", \"NaN\", \"nan\", \"None\", \"none\"])\n",
    "        # <-- FIX: use boolean mask indexing, not need_check.index\n",
    "        present_mask.loc[need_check] = good.values\n",
    "\n",
    "    # aggregate counts per itemid for this chunk\n",
    "    grp_total = chunk.groupby('itemid').size()\n",
    "    # <-- FIX: group the boolean series by the corresponding itemid values and sum\n",
    "    grp_present = present_mask.groupby(chunk['itemid']).sum()\n",
    "\n",
    "    for item, cnt in grp_total.items():\n",
    "        total_counts[int(item)] += int(cnt)\n",
    "    for item, cnt in grp_present.items():\n",
    "        if pd.isna(item):\n",
    "            continue\n",
    "        present_counts[int(item)] += int(cnt)\n",
    "\n",
    "    if chunk_i % 10 == 0:\n",
    "        print(f\"Processed {chunk_i*chunksize:,} rows...\")\n",
    "\n",
    "# build results DataFrame (unchanged from before)\n",
    "itemids = sorted(set(list(total_counts.keys()) + list(present_counts.keys())))\n",
    "rows = []\n",
    "for iid in itemids:\n",
    "    tot = total_counts.get(iid, 0)\n",
    "    pres = present_counts.get(iid, 0)\n",
    "    miss = tot - pres\n",
    "    frac = pres / tot if tot > 0 else 0.0\n",
    "    rows.append((iid, tot, pres, miss, frac))\n",
    "\n",
    "missingness_df = pd.DataFrame(rows, columns=['itemid','total_count','present_count','missing_count','present_fraction'])\n",
    "missingness_df = missingness_df.sort_values(by='present_fraction', ascending=False).reset_index(drop=True)\n",
    "missingness_df.to_csv(\"chartevents_itemid_missingness.csv\", index=False)\n",
    "print(\"Saved chartevents_itemid_missingness.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chartevents_itemid_missingness.csv\")\n",
    "\n",
    "# شرط: کمتر از 50 درصد داده موجود\n",
    "drop_ids = df.loc[df['present_fraction'] < 0.5, 'itemid'].tolist()\n",
    "\n",
    "print(f\"تعداد itemid هایی که باید drop بشن: {len(drop_ids)}\")\n",
    "print(drop_ids[:50])  # برای اینکه فقط ۵۰ تا اولی رو ببینی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c76ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(\"chartevents.csv\", chunksize=2_000_000)\n",
    "out_path = \"chartevents_missing50_dropped.csv\"\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "total_dropped = 0\n",
    "total_written = 0\n",
    "\n",
    "for i, chunk in enumerate(reader, start=1):\n",
    "    before = len(chunk)\n",
    "    filtered = chunk.loc[~chunk['itemid'].isin(drop_ids)]\n",
    "    after = len(filtered)\n",
    "    \n",
    "    # ذخیره به فایل\n",
    "    filtered.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    \n",
    "    # لاگ\n",
    "    total_rows += before\n",
    "    total_dropped += before - after\n",
    "    total_written += after\n",
    "    print(f\"Chunk {i}: rows={before:,}, dropped={before - after:,}, kept={after:,}\")\n",
    "\n",
    "print(\"---- DONE ----\")\n",
    "print(f\"Total rows processed: {total_rows:,}\")\n",
    "print(f\"Total rows dropped:   {total_dropped:,}\")\n",
    "print(f\"Total rows written:   {total_written:,}\")\n",
    "print(\"✅ فایل نهایی ذخیره شد:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"d_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aae3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# فایل ورودی/خروجی\n",
    "in_path = \"d_items.csv\"\n",
    "out_path = \"d_items_chartevents_missing50_dropped.csv\"\n",
    "\n",
    "# فرض می‌کنیم drop_ids رو قبلاً ساختی\n",
    "# drop_ids = [...]\n",
    "\n",
    "# خواندن d_items\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "# فیلتر کردن\n",
    "filtered = df.loc[\n",
    "    (df[\"linksto\"] == \"chartevents\") & \n",
    "    (~df[\"itemid\"].isin(drop_ids))\n",
    "]\n",
    "\n",
    "# ذخیره فایل نهایی\n",
    "filtered.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"✅ d_items filtered and saved:\", out_path)\n",
    "print(\"before:\", len(df), \"after:\", len(filtered), \"drop:\", len(df) - len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# مسیرها / اسم فایل خروجی\n",
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "merged_initial_file = Path(\"merged_initial.csv\")   # اگر merged_initial در حافظه نیست از این خوانده می‌شود\n",
    "out_path = Path(\"merged_initial_with_items_cols.csv\")\n",
    "\n",
    "# 1) load itemids\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "cols_to_add = [str(i) for i in itemids]   # ستون‌ها به صورت رشته نامیده می‌شوند\n",
    "print(\"Will add columns (count):\", len(cols_to_add))\n",
    "\n",
    "# 2) ensure merged_initial exists (either in memory or read from CSV)\n",
    "try:\n",
    "    merged_initial  # اگر در نوت‌بوک تعریف شده باشه از حافظه استفاده می‌کنیم\n",
    "    print(\"Using merged_initial from memory (existing DataFrame). rows:\", len(merged_initial))\n",
    "except NameError:\n",
    "    if not merged_initial_file.exists():\n",
    "        raise FileNotFoundError(f\"merged_initial not in memory and file {merged_initial_file} not found.\")\n",
    "    print(\"Loading merged_initial from disk:\", merged_initial_file)\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "    print(\"Loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "# 3) add columns (only those missing)\n",
    "n_rows = len(merged_initial)\n",
    "added = 0\n",
    "for c in cols_to_add:\n",
    "    if c not in merged_initial.columns:\n",
    "        # مقدار اولیه را pd.NA قرار می‌دهیم؛ dtype فعلی object خواهد بود (قابل نگهداری اعداد/متن)\n",
    "        merged_initial[c] = pd.Series([pd.NA] * n_rows, dtype=\"object\")\n",
    "        added += 1\n",
    "print(f\"Added {added} new columns. Total columns now: {len(merged_initial.columns)}\")\n",
    "\n",
    "# 4) ذخیره چانک‌چانک (برای جلوگیری از مصرف زیاد حافظه/IO spike)\n",
    "chunksize = 10000   # می‌تونی این عدد رو کم/زیاد کنی؛ اگر رم کم داری کوچکتر کن\n",
    "first = True\n",
    "written = 0\n",
    "for start in range(0, n_rows, chunksize):\n",
    "    end = min(start + chunksize, n_rows)\n",
    "    chunk = merged_initial.iloc[start:end]\n",
    "    chunk.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    written += len(chunk)\n",
    "    print(f\"Wrote rows {start:,}..{end-1:,} -> {len(chunk):,} rows\")\n",
    "    # پاکسازی کوچک برای آزاد کردن حافظه\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", written, \"Columns in output:\", len(merged_initial.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57308ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_initial.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- params ----------\n",
    "chartevents_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "# merged_initial already in memory (as per your notebook). If not, set merged_initial_file and read it.\n",
    "merged_initial_file = None   # e.g. \"merged_initial.csv\" if needed; otherwise merged_initial must exist\n",
    "chunksize = 500_000         # تعداد ردیف برای هر chunk از chartevents (کمتر کن اگر رم کم داری)\n",
    "save_after = False          # اگر True می‌خواهد پس از هر chunk intermediate ذخیره کنیم (معمولاً False کافی است)\n",
    "# ----------------------------\n",
    "\n",
    "# sanity checks\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(chartevents_path)\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "\n",
    "# load keep itemids\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "keep_itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "keep_itemids_set = set(keep_itemids)\n",
    "print(\"Keep itemids count:\", len(keep_itemids))\n",
    "\n",
    "# ensure merged_initial present (either in memory or read from file)\n",
    "try:\n",
    "    merged_initial  # noqa: F821\n",
    "except NameError:\n",
    "    if merged_initial_file is None:\n",
    "        raise NameError(\"merged_initial not in memory. Set merged_initial_file path or load it.\")\n",
    "    print(\"Loading merged_initial from disk...\")\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime'])\n",
    "    print(\"loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "# ensure the itemid columns exist in merged_initial\n",
    "# (you said you already added them — if not, add with pd.NA)\n",
    "for iid in keep_itemids:\n",
    "    col = str(iid)\n",
    "    if col not in merged_initial.columns:\n",
    "        merged_initial[col] = pd.Series([pd.NA] * len(merged_initial), dtype=\"object\")\n",
    "\n",
    "# build admit_map for computing day_index quickly: (subject_id,hadm_id) -> admit_date (normalized date)\n",
    "admit_map = merged_initial.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "# turn into dict for fast lookup\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "\n",
    "# build mapping from (subject_id,hadm_id,day_index) -> row index in merged_initial\n",
    "# assume combination unique (one row per day per hadm)\n",
    "merged_initial_index_map = {}\n",
    "for idx, row in merged_initial[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    key = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "    merged_initial_index_map[key] = idx\n",
    "\n",
    "print(\"Admit map keys:\", len(admit_dict), \"merged rows map size:\", len(merged_initial_index_map))\n",
    "\n",
    "# Reader for chartevents\n",
    "reader = pd.read_csv(chartevents_path, usecols=['subject_id','hadm_id','itemid','charttime','value','valuenum'],\n",
    "                     parse_dates=['charttime'], chunksize=chunksize, low_memory=True)\n",
    "\n",
    "total_assigned = 0\n",
    "chunk_no = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_no += 1\n",
    "    print(f\"\\n--- Processing chunk {chunk_no} (rows: {len(chunk)}) ---\")\n",
    "    # normalize itemid and filter to wanted ones\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    chunk = chunk[chunk['itemid'].isin(keep_itemids)]\n",
    "    if chunk.empty:\n",
    "        print(\"no relevant itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # quick type conversions\n",
    "    chunk['subject_id'] = chunk['subject_id'].astype(int)\n",
    "    chunk['hadm_id'] = chunk['hadm_id'].astype(int)\n",
    "\n",
    "    # map admit_date via admit_dict (faster than merge)\n",
    "    def lookup_admit_date(s):\n",
    "        return admit_dict.get((int(s.subject_id), int(s.hadm_id)), pd.NaT)\n",
    "    # vectorized-ish: create key tuples and map using dict\n",
    "    keys = list(zip(chunk['subject_id'].astype(int), chunk['hadm_id'].astype(int)))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    # drop rows without admit_date\n",
    "    chunk = chunk[chunk['admit_date'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(\"no rows with admit_date in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # compute chart_date (normalized) and day_index relative to admit_date\n",
    "    chunk['chart_date'] = chunk['charttime'].dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    # resolved numeric value: prefer valuenum if present, else try numeric parse of 'value'\n",
    "    # create numeric_val (float) and raw_value (str)\n",
    "    chunk['numeric_val'] = pd.to_numeric(chunk['valuenum'], errors='coerce')\n",
    "    # for rows where numeric_val is NaN, try parse from value string\n",
    "    mask_num_missing = chunk['numeric_val'].isna()\n",
    "    if mask_num_missing.any():\n",
    "        parsed = pd.to_numeric(chunk.loc[mask_num_missing, 'value'].astype(str).str.replace(',',''), errors='coerce')\n",
    "        chunk.loc[mask_num_missing, 'numeric_val'] = parsed\n",
    "\n",
    "    chunk['value_raw'] = chunk['value'].astype(str)\n",
    "\n",
    "    # GROUP AGGREGATION:\n",
    "    # keys: subject_id, hadm_id, day_index, itemid\n",
    "    grp_keys = ['subject_id','hadm_id','day_index','itemid']\n",
    "\n",
    "    # 1) groups that have numeric values -> take numeric max per group\n",
    "    numeric_rows = chunk[chunk['numeric_val'].notna()].copy()\n",
    "    if not numeric_rows.empty:\n",
    "        grp_num = numeric_rows.groupby(grp_keys, as_index=False)['numeric_val'].max()\n",
    "        grp_num = grp_num.rename(columns={'numeric_val':'agg_value_num'})\n",
    "    else:\n",
    "        grp_num = pd.DataFrame(columns=grp_keys + ['agg_value_num'])\n",
    "\n",
    "    # 2) for groups that have no numeric val, take last text by charttime\n",
    "    # To compute last by charttime: sort then group.last()\n",
    "    chunk_sorted = chunk.sort_values('charttime')\n",
    "    grp_last = chunk_sorted.groupby(grp_keys, as_index=False).last()[grp_keys + ['value_raw','charttime']]\n",
    "    grp_last = grp_last.rename(columns={'value_raw':'agg_value_text', 'charttime':'agg_time_text'})\n",
    "\n",
    "    # 3) combine: if grp has numeric in grp_num use that, else use grp_last\n",
    "    # merge grp_last with grp_num to know which groups have numeric\n",
    "    merged_grps = pd.merge(grp_last, grp_num, on=grp_keys, how='left')\n",
    "\n",
    "    # build final aggregated DataFrame for this chunk\n",
    "    # if agg_value_num notna -> use that else use agg_value_text\n",
    "    def pick_final_val(row):\n",
    "        if pd.notna(row.get('agg_value_num')):\n",
    "            return row['agg_value_num']\n",
    "        else:\n",
    "            # if text 'nan' or 'None' control\n",
    "            v = row.get('agg_value_text')\n",
    "            if pd.isna(v) or v in (\"nan\",\"None\",\"NoneType\",\"NA\",\"<NA>\"):\n",
    "                return pd.NA\n",
    "            return v\n",
    "\n",
    "    merged_grps['final_value'] = merged_grps.apply(pick_final_val, axis=1)\n",
    "\n",
    "    # 4) assign values into merged_initial using mapping dict merged_initial_index_map\n",
    "    assigned = 0\n",
    "    for _, r in merged_grps.iterrows():\n",
    "        key = (int(r['subject_id']), int(r['hadm_id']), int(r['day_index']))\n",
    "        row_idx = merged_initial_index_map.get(key)\n",
    "        if row_idx is None:\n",
    "            # no matching admission-day row in merged_initial (possible) -> skip\n",
    "            continue\n",
    "        itemid_col = str(int(r['itemid']))\n",
    "        val = r['final_value']\n",
    "        # write to DataFrame cell (in-place)\n",
    "        # convert floats that are integer-like to native python types optional\n",
    "        merged_initial.at[row_idx, itemid_col] = val\n",
    "        assigned += 1\n",
    "\n",
    "    total_assigned += assigned\n",
    "    print(f\"Chunk {chunk_no}: groups aggregated = {len(merged_grps)}, assigned = {assigned}, total_assigned so far = {total_assigned}\")\n",
    "\n",
    "    # cleanup\n",
    "    del chunk, chunk_sorted, numeric_rows, grp_num, grp_last, merged_grps\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- ALL CHUNKS PROCESSED ---\")\n",
    "print(\"Total assigned cells:\", total_assigned)\n",
    "\n",
    "# finally, save merged_initial to CSV in row-chunks to avoid memory spikes\n",
    "out_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "n_rows = len(merged_initial)\n",
    "write_chunk = 20000\n",
    "first = True\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_initial.iloc[start:end].to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved final to:\", out_path) # Saved final to: merged_with_chartevents_filled.csv (43m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "merged_with_chartevents_filled = pd.read_csv(merged_with_chartevents_filled_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82151eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "output_path = Path(\"chartevents_missing50_dropped_filtered_hadm_id_23282506.csv\")\n",
    "\n",
    "chunksize = 2_000_000  # می‌تونی تغییر بدی\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize, low_memory=False)):\n",
    "    filtered = chunk[chunk['hadm_id'] == 23282506]\n",
    "    if not filtered.empty:\n",
    "        filtered.to_csv(output_path, mode='w' if first else 'a',\n",
    "                        index=False, header=first)\n",
    "        first = False\n",
    "        total_rows += len(filtered)\n",
    "        print(f\"Chunk {i}: wrote {len(filtered)} rows (total so far: {total_rows})\")\n",
    "\n",
    "print(\"Done! Final rows written:\", total_rows)\n",
    "print(\"Output file:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40055979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- paths ----------\n",
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "in_path = Path(\"merged_with_chartevents_filled.csv\")   # ورودی بزرگ\n",
    "out_path = Path(\"merged_with_chartevents_filled_renamed.csv\")  # خروجی جدید\n",
    "# ---------- params ----------\n",
    "chunksize = 20_000   # تعداد ردیف برای هر chunk خواندن/نوشتن (کمتر کن اگر رم کمتر داری)\n",
    "max_name_len = 80    # حداکثر طول برای نام ستون (اختیاری)\n",
    "# ------------------------\n",
    "\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(in_path)\n",
    "\n",
    "# ---------- 1) بساز نگاشت itemid -> desired_name (abbrev if present else label) ----------\n",
    "d = pd.read_csv(ditems_path, usecols=['itemid','label','abbreviation'], dtype=str)\n",
    "# clean whitespace\n",
    "d['itemid'] = d['itemid'].str.strip()\n",
    "d['label'] = d['label'].fillna('').astype(str).str.strip()\n",
    "d['abbreviation'] = d['abbreviation'].fillna('').astype(str).str.strip()\n",
    "\n",
    "# choose abbreviation if present else label\n",
    "d['chosen'] = d.apply(lambda r: r['abbreviation'] if r['abbreviation']!='' else (r['label'] if r['label']!='' else ''), axis=1)\n",
    "\n",
    "def sanitize_name(s):\n",
    "    \"\"\"Make header-safe, short, and deterministic string.\"\"\"\n",
    "    if pd.isna(s) or s is None:\n",
    "        return ''\n",
    "    s = str(s).strip()\n",
    "    # replace whitespace with underscore\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    # remove characters except letters, numbers, underscore, dash\n",
    "    s = re.sub(r'[^\\w\\-]', '', s)\n",
    "    # collapse multiple underscores\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s[:max_name_len]\n",
    "    return s\n",
    "\n",
    "# build initial mapping and ensure uniqueness\n",
    "name_map = {}    # key: itemid as string, value: final column name\n",
    "used = set()\n",
    "\n",
    "for _, row in d.iterrows():\n",
    "    iid = row['itemid']\n",
    "    chosen = row['chosen']\n",
    "    if chosen == '':\n",
    "        base = f\"item_{iid}\"\n",
    "    else:\n",
    "        base = sanitize_name(chosen)\n",
    "        if base == '':\n",
    "            base = f\"item_{iid}\"\n",
    "    # ensure unique: if collision append __<itemid>\n",
    "    name = base\n",
    "    if name in used:\n",
    "        name = f\"{base}__{iid}\"\n",
    "    # as fallback, if still collision (very unlikely), append counter\n",
    "    counter = 1\n",
    "    while name in used:\n",
    "        name = f\"{base}__{iid}_{counter}\"\n",
    "        counter += 1\n",
    "    used.add(name)\n",
    "    name_map[str(iid)] = name\n",
    "\n",
    "# ---------- 2) read header of input and prepare final new_columns list ----------\n",
    "orig_header = pd.read_csv(in_path, nrows=0).columns.tolist()\n",
    "new_header = []\n",
    "conflicts = 0\n",
    "for col in orig_header:\n",
    "    # if column is an itemid (digits only) and exists in mapping -> replace\n",
    "    new_col = col\n",
    "    col_str = str(col).strip()\n",
    "    # try exact match using numeric-like names (most itemid columns are digits strings)\n",
    "    if col_str in name_map:\n",
    "        new_col = \"chartevents_\" + name_map[col_str]\n",
    "    else:\n",
    "        # try converting to int then str (covers possible leading zeros or int types)\n",
    "        try:\n",
    "            icol = str(int(float(col_str)))  # safe convert e.g. '220045.0' -> '220045'\n",
    "            if icol in name_map:\n",
    "                new_col = name_map[icol]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # ensure no duplicate among new_header; if duplicate, append suffix with original col id\n",
    "    if new_col in new_header:\n",
    "        conflicts += 1\n",
    "        new_col = f\"{new_col}__orig_{sanitize_name(col_str)}\"\n",
    "        # still ensure uniqueness\n",
    "        k = 1\n",
    "        while new_col in new_header:\n",
    "            new_col = f\"{new_col}_{k}\"; k += 1\n",
    "    new_header.append(new_col)\n",
    "\n",
    "print(f\"Prepared header mapping. Total cols: {len(orig_header)}, conflicts resolved: {conflicts}\")\n",
    "\n",
    "# ---------- show a small sample of mapping (optional) ----------\n",
    "sample_map = {k: name_map[k] for k in list(name_map)[:10]}\n",
    "print(\"sample itemid->name (first 10):\", sample_map)\n",
    "\n",
    "# ---------- 3) stream through input file in chunks, set df.columns = new_header and write ----------\n",
    "first = True\n",
    "rows_written = 0\n",
    "for i, chunk in enumerate(pd.read_csv(in_path, chunksize=chunksize, low_memory=False)):\n",
    "    # assign new header (chunk.columns matches orig_header length)\n",
    "    chunk.columns = new_header\n",
    "    # write\n",
    "    chunk.to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    rows_written += len(chunk)\n",
    "    print(f\"Chunk {i+1}: wrote {len(chunk):,} rows (total {rows_written:,})\")\n",
    "    # cleanup\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", rows_written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed_path = Path(\"merged_with_chartevents_filled_renamed.csv\")\n",
    "merged_with_chartevents_filled_renamed = pd.read_csv(merged_with_chartevents_filled_renamed_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents_path = Path(\"datetimeevents.csv\")\n",
    "datetimeevents = pd.read_csv(datetimeevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252369fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetimeevents.head(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
