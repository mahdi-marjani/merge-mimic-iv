{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"admissions.csv\")\n",
    "\n",
    "admissions = pd.read_csv(csv_path, low_memory=False,\n",
    "                            parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "print(\"Loaded admissions.csv from disk. Rows:\", len(admissions))\n",
    "\n",
    "admissions['admittime'] = pd.to_datetime(admissions['admittime'], errors='coerce')\n",
    "admissions['dischtime'] = pd.to_datetime(admissions['dischtime'], errors='coerce')\n",
    "\n",
    "admissions['dischtime'] = admissions['dischtime'].fillna(admissions['admittime'])\n",
    "\n",
    "def expand_admission_row(row):\n",
    "    adm_date = row['admittime'].normalize().date()\n",
    "    dis_date = row['dischtime'].normalize().date()\n",
    "    n_days = (dis_date - adm_date).days + 1\n",
    "    if n_days <= 0:\n",
    "        n_days = 1\n",
    "    rows = []\n",
    "    for d in range(n_days):\n",
    "        new = {\n",
    "            'subject_id': row['subject_id'],\n",
    "            'hadm_id': row['hadm_id'],\n",
    "            'day_index': int(d),\n",
    "            'admittime': row['admittime'],\n",
    "            'dischtime': row['dischtime'],\n",
    "            'deathtime': row['deathtime'],\n",
    "            'admission_type': row.get('admission_type', np.nan),\n",
    "            'admit_provider_id': row.get('admit_provider_id', np.nan),\n",
    "            'admission_location': row.get('admission_location', np.nan),\n",
    "            'discharge_location': row.get('discharge_location', np.nan),\n",
    "            'insurance': row.get('insurance', np.nan),\n",
    "            'language': row.get('language', np.nan),\n",
    "            'marital_status': row.get('marital_status', np.nan),\n",
    "            'race': row.get('race', np.nan),\n",
    "            'edregtime': row.get('edregtime', pd.NaT),\n",
    "            'edouttime': row.get('edouttime', pd.NaT),\n",
    "            'hospital_expire_flag': row.get('hospital_expire_flag', np.nan)\n",
    "        }\n",
    "        rows.append(new)\n",
    "    return rows\n",
    "\n",
    "expanded = []\n",
    "for _, r in admissions.iterrows():\n",
    "    expanded.extend(expand_admission_row(r))\n",
    "\n",
    "merged_initial = pd.DataFrame(expanded)\n",
    "merged_initial[['subject_id','hadm_id','day_index']] = merged_initial[['subject_id','hadm_id','day_index']].astype('Int64')\n",
    "\n",
    "print(\"Expanded admissions -> rows:\", merged_initial.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv(\"admissions_expanded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_path = Path(\"icustays.csv\")\n",
    "if not icu_path.exists():\n",
    "    raise FileNotFoundError(f\"icustays.csv not found at {icu_path.resolve()}  -- put the file next to admissions.csv\")\n",
    "\n",
    "icustays = pd.read_csv(icu_path, low_memory=False, parse_dates=['intime','outtime'])\n",
    "\n",
    "for col in ['subject_id','hadm_id','intime','outtime']:\n",
    "    if col not in icustays.columns:\n",
    "        raise KeyError(f\"Expected column '{col}' in icustays.csv but it is missing.\")\n",
    "\n",
    "optional_cols = ['stay_id','first_careunit','last_careunit','los']\n",
    "for c in optional_cols:\n",
    "    if c not in icustays.columns:\n",
    "        icustays[c] = pd.NA\n",
    "\n",
    "merged_with_icu = merged_initial.copy().reset_index(drop=False).rename(columns={'index':'row_id'})\n",
    "for col in ['stay_id_icu','icustay_intime','icustay_outtime','first_careunit_icu','last_careunit_icu','los_icu']:\n",
    "    if col not in merged_with_icu.columns:\n",
    "        merged_with_icu[col] = pd.NA\n",
    "\n",
    "if 'admittime' not in merged_with_icu.columns:\n",
    "    raise KeyError(\"merged_initial must contain 'admittime' column\")\n",
    "\n",
    "merged_with_icu['admittime'] = pd.to_datetime(merged_with_icu['admittime'], errors='coerce')\n",
    "merged_with_icu['day_index_int'] = merged_with_icu['day_index'].fillna(0).astype(int)\n",
    "merged_with_icu['row_date'] = merged_with_icu['admittime'].dt.normalize() + pd.to_timedelta(merged_with_icu['day_index_int'], unit='D')\n",
    "\n",
    "icustays['intime'] = pd.to_datetime(icustays['intime'], errors='coerce')\n",
    "icustays['outtime'] = pd.to_datetime(icustays['outtime'], errors='coerce').fillna(icustays['intime'])\n",
    "icustays['intime_norm'] = icustays['intime'].dt.normalize()\n",
    "icustays['outtime_norm'] = icustays['outtime'].dt.normalize()\n",
    "\n",
    "icu_keep = ['subject_id','hadm_id','stay_id','intime','outtime','intime_norm','outtime_norm','first_careunit','last_careunit','los']\n",
    "candidate = merged_with_icu.merge(icustays[icu_keep], on=['subject_id','hadm_id'], how='left', suffixes=('','_icu'))\n",
    "\n",
    "mask_in_icu = (candidate['row_date'] >= candidate['intime_norm']) & (candidate['row_date'] <= candidate['outtime_norm'])\n",
    "candidate['in_icu'] = mask_in_icu.fillna(False)\n",
    "\n",
    "matched = candidate[candidate['in_icu']].copy()\n",
    "if not matched.empty:\n",
    "    matched = matched.sort_values(by=['row_id','intime'])\n",
    "    first_matches = matched.groupby('row_id', as_index=False).first()\n",
    "    map_cols = {\n",
    "        'stay_id':'stay_id_icu',\n",
    "        'intime':'icustay_intime',\n",
    "        'outtime':'icustay_outtime',\n",
    "        'first_careunit':'first_careunit_icu',\n",
    "        'last_careunit':'last_careunit_icu',\n",
    "        'los':'los_icu'\n",
    "    }\n",
    "    for src, dst in map_cols.items():\n",
    "        mapping = first_matches.set_index('row_id')[src]\n",
    "        merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), dst] = merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), 'row_id'].map(mapping)\n",
    "    assigned_count = len(first_matches)\n",
    "else:\n",
    "    assigned_count = 0\n",
    "\n",
    "merged_with_icu = merged_with_icu.drop(columns=['day_index_int','row_date'])\n",
    "\n",
    "print(f\"ICU assignment complete. Rows where ICU info filled: {int(assigned_count)}\")\n",
    "print(merged_with_icu[merged_with_icu['stay_id_icu'].notna()].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'stay_id_icu': 'stay_id',\n",
    "    'first_careunit_icu': 'first_careunit',\n",
    "    'last_careunit_icu': 'last_careunit',\n",
    "    'icustay_intime': 'icustays_intime',\n",
    "    'icustay_outtime': 'icustays_outtime',\n",
    "    'los_icu': 'los'\n",
    "}\n",
    "merged_with_icu = merged_with_icu.rename(columns=rename_map)\n",
    "\n",
    "icu_cols_ordered = ['stay_id', 'first_careunit', 'last_careunit', 'icustays_intime', 'icustays_outtime', 'los']\n",
    "\n",
    "other_cols = [c for c in merged_with_icu.columns if c not in icu_cols_ordered]\n",
    "\n",
    "merged_with_icu = merged_with_icu[other_cols + icu_cols_ordered]\n",
    "\n",
    "print(\"Renaming & reordering complete.\")\n",
    "print(merged_with_icu[icu_cols_ordered].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06434530",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e880f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f318e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv('merged_initial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.to_csv('merged_with_icu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.head(100).to_csv('merged_with_icu_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanco_path = Path(\"all_vanco.csv\")\n",
    "if not vanco_path.exists():\n",
    "    raise FileNotFoundError(f\"all_vanco.csv not found at {vanco_path.resolve()}\")\n",
    "\n",
    "all_vanco = pd.read_csv(vanco_path, low_memory=False, parse_dates=['charttime'])\n",
    "\n",
    "all_vanco['subject_id'] = pd.to_numeric(all_vanco['subject_id'], errors='coerce').astype('Int64')\n",
    "all_vanco['hadm_id'] = pd.to_numeric(all_vanco['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "def resolve_numeric(row):\n",
    "    v = row.get('value')\n",
    "    vn = row.get('valuenum')\n",
    "    if pd.isna(v) or str(v).strip() in ['', '___', 'NaN', 'nan']:\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    s = str(v).strip().replace(',', '')\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "all_vanco['resolved_val'] = all_vanco.apply(resolve_numeric, axis=1)\n",
    "\n",
    "merged_with_vanco = merged_with_icu.copy()\n",
    "if 'admittime' not in merged_with_vanco.columns:\n",
    "    raise KeyError(f\"merged_with_vanco must contain 'admittime' column before merging labs.\")\n",
    "merged_with_vanco['admittime'] = pd.to_datetime(merged_with_vanco['admittime'], errors='coerce')\n",
    "\n",
    "admit_map = merged_with_vanco.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time']).dt.normalize()\n",
    "\n",
    "all_vanco = all_vanco.merge(admit_map[['subject_id','hadm_id','admit_date']], on=['subject_id','hadm_id'], how='left')\n",
    "\n",
    "missing_admit = all_vanco['admit_date'].isna().sum()\n",
    "if missing_admit:\n",
    "    print(f\"Warning: {missing_admit} all_vanco rows have no matching admission (admit_date missing) and will be skipped.\")\n",
    "all_vanco = all_vanco[all_vanco['admit_date'].notna()].copy()\n",
    "\n",
    "all_vanco['chart_date'] = pd.to_datetime(all_vanco['charttime'], errors='coerce').dt.normalize()\n",
    "all_vanco['day_index_lab'] = (all_vanco['chart_date'] - all_vanco['admit_date']).dt.days.fillna(0).astype(int)\n",
    "all_vanco.loc[all_vanco['day_index_lab'] < 0, 'day_index_lab'] = 0\n",
    "\n",
    "group_cols = ['subject_id','hadm_id','day_index_lab']\n",
    "usable = all_vanco[~all_vanco['resolved_val'].isna()].copy()\n",
    "if usable.empty:\n",
    "    print(\"No usable numeric vanco values found to aggregate.\")\n",
    "    daily_vanco = pd.DataFrame(columns=['subject_id','hadm_id','day_index_lab',\n",
    "                                       'charttime','value','valuenum','valueuom','flag','resolved_val'])\n",
    "else:\n",
    "    idx = usable.groupby(group_cols)['resolved_val'].idxmax()\n",
    "    daily_vanco = usable.loc[idx].copy()\n",
    "\n",
    "daily_vanco = daily_vanco.rename(columns={\n",
    "    'charttime':'all_vanco_charttime',\n",
    "    'value':'all_vanco_value',\n",
    "    'valuenum':'all_vanco_valuenum',\n",
    "    'valueuom':'all_vanco_valueuom',\n",
    "    'flag':'all_vanco_flag',\n",
    "    'day_index_lab':'day_index'\n",
    "})\n",
    "\n",
    "merge_cols = ['subject_id','hadm_id','day_index',\n",
    "              'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']\n",
    "daily_vanco = daily_vanco[merge_cols]\n",
    "\n",
    "daily_vanco['subject_id'] = daily_vanco['subject_id'].astype('Int64')\n",
    "daily_vanco['hadm_id'] = daily_vanco['hadm_id'].astype('Int64')\n",
    "daily_vanco['day_index'] = daily_vanco['day_index'].astype('Int64')\n",
    "\n",
    "merged_with_vanco = merged_with_vanco.merge(daily_vanco, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "print(f\"all_vanco merged -> rows with vanco info: {int(merged_with_vanco['all_vanco_charttime'].notna().sum())}\")\n",
    "\n",
    "preview = merged_with_vanco[merged_with_vanco['all_vanco_charttime'].notna()].head(20)\n",
    "print(preview[['subject_id','hadm_id','day_index',\n",
    "               'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_vanco.head(100).to_csv('merged_with_vanco_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "chartevents = pd.read_csv(chartevents_path, nrows=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e61e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(chartevents_path,nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chartevents.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f99c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(f\"chartevents.csv not found at {chartevents_path.resolve()}\")\n",
    "\n",
    "usecols = ['subject_id','hadm_id','itemid','charttime','value','valuenum']\n",
    "chunksize = 200_000\n",
    "\n",
    "total_counts = defaultdict(int)\n",
    "present_counts = defaultdict(int)\n",
    "\n",
    "reader = pd.read_csv(chartevents_path, usecols=usecols, chunksize=chunksize, low_memory=True)\n",
    "\n",
    "chunk_i = 0\n",
    "for chunk in reader:\n",
    "    chunk_i += 1\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    present_mask = ~chunk['valuenum'].isna()\n",
    "\n",
    "    need_check = chunk['valuenum'].isna()\n",
    "    if need_check.any():\n",
    "        vals = chunk.loc[need_check, 'value'].astype(str).str.strip()\n",
    "        good = ~vals.isin([\"\", \"___\", \"NaN\", \"nan\", \"None\", \"none\"])\n",
    "        present_mask.loc[need_check] = good.values\n",
    "\n",
    "    grp_total = chunk.groupby('itemid').size()\n",
    "    grp_present = present_mask.groupby(chunk['itemid']).sum()\n",
    "\n",
    "    for item, cnt in grp_total.items():\n",
    "        total_counts[int(item)] += int(cnt)\n",
    "    for item, cnt in grp_present.items():\n",
    "        if pd.isna(item):\n",
    "            continue\n",
    "        present_counts[int(item)] += int(cnt)\n",
    "\n",
    "    if chunk_i % 10 == 0:\n",
    "        print(f\"Processed {chunk_i*chunksize:,} rows...\")\n",
    "\n",
    "itemids = sorted(set(list(total_counts.keys()) + list(present_counts.keys())))\n",
    "rows = []\n",
    "for iid in itemids:\n",
    "    tot = total_counts.get(iid, 0)\n",
    "    pres = present_counts.get(iid, 0)\n",
    "    miss = tot - pres\n",
    "    frac = pres / tot if tot > 0 else 0.0\n",
    "    rows.append((iid, tot, pres, miss, frac))\n",
    "\n",
    "missingness_df = pd.DataFrame(rows, columns=['itemid','total_count','present_count','missing_count','present_fraction'])\n",
    "missingness_df = missingness_df.sort_values(by='present_fraction', ascending=False).reset_index(drop=True)\n",
    "missingness_df.to_csv(\"chartevents_itemid_missingness.csv\", index=False)\n",
    "print(\"Saved chartevents_itemid_missingness.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chartevents_itemid_missingness.csv\")\n",
    "\n",
    "drop_ids = df.loc[df['present_fraction'] < 0.5, 'itemid'].tolist()\n",
    "\n",
    "print(f\"تعداد itemid هایی که باید drop بشن: {len(drop_ids)}\")\n",
    "print(drop_ids[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c76ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(\"chartevents.csv\", chunksize=2_000_000)\n",
    "out_path = \"chartevents_missing50_dropped.csv\"\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "total_dropped = 0\n",
    "total_written = 0\n",
    "\n",
    "for i, chunk in enumerate(reader, start=1):\n",
    "    before = len(chunk)\n",
    "    filtered = chunk.loc[~chunk['itemid'].isin(drop_ids)]\n",
    "    after = len(filtered)\n",
    "    \n",
    "    filtered.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    \n",
    "    total_rows += before\n",
    "    total_dropped += before - after\n",
    "    total_written += after\n",
    "    print(f\"Chunk {i}: rows={before:,}, dropped={before - after:,}, kept={after:,}\")\n",
    "\n",
    "print(\"---- DONE ----\")\n",
    "print(f\"Total rows processed: {total_rows:,}\")\n",
    "print(f\"Total rows dropped:   {total_dropped:,}\")\n",
    "print(f\"Total rows written:   {total_written:,}\")\n",
    "print(\"✅ فایل نهایی ذخیره شد:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"d_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aae3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"d_items.csv\"\n",
    "out_path = \"d_items_chartevents_missing50_dropped.csv\"\n",
    "\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "filtered = df.loc[\n",
    "    (df[\"linksto\"] == \"chartevents\") & \n",
    "    (~df[\"itemid\"].isin(drop_ids))\n",
    "]\n",
    "\n",
    "filtered.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"✅ d_items filtered and saved:\", out_path)\n",
    "print(\"before:\", len(df), \"after:\", len(filtered), \"drop:\", len(df) - len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "merged_initial_file = Path(\"merged_initial.csv\")\n",
    "out_path = Path(\"merged_initial_with_items_cols.csv\")\n",
    "\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "cols_to_add = [str(i) for i in itemids]\n",
    "print(\"Will add columns (count):\", len(cols_to_add))\n",
    "\n",
    "try:\n",
    "    merged_initial\n",
    "    print(\"Using merged_initial from memory (existing DataFrame). rows:\", len(merged_initial))\n",
    "except NameError:\n",
    "    if not merged_initial_file.exists():\n",
    "        raise FileNotFoundError(f\"merged_initial not in memory and file {merged_initial_file} not found.\")\n",
    "    print(\"Loading merged_initial from disk:\", merged_initial_file)\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "    print(\"Loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "n_rows = len(merged_initial)\n",
    "added = 0\n",
    "for c in cols_to_add:\n",
    "    if c not in merged_initial.columns:\n",
    "        merged_initial[c] = pd.Series([pd.NA] * n_rows, dtype=\"object\")\n",
    "        added += 1\n",
    "print(f\"Added {added} new columns. Total columns now: {len(merged_initial.columns)}\")\n",
    "\n",
    "chunksize = 10000\n",
    "first = True\n",
    "written = 0\n",
    "for start in range(0, n_rows, chunksize):\n",
    "    end = min(start + chunksize, n_rows)\n",
    "    chunk = merged_initial.iloc[start:end]\n",
    "    chunk.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    written += len(chunk)\n",
    "    print(f\"Wrote rows {start:,}..{end-1:,} -> {len(chunk):,} rows\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", written, \"Columns in output:\", len(merged_initial.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57308ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_initial.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "merged_initial_file = None\n",
    "chunksize = 500_000\n",
    "save_after = False\n",
    "\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(chartevents_path)\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "keep_itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "keep_itemids_set = set(keep_itemids)\n",
    "print(\"Keep itemids count:\", len(keep_itemids))\n",
    "\n",
    "try:\n",
    "    merged_initial\n",
    "except NameError:\n",
    "    if merged_initial_file is None:\n",
    "        raise NameError(\"merged_initial not in memory. Set merged_initial_file path or load it.\")\n",
    "    print(\"Loading merged_initial from disk...\")\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime'])\n",
    "    print(\"loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "for iid in keep_itemids:\n",
    "    col = str(iid)\n",
    "    if col not in merged_initial.columns:\n",
    "        merged_initial[col] = pd.Series([pd.NA] * len(merged_initial), dtype=\"object\")\n",
    "\n",
    "admit_map = merged_initial.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "\n",
    "merged_initial_index_map = {}\n",
    "for idx, row in merged_initial[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    key = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "    merged_initial_index_map[key] = idx\n",
    "\n",
    "print(\"Admit map keys:\", len(admit_dict), \"merged rows map size:\", len(merged_initial_index_map))\n",
    "\n",
    "reader = pd.read_csv(chartevents_path, usecols=['subject_id','hadm_id','itemid','charttime','value','valuenum'],\n",
    "                     parse_dates=['charttime'], chunksize=chunksize, low_memory=True)\n",
    "\n",
    "total_assigned = 0\n",
    "chunk_no = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_no += 1\n",
    "    print(f\"\\n--- Processing chunk {chunk_no} (rows: {len(chunk)}) ---\")\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    chunk = chunk[chunk['itemid'].isin(keep_itemids)]\n",
    "    if chunk.empty:\n",
    "        print(\"no relevant itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    chunk['subject_id'] = chunk['subject_id'].astype(int)\n",
    "    chunk['hadm_id'] = chunk['hadm_id'].astype(int)\n",
    "\n",
    "    def lookup_admit_date(s):\n",
    "        return admit_dict.get((int(s.subject_id), int(s.hadm_id)), pd.NaT)\n",
    "    keys = list(zip(chunk['subject_id'].astype(int), chunk['hadm_id'].astype(int)))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    chunk = chunk[chunk['admit_date'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(\"no rows with admit_date in this chunk\")\n",
    "        continue\n",
    "\n",
    "    chunk['chart_date'] = chunk['charttime'].dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    chunk['numeric_val'] = pd.to_numeric(chunk['valuenum'], errors='coerce')\n",
    "    mask_num_missing = chunk['numeric_val'].isna()\n",
    "    if mask_num_missing.any():\n",
    "        parsed = pd.to_numeric(chunk.loc[mask_num_missing, 'value'].astype(str).str.replace(',',''), errors='coerce')\n",
    "        chunk.loc[mask_num_missing, 'numeric_val'] = parsed\n",
    "\n",
    "    chunk['value_raw'] = chunk['value'].astype(str)\n",
    "\n",
    "    grp_keys = ['subject_id','hadm_id','day_index','itemid']\n",
    "\n",
    "    numeric_rows = chunk[chunk['numeric_val'].notna()].copy()\n",
    "    if not numeric_rows.empty:\n",
    "        grp_num = numeric_rows.groupby(grp_keys, as_index=False)['numeric_val'].max()\n",
    "        grp_num = grp_num.rename(columns={'numeric_val':'agg_value_num'})\n",
    "    else:\n",
    "        grp_num = pd.DataFrame(columns=grp_keys + ['agg_value_num'])\n",
    "\n",
    "    chunk_sorted = chunk.sort_values('charttime')\n",
    "    grp_last = chunk_sorted.groupby(grp_keys, as_index=False).last()[grp_keys + ['value_raw','charttime']]\n",
    "    grp_last = grp_last.rename(columns={'value_raw':'agg_value_text', 'charttime':'agg_time_text'})\n",
    "\n",
    "    merged_grps = pd.merge(grp_last, grp_num, on=grp_keys, how='left')\n",
    "\n",
    "    def pick_final_val(row):\n",
    "        if pd.notna(row.get('agg_value_num')):\n",
    "            return row['agg_value_num']\n",
    "        else:\n",
    "            v = row.get('agg_value_text')\n",
    "            if pd.isna(v) or v in (\"nan\",\"None\",\"NoneType\",\"NA\",\"<NA>\"):\n",
    "                return pd.NA\n",
    "            return v\n",
    "\n",
    "    merged_grps['final_value'] = merged_grps.apply(pick_final_val, axis=1)\n",
    "\n",
    "    assigned = 0\n",
    "    for _, r in merged_grps.iterrows():\n",
    "        key = (int(r['subject_id']), int(r['hadm_id']), int(r['day_index']))\n",
    "        row_idx = merged_initial_index_map.get(key)\n",
    "        if row_idx is None:\n",
    "            continue\n",
    "        itemid_col = str(int(r['itemid']))\n",
    "        val = r['final_value']\n",
    "        merged_initial.at[row_idx, itemid_col] = val\n",
    "        assigned += 1\n",
    "\n",
    "    total_assigned += assigned\n",
    "    print(f\"Chunk {chunk_no}: groups aggregated = {len(merged_grps)}, assigned = {assigned}, total_assigned so far = {total_assigned}\")\n",
    "\n",
    "    del chunk, chunk_sorted, numeric_rows, grp_num, grp_last, merged_grps\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- ALL CHUNKS PROCESSED ---\")\n",
    "print(\"Total assigned cells:\", total_assigned)\n",
    "\n",
    "out_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "n_rows = len(merged_initial)\n",
    "write_chunk = 20000\n",
    "first = True\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_initial.iloc[start:end].to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved final to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "merged_with_chartevents_filled = pd.read_csv(merged_with_chartevents_filled_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82151eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "output_path = Path(\"chartevents_missing50_dropped_filtered_hadm_id_23282506.csv\")\n",
    "\n",
    "chunksize = 2_000_000\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize, low_memory=False)):\n",
    "    filtered = chunk[chunk['hadm_id'] == 23282506]\n",
    "    if not filtered.empty:\n",
    "        filtered.to_csv(output_path, mode='w' if first else 'a',\n",
    "                        index=False, header=first)\n",
    "        first = False\n",
    "        total_rows += len(filtered)\n",
    "        print(f\"Chunk {i}: wrote {len(filtered)} rows (total so far: {total_rows})\")\n",
    "\n",
    "print(\"Done! Final rows written:\", total_rows)\n",
    "print(\"Output file:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40055979",
   "metadata": {},
   "outputs": [],
   "source": [
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "in_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "out_path = Path(\"merged_with_chartevents_filled_renamed.csv\")\n",
    "chunksize = 20_000\n",
    "max_name_len = 80\n",
    "\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(in_path)\n",
    "\n",
    "d = pd.read_csv(ditems_path, usecols=['itemid','label','abbreviation'], dtype=str)\n",
    "d['itemid'] = d['itemid'].str.strip()\n",
    "d['label'] = d['label'].fillna('').astype(str).str.strip()\n",
    "d['abbreviation'] = d['abbreviation'].fillna('').astype(str).str.strip()\n",
    "\n",
    "d['chosen'] = d.apply(lambda r: r['abbreviation'] if r['abbreviation']!='' else (r['label'] if r['label']!='' else ''), axis=1)\n",
    "\n",
    "def sanitize_name(s):\n",
    "    if pd.isna(s) or s is None:\n",
    "        return ''\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w\\-]', '', s)\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s[:max_name_len]\n",
    "    return s\n",
    "\n",
    "name_map = {}\n",
    "used = set()\n",
    "\n",
    "for _, row in d.iterrows():\n",
    "    iid = row['itemid']\n",
    "    chosen = row['chosen']\n",
    "    if chosen == '':\n",
    "        base = f\"item_{iid}\"\n",
    "    else:\n",
    "        base = sanitize_name(chosen)\n",
    "        if base == '':\n",
    "            base = f\"item_{iid}\"\n",
    "    name = base\n",
    "    if name in used:\n",
    "        name = f\"{base}__{iid}\"\n",
    "    counter = 1\n",
    "    while name in used:\n",
    "        name = f\"{base}__{iid}_{counter}\"\n",
    "        counter += 1\n",
    "    used.add(name)\n",
    "    name_map[str(iid)] = name\n",
    "\n",
    "orig_header = pd.read_csv(in_path, nrows=0).columns.tolist()\n",
    "new_header = []\n",
    "conflicts = 0\n",
    "for col in orig_header:\n",
    "    new_col = col\n",
    "    col_str = str(col).strip()\n",
    "    if col_str in name_map:\n",
    "        new_col = \"chartevents_\" + name_map[col_str]\n",
    "    else:\n",
    "        try:\n",
    "            icol = str(int(float(col_str)))\n",
    "            if icol in name_map:\n",
    "                new_col = name_map[icol]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if new_col in new_header:\n",
    "        conflicts += 1\n",
    "        new_col = f\"{new_col}__orig_{sanitize_name(col_str)}\"\n",
    "        k = 1\n",
    "        while new_col in new_header:\n",
    "            new_col = f\"{new_col}_{k}\"; k += 1\n",
    "    new_header.append(new_col)\n",
    "\n",
    "print(f\"Prepared header mapping. Total cols: {len(orig_header)}, conflicts resolved: {conflicts}\")\n",
    "\n",
    "sample_map = {k: name_map[k] for k in list(name_map)[:10]}\n",
    "print(\"sample itemid->name (first 10):\", sample_map)\n",
    "\n",
    "first = True\n",
    "rows_written = 0\n",
    "for i, chunk in enumerate(pd.read_csv(in_path, chunksize=chunksize, low_memory=False)):\n",
    "    chunk.columns = new_header\n",
    "    chunk.to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    rows_written += len(chunk)\n",
    "    print(f\"Chunk {i+1}: wrote {len(chunk):,} rows (total {rows_written:,})\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", rows_written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed_path = Path(\"merged_with_chartevents_filled_renamed.csv\")\n",
    "merged_with_chartevents_filled_renamed = pd.read_csv(merged_with_chartevents_filled_renamed_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents_path = Path(\"datetimeevents.csv\")\n",
    "datetimeevents = pd.read_csv(datetimeevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252369fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetimeevents.head(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
