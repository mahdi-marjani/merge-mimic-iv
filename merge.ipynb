{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"admissions.csv\")\n",
    "\n",
    "admissions = pd.read_csv(csv_path, low_memory=False,\n",
    "                            parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "print(\"Loaded admissions.csv from disk. Rows:\", len(admissions))\n",
    "\n",
    "admissions['admittime'] = pd.to_datetime(admissions['admittime'], errors='coerce')\n",
    "admissions['dischtime'] = pd.to_datetime(admissions['dischtime'], errors='coerce')\n",
    "\n",
    "admissions['dischtime'] = admissions['dischtime'].fillna(admissions['admittime'])\n",
    "\n",
    "def expand_admission_row(row):\n",
    "    adm_date = row['admittime'].normalize().date()\n",
    "    dis_date = row['dischtime'].normalize().date()\n",
    "    n_days = (dis_date - adm_date).days + 1\n",
    "    if n_days <= 0:\n",
    "        n_days = 1\n",
    "    rows = []\n",
    "    for d in range(n_days):\n",
    "        new = {\n",
    "            'subject_id': row['subject_id'],\n",
    "            'hadm_id': row['hadm_id'],\n",
    "            'day_index': int(d),\n",
    "            'admittime': row['admittime'],\n",
    "            'dischtime': row['dischtime'],\n",
    "            'deathtime': row['deathtime'],\n",
    "            'admission_type': row.get('admission_type', np.nan),\n",
    "            'admit_provider_id': row.get('admit_provider_id', np.nan),\n",
    "            'admission_location': row.get('admission_location', np.nan),\n",
    "            'discharge_location': row.get('discharge_location', np.nan),\n",
    "            'insurance': row.get('insurance', np.nan),\n",
    "            'language': row.get('language', np.nan),\n",
    "            'marital_status': row.get('marital_status', np.nan),\n",
    "            'race': row.get('race', np.nan),\n",
    "            'edregtime': row.get('edregtime', pd.NaT),\n",
    "            'edouttime': row.get('edouttime', pd.NaT),\n",
    "            'hospital_expire_flag': row.get('hospital_expire_flag', np.nan)\n",
    "        }\n",
    "        rows.append(new)\n",
    "    return rows\n",
    "\n",
    "expanded = []\n",
    "for _, r in admissions.iterrows():\n",
    "    expanded.extend(expand_admission_row(r))\n",
    "\n",
    "merged_initial = pd.DataFrame(expanded)\n",
    "merged_initial[['subject_id','hadm_id','day_index']] = merged_initial[['subject_id','hadm_id','day_index']].astype('Int64')\n",
    "\n",
    "print(\"Expanded admissions -> rows:\", merged_initial.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv(\"admissions_expanded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad502bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_path = Path(\"icustays.csv\")\n",
    "if not icu_path.exists():\n",
    "    raise FileNotFoundError(f\"icustays.csv not found at {icu_path.resolve()}  -- put the file next to admissions.csv\")\n",
    "\n",
    "icustays = pd.read_csv(icu_path, low_memory=False, parse_dates=['intime','outtime'])\n",
    "\n",
    "for col in ['subject_id','hadm_id','intime','outtime']:\n",
    "    if col not in icustays.columns:\n",
    "        raise KeyError(f\"Expected column '{col}' in icustays.csv but it is missing.\")\n",
    "\n",
    "optional_cols = ['stay_id','first_careunit','last_careunit','los']\n",
    "for c in optional_cols:\n",
    "    if c not in icustays.columns:\n",
    "        icustays[c] = pd.NA\n",
    "\n",
    "merged_with_icu = merged_initial.copy().reset_index(drop=False).rename(columns={'index':'row_id'})\n",
    "for col in ['stay_id_icu','icustay_intime','icustay_outtime','first_careunit_icu','last_careunit_icu','los_icu']:\n",
    "    if col not in merged_with_icu.columns:\n",
    "        merged_with_icu[col] = pd.NA\n",
    "\n",
    "if 'admittime' not in merged_with_icu.columns:\n",
    "    raise KeyError(\"merged_initial must contain 'admittime' column\")\n",
    "\n",
    "merged_with_icu['admittime'] = pd.to_datetime(merged_with_icu['admittime'], errors='coerce')\n",
    "merged_with_icu['day_index_int'] = merged_with_icu['day_index'].fillna(0).astype(int)\n",
    "merged_with_icu['row_date'] = merged_with_icu['admittime'].dt.normalize() + pd.to_timedelta(merged_with_icu['day_index_int'], unit='D')\n",
    "\n",
    "icustays['intime'] = pd.to_datetime(icustays['intime'], errors='coerce')\n",
    "icustays['outtime'] = pd.to_datetime(icustays['outtime'], errors='coerce').fillna(icustays['intime'])\n",
    "icustays['intime_norm'] = icustays['intime'].dt.normalize()\n",
    "icustays['outtime_norm'] = icustays['outtime'].dt.normalize()\n",
    "\n",
    "icu_keep = ['subject_id','hadm_id','stay_id','intime','outtime','intime_norm','outtime_norm','first_careunit','last_careunit','los']\n",
    "candidate = merged_with_icu.merge(icustays[icu_keep], on=['subject_id','hadm_id'], how='left', suffixes=('','_icu'))\n",
    "\n",
    "mask_in_icu = (candidate['row_date'] >= candidate['intime_norm']) & (candidate['row_date'] <= candidate['outtime_norm'])\n",
    "candidate['in_icu'] = mask_in_icu.fillna(False)\n",
    "\n",
    "matched = candidate[candidate['in_icu']].copy()\n",
    "if not matched.empty:\n",
    "    matched = matched.sort_values(by=['row_id','intime'])\n",
    "    first_matches = matched.groupby('row_id', as_index=False).first()\n",
    "    map_cols = {\n",
    "        'stay_id':'stay_id_icu',\n",
    "        'intime':'icustay_intime',\n",
    "        'outtime':'icustay_outtime',\n",
    "        'first_careunit':'first_careunit_icu',\n",
    "        'last_careunit':'last_careunit_icu',\n",
    "        'los':'los_icu'\n",
    "    }\n",
    "    for src, dst in map_cols.items():\n",
    "        mapping = first_matches.set_index('row_id')[src]\n",
    "        merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), dst] = merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), 'row_id'].map(mapping)\n",
    "    assigned_count = len(first_matches)\n",
    "else:\n",
    "    assigned_count = 0\n",
    "\n",
    "merged_with_icu = merged_with_icu.drop(columns=['day_index_int','row_date'])\n",
    "\n",
    "print(f\"ICU assignment complete. Rows where ICU info filled: {int(assigned_count)}\")\n",
    "print(merged_with_icu[merged_with_icu['stay_id_icu'].notna()].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'stay_id_icu': 'stay_id',\n",
    "    'first_careunit_icu': 'first_careunit',\n",
    "    'last_careunit_icu': 'last_careunit',\n",
    "    'icustay_intime': 'icustays_intime',\n",
    "    'icustay_outtime': 'icustays_outtime',\n",
    "    'los_icu': 'los'\n",
    "}\n",
    "merged_with_icu = merged_with_icu.rename(columns=rename_map)\n",
    "\n",
    "icu_cols_ordered = ['stay_id', 'first_careunit', 'last_careunit', 'icustays_intime', 'icustays_outtime', 'los']\n",
    "\n",
    "other_cols = [c for c in merged_with_icu.columns if c not in icu_cols_ordered]\n",
    "\n",
    "merged_with_icu = merged_with_icu[other_cols + icu_cols_ordered]\n",
    "\n",
    "print(\"Renaming & reordering complete.\")\n",
    "print(merged_with_icu[icu_cols_ordered].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06434530",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e880f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f318e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv('merged_initial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.to_csv('merged_with_icu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.head(100).to_csv('merged_with_icu_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanco_path = Path(\"all_vanco.csv\")\n",
    "if not vanco_path.exists():\n",
    "    raise FileNotFoundError(f\"all_vanco.csv not found at {vanco_path.resolve()}\")\n",
    "\n",
    "all_vanco = pd.read_csv(vanco_path, low_memory=False, parse_dates=['charttime'])\n",
    "\n",
    "all_vanco['subject_id'] = pd.to_numeric(all_vanco['subject_id'], errors='coerce').astype('Int64')\n",
    "all_vanco['hadm_id'] = pd.to_numeric(all_vanco['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "def resolve_numeric(row):\n",
    "    v = row.get('value')\n",
    "    vn = row.get('valuenum')\n",
    "    if pd.isna(v) or str(v).strip() in ['', '___', 'NaN', 'nan']:\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    s = str(v).strip().replace(',', '')\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "all_vanco['resolved_val'] = all_vanco.apply(resolve_numeric, axis=1)\n",
    "\n",
    "merged_with_vanco = merged_with_icu.copy()\n",
    "if 'admittime' not in merged_with_vanco.columns:\n",
    "    raise KeyError(f\"merged_with_vanco must contain 'admittime' column before merging labs.\")\n",
    "merged_with_vanco['admittime'] = pd.to_datetime(merged_with_vanco['admittime'], errors='coerce')\n",
    "\n",
    "admit_map = merged_with_vanco.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time']).dt.normalize()\n",
    "\n",
    "all_vanco = all_vanco.merge(admit_map[['subject_id','hadm_id','admit_date']], on=['subject_id','hadm_id'], how='left')\n",
    "\n",
    "missing_admit = all_vanco['admit_date'].isna().sum()\n",
    "if missing_admit:\n",
    "    print(f\"Warning: {missing_admit} all_vanco rows have no matching admission (admit_date missing) and will be skipped.\")\n",
    "all_vanco = all_vanco[all_vanco['admit_date'].notna()].copy()\n",
    "\n",
    "all_vanco['chart_date'] = pd.to_datetime(all_vanco['charttime'], errors='coerce').dt.normalize()\n",
    "all_vanco['day_index_lab'] = (all_vanco['chart_date'] - all_vanco['admit_date']).dt.days.fillna(0).astype(int)\n",
    "all_vanco.loc[all_vanco['day_index_lab'] < 0, 'day_index_lab'] = 0\n",
    "\n",
    "group_cols = ['subject_id','hadm_id','day_index_lab']\n",
    "usable = all_vanco[~all_vanco['resolved_val'].isna()].copy()\n",
    "if usable.empty:\n",
    "    print(\"No usable numeric vanco values found to aggregate.\")\n",
    "    daily_vanco = pd.DataFrame(columns=['subject_id','hadm_id','day_index_lab',\n",
    "                                       'charttime','value','valuenum','valueuom','flag','resolved_val'])\n",
    "else:\n",
    "    idx = usable.groupby(group_cols)['resolved_val'].idxmax()\n",
    "    daily_vanco = usable.loc[idx].copy()\n",
    "\n",
    "daily_vanco = daily_vanco.rename(columns={\n",
    "    'charttime':'all_vanco_charttime',\n",
    "    'value':'all_vanco_value',\n",
    "    'valuenum':'all_vanco_valuenum',\n",
    "    'valueuom':'all_vanco_valueuom',\n",
    "    'flag':'all_vanco_flag',\n",
    "    'day_index_lab':'day_index'\n",
    "})\n",
    "\n",
    "merge_cols = ['subject_id','hadm_id','day_index',\n",
    "              'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']\n",
    "daily_vanco = daily_vanco[merge_cols]\n",
    "\n",
    "daily_vanco['subject_id'] = daily_vanco['subject_id'].astype('Int64')\n",
    "daily_vanco['hadm_id'] = daily_vanco['hadm_id'].astype('Int64')\n",
    "daily_vanco['day_index'] = daily_vanco['day_index'].astype('Int64')\n",
    "\n",
    "merged_with_vanco = merged_with_vanco.merge(daily_vanco, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "print(f\"all_vanco merged -> rows with vanco info: {int(merged_with_vanco['all_vanco_charttime'].notna().sum())}\")\n",
    "\n",
    "preview = merged_with_vanco[merged_with_vanco['all_vanco_charttime'].notna()].head(20)\n",
    "print(preview[['subject_id','hadm_id','day_index',\n",
    "               'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_vanco.head(100).to_csv('merged_with_vanco_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "chartevents = pd.read_csv(chartevents_path, nrows=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e61e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(chartevents_path,nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chartevents.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f99c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(f\"chartevents.csv not found at {chartevents_path.resolve()}\")\n",
    "\n",
    "usecols = ['subject_id','hadm_id','itemid','charttime','value','valuenum']\n",
    "chunksize = 200_000\n",
    "\n",
    "total_counts = defaultdict(int)\n",
    "present_counts = defaultdict(int)\n",
    "\n",
    "reader = pd.read_csv(chartevents_path, usecols=usecols, chunksize=chunksize, low_memory=True)\n",
    "\n",
    "chunk_i = 0\n",
    "for chunk in reader:\n",
    "    chunk_i += 1\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    present_mask = ~chunk['valuenum'].isna()\n",
    "\n",
    "    need_check = chunk['valuenum'].isna()\n",
    "    if need_check.any():\n",
    "        vals = chunk.loc[need_check, 'value'].astype(str).str.strip()\n",
    "        good = ~vals.isin([\"\", \"___\", \"NaN\", \"nan\", \"None\", \"none\"])\n",
    "        present_mask.loc[need_check] = good.values\n",
    "\n",
    "    grp_total = chunk.groupby('itemid').size()\n",
    "    grp_present = present_mask.groupby(chunk['itemid']).sum()\n",
    "\n",
    "    for item, cnt in grp_total.items():\n",
    "        total_counts[int(item)] += int(cnt)\n",
    "    for item, cnt in grp_present.items():\n",
    "        if pd.isna(item):\n",
    "            continue\n",
    "        present_counts[int(item)] += int(cnt)\n",
    "\n",
    "    if chunk_i % 10 == 0:\n",
    "        print(f\"Processed {chunk_i*chunksize:,} rows...\")\n",
    "\n",
    "itemids = sorted(set(list(total_counts.keys()) + list(present_counts.keys())))\n",
    "rows = []\n",
    "for iid in itemids:\n",
    "    tot = total_counts.get(iid, 0)\n",
    "    pres = present_counts.get(iid, 0)\n",
    "    miss = tot - pres\n",
    "    frac = pres / tot if tot > 0 else 0.0\n",
    "    rows.append((iid, tot, pres, miss, frac))\n",
    "\n",
    "missingness_df = pd.DataFrame(rows, columns=['itemid','total_count','present_count','missing_count','present_fraction'])\n",
    "missingness_df = missingness_df.sort_values(by='present_fraction', ascending=False).reset_index(drop=True)\n",
    "missingness_df.to_csv(\"chartevents_itemid_missingness.csv\", index=False)\n",
    "print(\"Saved chartevents_itemid_missingness.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chartevents_itemid_missingness.csv\")\n",
    "\n",
    "drop_ids = df.loc[df['present_fraction'] < 0.5, 'itemid'].tolist()\n",
    "\n",
    "print(f\"تعداد itemid هایی که باید drop بشن: {len(drop_ids)}\")\n",
    "print(drop_ids[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c76ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(\"chartevents.csv\", chunksize=2_000_000)\n",
    "out_path = \"chartevents_missing50_dropped.csv\"\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "total_dropped = 0\n",
    "total_written = 0\n",
    "\n",
    "for i, chunk in enumerate(reader, start=1):\n",
    "    before = len(chunk)\n",
    "    filtered = chunk.loc[~chunk['itemid'].isin(drop_ids)]\n",
    "    after = len(filtered)\n",
    "    \n",
    "    filtered.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    \n",
    "    total_rows += before\n",
    "    total_dropped += before - after\n",
    "    total_written += after\n",
    "    print(f\"Chunk {i}: rows={before:,}, dropped={before - after:,}, kept={after:,}\")\n",
    "\n",
    "print(\"---- DONE ----\")\n",
    "print(f\"Total rows processed: {total_rows:,}\")\n",
    "print(f\"Total rows dropped:   {total_dropped:,}\")\n",
    "print(f\"Total rows written:   {total_written:,}\")\n",
    "print(\"✅ فایل نهایی ذخیره شد:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"d_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aae3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"d_items.csv\"\n",
    "out_path = \"d_items_chartevents_missing50_dropped.csv\"\n",
    "\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "filtered = df.loc[\n",
    "    (df[\"linksto\"] == \"chartevents\") & \n",
    "    (~df[\"itemid\"].isin(drop_ids))\n",
    "]\n",
    "\n",
    "filtered.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"✅ d_items filtered and saved:\", out_path)\n",
    "print(\"before:\", len(df), \"after:\", len(filtered), \"drop:\", len(df) - len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "merged_initial_file = Path(\"merged_initial.csv\")\n",
    "out_path = Path(\"merged_initial_with_items_cols.csv\")\n",
    "\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "cols_to_add = [str(i) for i in itemids]\n",
    "print(\"Will add columns (count):\", len(cols_to_add))\n",
    "\n",
    "try:\n",
    "    merged_initial\n",
    "    print(\"Using merged_initial from memory (existing DataFrame). rows:\", len(merged_initial))\n",
    "except NameError:\n",
    "    if not merged_initial_file.exists():\n",
    "        raise FileNotFoundError(f\"merged_initial not in memory and file {merged_initial_file} not found.\")\n",
    "    print(\"Loading merged_initial from disk:\", merged_initial_file)\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "    print(\"Loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "n_rows = len(merged_initial)\n",
    "added = 0\n",
    "for c in cols_to_add:\n",
    "    if c not in merged_initial.columns:\n",
    "        merged_initial[c] = pd.Series([pd.NA] * n_rows, dtype=\"object\")\n",
    "        added += 1\n",
    "print(f\"Added {added} new columns. Total columns now: {len(merged_initial.columns)}\")\n",
    "\n",
    "chunksize = 10000\n",
    "first = True\n",
    "written = 0\n",
    "for start in range(0, n_rows, chunksize):\n",
    "    end = min(start + chunksize, n_rows)\n",
    "    chunk = merged_initial.iloc[start:end]\n",
    "    chunk.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    written += len(chunk)\n",
    "    print(f\"Wrote rows {start:,}..{end-1:,} -> {len(chunk):,} rows\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", written, \"Columns in output:\", len(merged_initial.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57308ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_initial.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "merged_initial_file = None\n",
    "chunksize = 500_000\n",
    "save_after = False\n",
    "\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(chartevents_path)\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "keep_itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "keep_itemids_set = set(keep_itemids)\n",
    "print(\"Keep itemids count:\", len(keep_itemids))\n",
    "\n",
    "try:\n",
    "    merged_initial\n",
    "except NameError:\n",
    "    if merged_initial_file is None:\n",
    "        raise NameError(\"merged_initial not in memory. Set merged_initial_file path or load it.\")\n",
    "    print(\"Loading merged_initial from disk...\")\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime'])\n",
    "    print(\"loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "for iid in keep_itemids:\n",
    "    col = str(iid)\n",
    "    if col not in merged_initial.columns:\n",
    "        merged_initial[col] = pd.Series([pd.NA] * len(merged_initial), dtype=\"object\")\n",
    "\n",
    "admit_map = merged_initial.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "\n",
    "merged_initial_index_map = {}\n",
    "for idx, row in merged_initial[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    key = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "    merged_initial_index_map[key] = idx\n",
    "\n",
    "print(\"Admit map keys:\", len(admit_dict), \"merged rows map size:\", len(merged_initial_index_map))\n",
    "\n",
    "reader = pd.read_csv(chartevents_path, usecols=['subject_id','hadm_id','itemid','charttime','value','valuenum'],\n",
    "                     parse_dates=['charttime'], chunksize=chunksize, low_memory=True)\n",
    "\n",
    "total_assigned = 0\n",
    "chunk_no = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_no += 1\n",
    "    print(f\"\\n--- Processing chunk {chunk_no} (rows: {len(chunk)}) ---\")\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    chunk = chunk[chunk['itemid'].isin(keep_itemids)]\n",
    "    if chunk.empty:\n",
    "        print(\"no relevant itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    chunk['subject_id'] = chunk['subject_id'].astype(int)\n",
    "    chunk['hadm_id'] = chunk['hadm_id'].astype(int)\n",
    "\n",
    "    def lookup_admit_date(s):\n",
    "        return admit_dict.get((int(s.subject_id), int(s.hadm_id)), pd.NaT)\n",
    "    keys = list(zip(chunk['subject_id'].astype(int), chunk['hadm_id'].astype(int)))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    chunk = chunk[chunk['admit_date'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(\"no rows with admit_date in this chunk\")\n",
    "        continue\n",
    "\n",
    "    chunk['chart_date'] = chunk['charttime'].dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    chunk['numeric_val'] = pd.to_numeric(chunk['valuenum'], errors='coerce')\n",
    "    mask_num_missing = chunk['numeric_val'].isna()\n",
    "    if mask_num_missing.any():\n",
    "        parsed = pd.to_numeric(chunk.loc[mask_num_missing, 'value'].astype(str).str.replace(',',''), errors='coerce')\n",
    "        chunk.loc[mask_num_missing, 'numeric_val'] = parsed\n",
    "\n",
    "    chunk['value_raw'] = chunk['value'].astype(str)\n",
    "\n",
    "    grp_keys = ['subject_id','hadm_id','day_index','itemid']\n",
    "\n",
    "    numeric_rows = chunk[chunk['numeric_val'].notna()].copy()\n",
    "    if not numeric_rows.empty:\n",
    "        grp_num = numeric_rows.groupby(grp_keys, as_index=False)['numeric_val'].max()\n",
    "        grp_num = grp_num.rename(columns={'numeric_val':'agg_value_num'})\n",
    "    else:\n",
    "        grp_num = pd.DataFrame(columns=grp_keys + ['agg_value_num'])\n",
    "\n",
    "    chunk_sorted = chunk.sort_values('charttime')\n",
    "    grp_last = chunk_sorted.groupby(grp_keys, as_index=False).last()[grp_keys + ['value_raw','charttime']]\n",
    "    grp_last = grp_last.rename(columns={'value_raw':'agg_value_text', 'charttime':'agg_time_text'})\n",
    "\n",
    "    merged_grps = pd.merge(grp_last, grp_num, on=grp_keys, how='left')\n",
    "\n",
    "    def pick_final_val(row):\n",
    "        if pd.notna(row.get('agg_value_num')):\n",
    "            return row['agg_value_num']\n",
    "        else:\n",
    "            v = row.get('agg_value_text')\n",
    "            if pd.isna(v) or v in (\"nan\",\"None\",\"NoneType\",\"NA\",\"<NA>\"):\n",
    "                return pd.NA\n",
    "            return v\n",
    "\n",
    "    merged_grps['final_value'] = merged_grps.apply(pick_final_val, axis=1)\n",
    "\n",
    "    assigned = 0\n",
    "    for _, r in merged_grps.iterrows():\n",
    "        key = (int(r['subject_id']), int(r['hadm_id']), int(r['day_index']))\n",
    "        row_idx = merged_initial_index_map.get(key)\n",
    "        if row_idx is None:\n",
    "            continue\n",
    "        itemid_col = str(int(r['itemid']))\n",
    "        val = r['final_value']\n",
    "        merged_initial.at[row_idx, itemid_col] = val\n",
    "        assigned += 1\n",
    "\n",
    "    total_assigned += assigned\n",
    "    print(f\"Chunk {chunk_no}: groups aggregated = {len(merged_grps)}, assigned = {assigned}, total_assigned so far = {total_assigned}\")\n",
    "\n",
    "    del chunk, chunk_sorted, numeric_rows, grp_num, grp_last, merged_grps\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- ALL CHUNKS PROCESSED ---\")\n",
    "print(\"Total assigned cells:\", total_assigned)\n",
    "\n",
    "out_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "n_rows = len(merged_initial)\n",
    "write_chunk = 20000\n",
    "first = True\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_initial.iloc[start:end].to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved final to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "merged_with_chartevents_filled = pd.read_csv(merged_with_chartevents_filled_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82151eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "output_path = Path(\"chartevents_missing50_dropped_filtered_hadm_id_23282506.csv\")\n",
    "\n",
    "chunksize = 2_000_000\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize, low_memory=False)):\n",
    "    filtered = chunk[chunk['hadm_id'] == 23282506]\n",
    "    if not filtered.empty:\n",
    "        filtered.to_csv(output_path, mode='w' if first else 'a',\n",
    "                        index=False, header=first)\n",
    "        first = False\n",
    "        total_rows += len(filtered)\n",
    "        print(f\"Chunk {i}: wrote {len(filtered)} rows (total so far: {total_rows})\")\n",
    "\n",
    "print(\"Done! Final rows written:\", total_rows)\n",
    "print(\"Output file:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40055979",
   "metadata": {},
   "outputs": [],
   "source": [
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "in_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "out_path = Path(\"merged_with_chartevents_filled_renamed.csv\")\n",
    "chunksize = 20_000\n",
    "max_name_len = 80\n",
    "\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(in_path)\n",
    "\n",
    "d = pd.read_csv(ditems_path, usecols=['itemid','label','abbreviation'], dtype=str)\n",
    "d['itemid'] = d['itemid'].str.strip()\n",
    "d['label'] = d['label'].fillna('').astype(str).str.strip()\n",
    "d['abbreviation'] = d['abbreviation'].fillna('').astype(str).str.strip()\n",
    "\n",
    "d['chosen'] = d.apply(lambda r: r['abbreviation'] if r['abbreviation']!='' else (r['label'] if r['label']!='' else ''), axis=1)\n",
    "\n",
    "def sanitize_name(s):\n",
    "    if pd.isna(s) or s is None:\n",
    "        return ''\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w\\-]', '', s)\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s[:max_name_len]\n",
    "    return s\n",
    "\n",
    "name_map = {}\n",
    "used = set()\n",
    "\n",
    "for _, row in d.iterrows():\n",
    "    iid = row['itemid']\n",
    "    chosen = row['chosen']\n",
    "    if chosen == '':\n",
    "        base = f\"item_{iid}\"\n",
    "    else:\n",
    "        base = sanitize_name(chosen)\n",
    "        if base == '':\n",
    "            base = f\"item_{iid}\"\n",
    "    name = base\n",
    "    if name in used:\n",
    "        name = f\"{base}__{iid}\"\n",
    "    counter = 1\n",
    "    while name in used:\n",
    "        name = f\"{base}__{iid}_{counter}\"\n",
    "        counter += 1\n",
    "    used.add(name)\n",
    "    name_map[str(iid)] = name\n",
    "\n",
    "orig_header = pd.read_csv(in_path, nrows=0).columns.tolist()\n",
    "new_header = []\n",
    "conflicts = 0\n",
    "for col in orig_header:\n",
    "    new_col = col\n",
    "    col_str = str(col).strip()\n",
    "    if col_str in name_map:\n",
    "        new_col = \"chartevents_\" + name_map[col_str]\n",
    "    else:\n",
    "        try:\n",
    "            icol = str(int(float(col_str)))\n",
    "            if icol in name_map:\n",
    "                new_col = name_map[icol]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if new_col in new_header:\n",
    "        conflicts += 1\n",
    "        new_col = f\"{new_col}__orig_{sanitize_name(col_str)}\"\n",
    "        k = 1\n",
    "        while new_col in new_header:\n",
    "            new_col = f\"{new_col}_{k}\"; k += 1\n",
    "    new_header.append(new_col)\n",
    "\n",
    "print(f\"Prepared header mapping. Total cols: {len(orig_header)}, conflicts resolved: {conflicts}\")\n",
    "\n",
    "sample_map = {k: name_map[k] for k in list(name_map)[:10]}\n",
    "print(\"sample itemid->name (first 10):\", sample_map)\n",
    "\n",
    "first = True\n",
    "rows_written = 0\n",
    "for i, chunk in enumerate(pd.read_csv(in_path, chunksize=chunksize, low_memory=False)):\n",
    "    chunk.columns = new_header\n",
    "    chunk.to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    rows_written += len(chunk)\n",
    "    print(f\"Chunk {i+1}: wrote {len(chunk):,} rows (total {rows_written:,})\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", rows_written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed_path = Path(\"merged_with_chartevents_filled_renamed.csv\")\n",
    "merged_with_chartevents_filled_renamed = pd.read_csv(merged_with_chartevents_filled_renamed_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents_path = Path(\"datetimeevents.csv\")\n",
    "datetimeevents = pd.read_csv(datetimeevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252369fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents.head(100).to_csv('test_datetimeevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5685e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyevents_path = Path(\"microbiologyevents.csv\")\n",
    "microbiologyevents = pd.read_csv(microbiologyevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyevents.head(100).to_csv('test_microbiologyevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "DIAGNOSIS_CSV = Path(\"diagnoses_icd.csv\")\n",
    "DICT_CSV = Path(\"d_icd_diagnoses.csv\")\n",
    "MERGED_INITIAL_CSV = Path(\"admissions_expanded.csv\")   # input (admission x day)\n",
    "OUT_CSV = Path(\"merged_with_diagnoses.csv\")      # output\n",
    "CHUNK_DIAG = 200_000        # chunk size for diagnoses reading (diagnoses table is usually small)\n",
    "CHUNK_WRITE = 20_000        # chunk size for writing merged_initial with diagnoses\n",
    "TOP_K = 5                   # produce diag_1..diag_K columns (set to 0 to disable)\n",
    "\n",
    "# --- Helpers ---\n",
    "def normalize_code(code):\n",
    "    \"\"\"Normalize ICD code strings for matching: str, strip, uppercase, remove dots.\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return \"\"\n",
    "    s = str(code).strip().upper()\n",
    "    s = s.replace(\".\", \"\")\n",
    "    return s\n",
    "\n",
    "def try_lookup_description(code, version, dict_map):\n",
    "    \"\"\"Attempt to find long_title for (code, version) with fallback strategies.\"\"\"\n",
    "    key = (code, str(int(version)) if pd.notna(version) else str(version))\n",
    "    if key in dict_map:\n",
    "        return dict_map[key]\n",
    "    # fallback: remove leading zeros from both sides (e.g., '0010' -> '10')\n",
    "    code_nolead = code.lstrip(\"0\")\n",
    "    key2 = (code_nolead, key[1])\n",
    "    if key2 in dict_map:\n",
    "        return dict_map[key2]\n",
    "    # fallback: if dict keys have leading zeros and code doesn't, try to left-pad to 4 (common for old formats)\n",
    "    if code.isdigit():\n",
    "        for pad in (3,4,5):\n",
    "            kp = (code.zfill(pad), key[1])\n",
    "            if kp in dict_map:\n",
    "                return dict_map[kp]\n",
    "    return pd.NA\n",
    "\n",
    "# --- Step 1: load ICD dictionary into memory (small) ---\n",
    "if not DICT_CSV.exists():\n",
    "    raise FileNotFoundError(f\"{DICT_CSV} not found. Place d_icd_diagnoses.csv next to this script.\")\n",
    "\n",
    "dict_df = pd.read_csv(DICT_CSV, dtype=str)  # icd_code, icd_version, long_title\n",
    "# normalize dict codes:\n",
    "dict_df['icd_code_norm'] = dict_df['icd_code'].astype(str).apply(normalize_code)\n",
    "dict_df['icd_version_norm'] = dict_df['icd_version'].astype(str).str.strip()\n",
    "# build mapping (code_norm, version) -> long_title\n",
    "dict_map = dict(((row.icd_code_norm, row.icd_version_norm), row.long_title) for row in dict_df.itertuples(index=False))\n",
    "\n",
    "print(f\"Loaded ICD dictionary rows: {len(dict_df)}\")\n",
    "\n",
    "# --- Step 2: read diagnoses_icd in chunks and accumulate per-admission lists ---\n",
    "if not DIAGNOSIS_CSV.exists():\n",
    "    raise FileNotFoundError(f\"{DIAGNOSIS_CSV} not found. Place diagnoses_icd.csv next to this script.\")\n",
    "\n",
    "acc = defaultdict(list)   # key -> list of (seq_num (int), icd_code_norm (str), icd_version)\n",
    "rows_seen = 0\n",
    "for chunk in pd.read_csv(DIAGNOSIS_CSV, chunksize=CHUNK_DIAG, dtype=str, low_memory=False):\n",
    "    # ensure required columns exist\n",
    "    for col in (\"subject_id\",\"hadm_id\",\"seq_num\",\"icd_code\",\"icd_version\"):\n",
    "        if col not in chunk.columns:\n",
    "            raise KeyError(f\"Expected column '{col}' in diagnoses_icd.csv but missing.\")\n",
    "    # normalize and iterate\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    chunk['seq_num'] = pd.to_numeric(chunk['seq_num'], errors='coerce').fillna(99999).astype(int)\n",
    "    chunk['icd_code_norm'] = chunk['icd_code'].astype(str).apply(normalize_code)\n",
    "    chunk['icd_version_norm'] = chunk['icd_version'].astype(str).str.strip()\n",
    "\n",
    "    for r in chunk.itertuples(index=False):\n",
    "        # skip if missing hadm or subject\n",
    "        if pd.isna(r.subject_id) or pd.isna(r.hadm_id):\n",
    "            continue\n",
    "        key = (int(r.subject_id), int(r.hadm_id))\n",
    "        acc[key].append((int(r.seq_num), r.icd_code_norm, r.icd_version_norm))\n",
    "        rows_seen += 1\n",
    "    print(f\"Processed diagnoses rows so far: {rows_seen}\", end='\\r')\n",
    "\n",
    "print(f\"\\nTotal diagnosis rows processed: {rows_seen}; unique admissions with diagnoses: {len(acc)}\")\n",
    "\n",
    "# --- Step 3: build per-admission aggregate DataFrame ---\n",
    "agg_rows = []\n",
    "for (subj, hadm), entries in acc.items():\n",
    "    # sort by seq_num ascending\n",
    "    entries_sorted = sorted(entries, key=lambda x: (x[0] if x[0] is not None else 99999))\n",
    "    codes = [e[1] for e in entries_sorted if e[1] != \"\"]\n",
    "    versions = [e[2] for e in entries_sorted]\n",
    "    # lookup descriptions (preserve order)\n",
    "    descs = [ try_lookup_description(c, v, dict_map) if c != \"\" else pd.NA for c,v in zip(codes, versions) ]\n",
    "    n = len(codes)\n",
    "    primary_code = codes[0] if n >= 1 else pd.NA\n",
    "    primary_desc = descs[0] if n >= 1 else pd.NA\n",
    "    # top-K split\n",
    "    top_codes = {}\n",
    "    top_descs = {}\n",
    "    for k in range(1, TOP_K+1):\n",
    "        if n >= k:\n",
    "            top_codes[f\"diag_{k}_code\"] = codes[k-1]\n",
    "            top_descs[f\"diag_{k}_desc\"] = descs[k-1]\n",
    "        else:\n",
    "            top_codes[f\"diag_{k}_code\"] = pd.NA\n",
    "            top_descs[f\"diag_{k}_desc\"] = pd.NA\n",
    "\n",
    "    agg_rows.append({\n",
    "        \"subject_id\": int(subj),\n",
    "        \"hadm_id\": int(hadm),\n",
    "        \"diag_n\": int(n),\n",
    "        \"diag_codes\": \";\".join(codes) if codes else pd.NA,\n",
    "        \"diag_descs\": \";\".join([str(d) for d in descs]) if descs else pd.NA,\n",
    "        \"primary_diag_code\": primary_code,\n",
    "        \"primary_diag_desc\": primary_desc,\n",
    "        **top_codes,\n",
    "        **top_descs\n",
    "    })\n",
    "\n",
    "diag_df = pd.DataFrame(agg_rows)\n",
    "# ensure dtypes\n",
    "if not diag_df.empty:\n",
    "    diag_df['subject_id'] = diag_df['subject_id'].astype('Int64')\n",
    "    diag_df['hadm_id'] = diag_df['hadm_id'].astype('Int64')\n",
    "    diag_df['diag_n'] = diag_df['diag_n'].astype('Int64')\n",
    "\n",
    "print(\"Built diag_df with rows:\", len(diag_df))\n",
    "\n",
    "# --- Step 4: merge diag_df into merged_initial.csv in chunks (so we don't load merged_initial fully) ---\n",
    "if not MERGED_INITIAL_CSV.exists():\n",
    "    raise FileNotFoundError(f\"{MERGED_INITIAL_CSV} not found. Place merged_initial.csv next to this script.\")\n",
    "\n",
    "first_write = True\n",
    "written = 0\n",
    "for chunk in pd.read_csv(MERGED_INITIAL_CSV, chunksize=CHUNK_WRITE, parse_dates=['admittime'], low_memory=False):\n",
    "    # ensure keys have correct dtype\n",
    "    if 'subject_id' in chunk.columns:\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    if 'hadm_id' in chunk.columns:\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "    merged_chunk = chunk.merge(diag_df, on=['subject_id','hadm_id'], how='left')\n",
    "    # if diag_df is empty, the merge will just add nothing; that's okay.\n",
    "\n",
    "    merged_chunk.to_csv(OUT_CSV, mode='w' if first_write else 'a', index=False, header=first_write)\n",
    "    first_write = False\n",
    "    written += len(merged_chunk)\n",
    "    print(f\"Wrote merged rows: {written}\", end='\\r')\n",
    "    del chunk, merged_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nDone. Output saved to: {OUT_CSV} (rows written: {written})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9258013",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(MERGED_INITIAL_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02496607",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_diagnoses = pd.read_csv(\"merged_with_diagnoses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_diagnoses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_diagnoses.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c838f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a743cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures_icd = pd.read_csv(\"procedures_icd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures_icd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_icd_procedures = pd.read_csv(\"d_icd_procedures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a618761",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_icd_procedures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "procedures_path = Path(\"procedures_icd.csv\")\n",
    "dprocedures_path = Path(\"d_icd_procedures.csv\")\n",
    "merged_initial_path = Path(\"admissions_expanded.csv\")\n",
    "intermediate_chunks_path = Path(\"procedures_daily_chunks.csv\")\n",
    "final_daily_path = Path(\"procedures_daily_final.csv\")\n",
    "out_merged_path = Path(\"merged_with_procedures.csv\")\n",
    "\n",
    "chunksize = 500_000   # tune to your environment\n",
    "write_chunk = 20000\n",
    "\n",
    "# === 0) sanity checks ===\n",
    "for p in (procedures_path, dprocedures_path, merged_initial_path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Required file not found: {p.resolve()}\")\n",
    "\n",
    "# === 1) build admit_date lookup from merged_initial ===\n",
    "print(\"Loading merged_initial admissions (admit_time -> admit_date map)...\")\n",
    "mi_cols = ['subject_id', 'hadm_id', 'admittime']\n",
    "mi = pd.read_csv(merged_initial_path, usecols=mi_cols, parse_dates=['admittime'], low_memory=False)\n",
    "mi['subject_id'] = pd.to_numeric(mi['subject_id'], errors='coerce').astype('Int64')\n",
    "mi['hadm_id'] = pd.to_numeric(mi['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "# take first admittime per (subject_id, hadm_id)\n",
    "admit_map = mi.groupby(['subject_id', 'hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "\n",
    "# make a dict: (int(subject_id), int(hadm_id)) -> admit_date (Timestamp) for fast lookup\n",
    "admit_dict = {}\n",
    "for r in admit_map.itertuples(index=False):\n",
    "    try:\n",
    "        key = (int(r.subject_id), int(r.hadm_id))\n",
    "    except Exception:\n",
    "        continue\n",
    "    admit_dict[key] = r.admit_date\n",
    "print(\"Admit map size:\", len(admit_dict))\n",
    "\n",
    "del mi, admit_map; gc.collect()\n",
    "\n",
    "# === 2) chunked read of procedures_icd -> per-chunk daily aggregates ===\n",
    "print(\"Streaming procedures_icd in chunks and writing per-chunk daily aggregates...\")\n",
    "first_out = True\n",
    "total_skipped_no_admit = 0\n",
    "total_rows_processed = 0\n",
    "reader = pd.read_csv(procedures_path,\n",
    "                     usecols=['subject_id','hadm_id','seq_num','chartdate','icd_code','icd_version'],\n",
    "                     parse_dates=['chartdate'],\n",
    "                     chunksize=chunksize,\n",
    "                     low_memory=True)\n",
    "\n",
    "for chunk_i, chunk in enumerate(reader, start=1):\n",
    "    total_rows_processed += len(chunk)\n",
    "    # normalize ids\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # drop rows with missing ids\n",
    "    chunk = chunk[chunk['subject_id'].notna() & chunk['hadm_id'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(f\"Chunk {chunk_i}: no valid subject/hadm ids, skipping\")\n",
    "        continue\n",
    "\n",
    "    # icd_code as cleaned string\n",
    "    chunk['icd_code'] = chunk['icd_code'].astype(str).str.strip()\n",
    "    # map admit_date quickly using vectorized approach via list comprehension (safe for chunk sizes)\n",
    "    keys = list(zip(chunk['subject_id'].astype(int), chunk['hadm_id'].astype(int)))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    # drop rows without admit_date (no matching admission in merged_initial)\n",
    "    missing_admit_mask = chunk['admit_date'].isna()\n",
    "    n_missing = int(missing_admit_mask.sum())\n",
    "    total_skipped_no_admit += n_missing\n",
    "    if n_missing:\n",
    "        # keep memory low by filtering now\n",
    "        chunk = chunk.loc[~missing_admit_mask]\n",
    "    if chunk.empty:\n",
    "        print(f\"Chunk {chunk_i}: {n_missing} rows had no admit_date; chunk empty after drop -> continue\")\n",
    "        continue\n",
    "\n",
    "    # compute day_index (chart_date normalized minus admit_date), clipped to >= 0\n",
    "    chunk['chart_date'] = pd.to_datetime(chunk['chartdate'], errors='coerce').dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    # group per day and aggregate:\n",
    "    # - proc_count: number of procedure rows that day\n",
    "    # - last_proc_charttime: most recent chartdate (max)\n",
    "    # - proc_codes: unique semicolon-separated icd_code strings (sorted)\n",
    "    def join_unique_codes(series):\n",
    "        s = set([str(x).strip() for x in series.dropna() if str(x).strip() not in (\"\", \"nan\", \"None\")])\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        return \";\".join(sorted(s))\n",
    "\n",
    "    grp = chunk.groupby(['subject_id','hadm_id','day_index'], dropna=False)\n",
    "    df_agg = grp.agg(\n",
    "        proc_count = ('icd_code', 'size'),\n",
    "        last_proc_charttime = ('chartdate', 'max'),\n",
    "        proc_codes = ('icd_code', join_unique_codes)\n",
    "    ).reset_index()\n",
    "\n",
    "    # write per-chunk aggregates (append)\n",
    "    df_agg.to_csv(intermediate_chunks_path, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out = False\n",
    "\n",
    "    print(f\"Chunk {chunk_i}: rows_in={len(chunk):,}, groups_out={len(df_agg):,}, skipped_no_admit={n_missing}\")\n",
    "    del chunk, df_agg, grp\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Streaming done. Total rows processed:\", total_rows_processed)\n",
    "print(\"Total rows skipped because no matching admission:\", total_skipped_no_admit)\n",
    "\n",
    "# === 3) finalize aggregated daily procedures by grouping intermediate file ===\n",
    "print(\"Reading intermediate chunks and final-aggregating...\")\n",
    "if not intermediate_chunks_path.exists():\n",
    "    raise FileNotFoundError(f\"Expected intermediate file {intermediate_chunks_path} not found.\")\n",
    "\n",
    "daily = pd.read_csv(intermediate_chunks_path, parse_dates=['last_proc_charttime'], low_memory=False)\n",
    "\n",
    "# final aggregation: sum counts, max(last_proc_charttime), union of proc_codes across chunked writes\n",
    "def union_semicolon_lists(series):\n",
    "    sset = set()\n",
    "    for val in series.dropna():\n",
    "        if val == \"\":\n",
    "            continue\n",
    "        parts = [p.strip() for p in str(val).split(\";\") if p.strip() != \"\"]\n",
    "        sset.update(parts)\n",
    "    if not sset:\n",
    "        return \"\"\n",
    "    return \";\".join(sorted(sset))\n",
    "\n",
    "final = daily.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "    proc_count = ('proc_count', 'sum'),\n",
    "    last_proc_charttime = ('last_proc_charttime', 'max'),\n",
    "    proc_codes = ('proc_codes', union_semicolon_lists)\n",
    ")\n",
    "\n",
    "final.to_csv(final_daily_path, index=False)\n",
    "print(\"Final per-day procedures saved to:\", final_daily_path)\n",
    "del daily; gc.collect()\n",
    "\n",
    "# === 4) optionally map codes -> titles from d_icd_procedures (if file available) ===\n",
    "print(\"Loading d_icd_procedures to map codes -> titles (if available)...\")\n",
    "dproc = pd.read_csv(dprocedures_path, dtype=str, low_memory=False)\n",
    "dproc['icd_code'] = dproc['icd_code'].astype(str).str.strip()\n",
    "code2title = dict(zip(dproc['icd_code'], dproc['long_title'].fillna(\"\").astype(str)))\n",
    "\n",
    "def map_codes_to_titles(codes_str):\n",
    "    if pd.isna(codes_str) or codes_str == \"\":\n",
    "        return \"\"\n",
    "    codes = [c for c in codes_str.split(\";\") if c.strip() != \"\"]\n",
    "    titles = [code2title.get(c, \"\") for c in codes]\n",
    "    titles = [t for t in titles if t != \"\"]\n",
    "    return \";\".join(titles)\n",
    "\n",
    "final['proc_titles'] = final['proc_codes'].apply(map_codes_to_titles)\n",
    "# Save updated final\n",
    "final.to_csv(final_daily_path, index=False)\n",
    "print(\"Final per-day procedures (with titles) saved to:\", final_daily_path)\n",
    "\n",
    "# === 5) LEFT JOIN final daily procedures into merged_initial and write merged file ===\n",
    "print(\"Merging final daily procedure aggregates into merged_initial master table...\")\n",
    "merged = pd.read_csv(merged_initial_path, low_memory=False, parse_dates=['admittime','dischtime','deathtime'])\n",
    "# ensure types\n",
    "merged['subject_id'] = pd.to_numeric(merged['subject_id'], errors='coerce').astype('Int64')\n",
    "merged['hadm_id'] = pd.to_numeric(merged['hadm_id'], errors='coerce').astype('Int64')\n",
    "merged['day_index'] = pd.to_numeric(merged['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "# load final daily procedures\n",
    "proc_daily = pd.read_csv(final_daily_path, parse_dates=['last_proc_charttime'], low_memory=False)\n",
    "proc_daily['subject_id'] = pd.to_numeric(proc_daily['subject_id'], errors='coerce').astype('Int64')\n",
    "proc_daily['hadm_id'] = pd.to_numeric(proc_daily['hadm_id'], errors='coerce').astype('Int64')\n",
    "proc_daily['day_index'] = pd.to_numeric(proc_daily['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "# left join\n",
    "merged_with_proc = merged.merge(proc_daily, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "# optional: fill NaN counts with 0\n",
    "merged_with_proc['proc_count'] = merged_with_proc['proc_count'].fillna(0).astype('Int64')\n",
    "# keep proc_codes and proc_titles as empty string where missing\n",
    "merged_with_proc['proc_codes'] = merged_with_proc['proc_codes'].fillna(\"\").astype(str)\n",
    "merged_with_proc['proc_titles'] = merged_with_proc['proc_titles'].fillna(\"\").astype(str)\n",
    "\n",
    "# write final merged CSV in chunks (to avoid huge memory spikes)\n",
    "print(\"Writing final merged file (chunked writes)...\")\n",
    "n_rows = len(merged_with_proc)\n",
    "first = True\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_with_proc.iloc[start:end].to_csv(out_merged_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Wrote rows {start}..{end-1}\")\n",
    "print(\"Merged output saved to:\", out_merged_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d72538",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_procedures = pd.read_csv(\"merged_with_procedures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466714ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_procedures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20121989",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_procedures.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
