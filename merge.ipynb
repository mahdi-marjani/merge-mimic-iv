{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"admissions.csv\")\n",
    "\n",
    "admissions = pd.read_csv(csv_path, low_memory=False,\n",
    "                            parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "print(\"Loaded admissions.csv from disk. Rows:\", len(admissions))\n",
    "\n",
    "admissions['admittime'] = pd.to_datetime(admissions['admittime'], errors='coerce')\n",
    "admissions['dischtime'] = pd.to_datetime(admissions['dischtime'], errors='coerce')\n",
    "\n",
    "admissions['dischtime'] = admissions['dischtime'].fillna(admissions['admittime'])\n",
    "\n",
    "def expand_admission_row(row):\n",
    "    adm_date = row['admittime'].normalize().date()\n",
    "    dis_date = row['dischtime'].normalize().date()\n",
    "    n_days = (dis_date - adm_date).days + 1\n",
    "    if n_days <= 0:\n",
    "        n_days = 1\n",
    "    rows = []\n",
    "    for d in range(n_days):\n",
    "        new = {\n",
    "            'subject_id': row['subject_id'],\n",
    "            'hadm_id': row['hadm_id'],\n",
    "            'day_index': int(d),\n",
    "            'admittime': row['admittime'],\n",
    "            'dischtime': row['dischtime'],\n",
    "            'deathtime': row['deathtime'],\n",
    "            'admission_type': row.get('admission_type', np.nan),\n",
    "            'admit_provider_id': row.get('admit_provider_id', np.nan),\n",
    "            'admission_location': row.get('admission_location', np.nan),\n",
    "            'discharge_location': row.get('discharge_location', np.nan),\n",
    "            'insurance': row.get('insurance', np.nan),\n",
    "            'language': row.get('language', np.nan),\n",
    "            'marital_status': row.get('marital_status', np.nan),\n",
    "            'race': row.get('race', np.nan),\n",
    "            'edregtime': row.get('edregtime', pd.NaT),\n",
    "            'edouttime': row.get('edouttime', pd.NaT),\n",
    "            'hospital_expire_flag': row.get('hospital_expire_flag', np.nan)\n",
    "        }\n",
    "        rows.append(new)\n",
    "    return rows\n",
    "\n",
    "expanded = []\n",
    "for _, r in admissions.iterrows():\n",
    "    expanded.extend(expand_admission_row(r))\n",
    "\n",
    "merged_initial = pd.DataFrame(expanded)\n",
    "merged_initial[['subject_id','hadm_id','day_index']] = merged_initial[['subject_id','hadm_id','day_index']].astype('Int64')\n",
    "\n",
    "print(\"Expanded admissions -> rows:\", merged_initial.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv(\"admissions_expanded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad502bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_path = Path(\"icustays.csv\")\n",
    "if not icu_path.exists():\n",
    "    raise FileNotFoundError(f\"icustays.csv not found at {icu_path.resolve()}  -- put the file next to admissions.csv\")\n",
    "\n",
    "icustays = pd.read_csv(icu_path, low_memory=False, parse_dates=['intime','outtime'])\n",
    "\n",
    "for col in ['subject_id','hadm_id','intime','outtime']:\n",
    "    if col not in icustays.columns:\n",
    "        raise KeyError(f\"Expected column '{col}' in icustays.csv but it is missing.\")\n",
    "\n",
    "optional_cols = ['stay_id','first_careunit','last_careunit','los']\n",
    "for c in optional_cols:\n",
    "    if c not in icustays.columns:\n",
    "        icustays[c] = pd.NA\n",
    "\n",
    "merged_with_icu = merged_initial.copy().reset_index(drop=False).rename(columns={'index':'row_id'})\n",
    "for col in ['stay_id_icu','icustay_intime','icustay_outtime','first_careunit_icu','last_careunit_icu','los_icu']:\n",
    "    if col not in merged_with_icu.columns:\n",
    "        merged_with_icu[col] = pd.NA\n",
    "\n",
    "if 'admittime' not in merged_with_icu.columns:\n",
    "    raise KeyError(\"merged_initial must contain 'admittime' column\")\n",
    "\n",
    "merged_with_icu['admittime'] = pd.to_datetime(merged_with_icu['admittime'], errors='coerce')\n",
    "merged_with_icu['day_index_int'] = merged_with_icu['day_index'].fillna(0).astype(int)\n",
    "merged_with_icu['row_date'] = merged_with_icu['admittime'].dt.normalize() + pd.to_timedelta(merged_with_icu['day_index_int'], unit='D')\n",
    "\n",
    "icustays['intime'] = pd.to_datetime(icustays['intime'], errors='coerce')\n",
    "icustays['outtime'] = pd.to_datetime(icustays['outtime'], errors='coerce').fillna(icustays['intime'])\n",
    "icustays['intime_norm'] = icustays['intime'].dt.normalize()\n",
    "icustays['outtime_norm'] = icustays['outtime'].dt.normalize()\n",
    "\n",
    "icu_keep = ['subject_id','hadm_id','stay_id','intime','outtime','intime_norm','outtime_norm','first_careunit','last_careunit','los']\n",
    "candidate = merged_with_icu.merge(icustays[icu_keep], on=['subject_id','hadm_id'], how='left', suffixes=('','_icu'))\n",
    "\n",
    "mask_in_icu = (candidate['row_date'] >= candidate['intime_norm']) & (candidate['row_date'] <= candidate['outtime_norm'])\n",
    "candidate['in_icu'] = mask_in_icu.fillna(False)\n",
    "\n",
    "matched = candidate[candidate['in_icu']].copy()\n",
    "if not matched.empty:\n",
    "    matched = matched.sort_values(by=['row_id','intime'])\n",
    "    first_matches = matched.groupby('row_id', as_index=False).first()\n",
    "    map_cols = {\n",
    "        'stay_id':'stay_id_icu',\n",
    "        'intime':'icustay_intime',\n",
    "        'outtime':'icustay_outtime',\n",
    "        'first_careunit':'first_careunit_icu',\n",
    "        'last_careunit':'last_careunit_icu',\n",
    "        'los':'los_icu'\n",
    "    }\n",
    "    for src, dst in map_cols.items():\n",
    "        mapping = first_matches.set_index('row_id')[src]\n",
    "        merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), dst] = merged_with_icu.loc[merged_with_icu['row_id'].isin(mapping.index), 'row_id'].map(mapping)\n",
    "    assigned_count = len(first_matches)\n",
    "else:\n",
    "    assigned_count = 0\n",
    "\n",
    "merged_with_icu = merged_with_icu.drop(columns=['day_index_int','row_date'])\n",
    "\n",
    "print(f\"ICU assignment complete. Rows where ICU info filled: {int(assigned_count)}\")\n",
    "print(merged_with_icu[merged_with_icu['stay_id_icu'].notna()].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'stay_id_icu': 'stay_id',\n",
    "    'first_careunit_icu': 'first_careunit',\n",
    "    'last_careunit_icu': 'last_careunit',\n",
    "    'icustay_intime': 'icustays_intime',\n",
    "    'icustay_outtime': 'icustays_outtime',\n",
    "    'los_icu': 'los'\n",
    "}\n",
    "merged_with_icu = merged_with_icu.rename(columns=rename_map)\n",
    "\n",
    "icu_cols_ordered = ['stay_id', 'first_careunit', 'last_careunit', 'icustays_intime', 'icustays_outtime', 'los']\n",
    "\n",
    "other_cols = [c for c in merged_with_icu.columns if c not in icu_cols_ordered]\n",
    "\n",
    "merged_with_icu = merged_with_icu[other_cols + icu_cols_ordered]\n",
    "\n",
    "print(\"Renaming & reordering complete.\")\n",
    "print(merged_with_icu[icu_cols_ordered].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06434530",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e880f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f318e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.to_csv('merged_initial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.to_csv('merged_with_icu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_icu.head(100).to_csv('merged_with_icu_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanco_path = Path(\"all_vanco.csv\")\n",
    "if not vanco_path.exists():\n",
    "    raise FileNotFoundError(f\"all_vanco.csv not found at {vanco_path.resolve()}\")\n",
    "\n",
    "all_vanco = pd.read_csv(vanco_path, low_memory=False, parse_dates=['charttime'])\n",
    "\n",
    "all_vanco['subject_id'] = pd.to_numeric(all_vanco['subject_id'], errors='coerce').astype('Int64')\n",
    "all_vanco['hadm_id'] = pd.to_numeric(all_vanco['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "def resolve_numeric(row):\n",
    "    v = row.get('value')\n",
    "    vn = row.get('valuenum')\n",
    "    if pd.isna(v) or str(v).strip() in ['', '___', 'NaN', 'nan']:\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    s = str(v).strip().replace(',', '')\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        try:\n",
    "            return float(vn) if not pd.isna(vn) else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "all_vanco['resolved_val'] = all_vanco.apply(resolve_numeric, axis=1)\n",
    "\n",
    "merged_with_vanco = merged_with_icu.copy()\n",
    "if 'admittime' not in merged_with_vanco.columns:\n",
    "    raise KeyError(f\"merged_with_vanco must contain 'admittime' column before merging labs.\")\n",
    "merged_with_vanco['admittime'] = pd.to_datetime(merged_with_vanco['admittime'], errors='coerce')\n",
    "\n",
    "admit_map = merged_with_vanco.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time']).dt.normalize()\n",
    "\n",
    "all_vanco = all_vanco.merge(admit_map[['subject_id','hadm_id','admit_date']], on=['subject_id','hadm_id'], how='left')\n",
    "\n",
    "missing_admit = all_vanco['admit_date'].isna().sum()\n",
    "if missing_admit:\n",
    "    print(f\"Warning: {missing_admit} all_vanco rows have no matching admission (admit_date missing) and will be skipped.\")\n",
    "all_vanco = all_vanco[all_vanco['admit_date'].notna()].copy()\n",
    "\n",
    "all_vanco['chart_date'] = pd.to_datetime(all_vanco['charttime'], errors='coerce').dt.normalize()\n",
    "all_vanco['day_index_lab'] = (all_vanco['chart_date'] - all_vanco['admit_date']).dt.days.fillna(0).astype(int)\n",
    "all_vanco.loc[all_vanco['day_index_lab'] < 0, 'day_index_lab'] = 0\n",
    "\n",
    "group_cols = ['subject_id','hadm_id','day_index_lab']\n",
    "usable = all_vanco[~all_vanco['resolved_val'].isna()].copy()\n",
    "if usable.empty:\n",
    "    print(\"No usable numeric vanco values found to aggregate.\")\n",
    "    daily_vanco = pd.DataFrame(columns=['subject_id','hadm_id','day_index_lab',\n",
    "                                       'charttime','value','valuenum','valueuom','flag','resolved_val'])\n",
    "else:\n",
    "    idx = usable.groupby(group_cols)['resolved_val'].idxmax()\n",
    "    daily_vanco = usable.loc[idx].copy()\n",
    "\n",
    "daily_vanco = daily_vanco.rename(columns={\n",
    "    'charttime':'all_vanco_charttime',\n",
    "    'value':'all_vanco_value',\n",
    "    'valuenum':'all_vanco_valuenum',\n",
    "    'valueuom':'all_vanco_valueuom',\n",
    "    'flag':'all_vanco_flag',\n",
    "    'day_index_lab':'day_index'\n",
    "})\n",
    "\n",
    "merge_cols = ['subject_id','hadm_id','day_index',\n",
    "              'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']\n",
    "daily_vanco = daily_vanco[merge_cols]\n",
    "\n",
    "daily_vanco['subject_id'] = daily_vanco['subject_id'].astype('Int64')\n",
    "daily_vanco['hadm_id'] = daily_vanco['hadm_id'].astype('Int64')\n",
    "daily_vanco['day_index'] = daily_vanco['day_index'].astype('Int64')\n",
    "\n",
    "merged_with_vanco = merged_with_vanco.merge(daily_vanco, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "print(f\"all_vanco merged -> rows with vanco info: {int(merged_with_vanco['all_vanco_charttime'].notna().sum())}\")\n",
    "\n",
    "preview = merged_with_vanco[merged_with_vanco['all_vanco_charttime'].notna()].head(20)\n",
    "print(preview[['subject_id','hadm_id','day_index',\n",
    "               'all_vanco_charttime','all_vanco_value','all_vanco_valuenum','all_vanco_valueuom','all_vanco_flag']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_vanco.head(100).to_csv('merged_with_vanco_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_vanco.to_csv('merged_with_vanco.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "chartevents = pd.read_csv(chartevents_path, nrows=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e61e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(chartevents_path,nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chartevents.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f99c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents.csv\")\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(f\"chartevents.csv not found at {chartevents_path.resolve()}\")\n",
    "\n",
    "usecols = ['subject_id','hadm_id','itemid','charttime','value','valuenum']\n",
    "chunksize = 200_000\n",
    "\n",
    "total_counts = defaultdict(int)\n",
    "present_counts = defaultdict(int)\n",
    "\n",
    "reader = pd.read_csv(chartevents_path, usecols=usecols, chunksize=chunksize, low_memory=True)\n",
    "\n",
    "chunk_i = 0\n",
    "for chunk in reader:\n",
    "    chunk_i += 1\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    present_mask = ~chunk['valuenum'].isna()\n",
    "\n",
    "    need_check = chunk['valuenum'].isna()\n",
    "    if need_check.any():\n",
    "        vals = chunk.loc[need_check, 'value'].astype(str).str.strip()\n",
    "        good = ~vals.isin([\"\", \"___\", \"NaN\", \"nan\", \"None\", \"none\"])\n",
    "        present_mask.loc[need_check] = good.values\n",
    "\n",
    "    grp_total = chunk.groupby('itemid').size()\n",
    "    grp_present = present_mask.groupby(chunk['itemid']).sum()\n",
    "\n",
    "    for item, cnt in grp_total.items():\n",
    "        total_counts[int(item)] += int(cnt)\n",
    "    for item, cnt in grp_present.items():\n",
    "        if pd.isna(item):\n",
    "            continue\n",
    "        present_counts[int(item)] += int(cnt)\n",
    "\n",
    "    if chunk_i % 10 == 0:\n",
    "        print(f\"Processed {chunk_i*chunksize:,} rows...\")\n",
    "\n",
    "itemids = sorted(set(list(total_counts.keys()) + list(present_counts.keys())))\n",
    "rows = []\n",
    "for iid in itemids:\n",
    "    tot = total_counts.get(iid, 0)\n",
    "    pres = present_counts.get(iid, 0)\n",
    "    miss = tot - pres\n",
    "    frac = pres / tot if tot > 0 else 0.0\n",
    "    rows.append((iid, tot, pres, miss, frac))\n",
    "\n",
    "missingness_df = pd.DataFrame(rows, columns=['itemid','total_count','present_count','missing_count','present_fraction'])\n",
    "missingness_df = missingness_df.sort_values(by='present_fraction', ascending=False).reset_index(drop=True)\n",
    "missingness_df.to_csv(\"chartevents_itemid_missingness.csv\", index=False)\n",
    "print(\"Saved chartevents_itemid_missingness.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chartevents_itemid_missingness.csv\")\n",
    "\n",
    "drop_ids = df.loc[df['present_fraction'] < 0.5, 'itemid'].tolist()\n",
    "\n",
    "print(f\"تعداد itemid هایی که باید drop بشن: {len(drop_ids)}\")\n",
    "print(drop_ids[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c76ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(\"chartevents.csv\", chunksize=2_000_000)\n",
    "out_path = \"chartevents_missing50_dropped.csv\"\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "total_dropped = 0\n",
    "total_written = 0\n",
    "\n",
    "for i, chunk in enumerate(reader, start=1):\n",
    "    before = len(chunk)\n",
    "    filtered = chunk.loc[~chunk['itemid'].isin(drop_ids)]\n",
    "    after = len(filtered)\n",
    "    \n",
    "    filtered.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    \n",
    "    total_rows += before\n",
    "    total_dropped += before - after\n",
    "    total_written += after\n",
    "    print(f\"Chunk {i}: rows={before:,}, dropped={before - after:,}, kept={after:,}\")\n",
    "\n",
    "print(\"---- DONE ----\")\n",
    "print(f\"Total rows processed: {total_rows:,}\")\n",
    "print(f\"Total rows dropped:   {total_dropped:,}\")\n",
    "print(f\"Total rows written:   {total_written:,}\")\n",
    "print(\"✅ فایل نهایی ذخیره شد:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"d_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aae3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"d_items.csv\"\n",
    "out_path = \"d_items_chartevents_missing50_dropped.csv\"\n",
    "\n",
    "df = pd.read_csv(in_path)\n",
    "\n",
    "filtered = df.loc[\n",
    "    (df[\"linksto\"] == \"chartevents\") & \n",
    "    (~df[\"itemid\"].isin(drop_ids))\n",
    "]\n",
    "\n",
    "filtered.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"✅ d_items filtered and saved:\", out_path)\n",
    "print(\"before:\", len(df), \"after:\", len(filtered), \"drop:\", len(df) - len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "merged_initial_file = Path(\"merged_initial.csv\")\n",
    "out_path = Path(\"merged_initial_with_items_cols.csv\")\n",
    "\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "cols_to_add = [str(i) for i in itemids]\n",
    "print(\"Will add columns (count):\", len(cols_to_add))\n",
    "\n",
    "try:\n",
    "    merged_initial\n",
    "    print(\"Using merged_initial from memory (existing DataFrame). rows:\", len(merged_initial))\n",
    "except NameError:\n",
    "    if not merged_initial_file.exists():\n",
    "        raise FileNotFoundError(f\"merged_initial not in memory and file {merged_initial_file} not found.\")\n",
    "    print(\"Loading merged_initial from disk:\", merged_initial_file)\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "    print(\"Loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "n_rows = len(merged_initial)\n",
    "added = 0\n",
    "for c in cols_to_add:\n",
    "    if c not in merged_initial.columns:\n",
    "        merged_initial[c] = pd.Series([pd.NA] * n_rows, dtype=\"object\")\n",
    "        added += 1\n",
    "print(f\"Added {added} new columns. Total columns now: {len(merged_initial.columns)}\")\n",
    "\n",
    "chunksize = 10000\n",
    "first = True\n",
    "written = 0\n",
    "for start in range(0, n_rows, chunksize):\n",
    "    end = min(start + chunksize, n_rows)\n",
    "    chunk = merged_initial.iloc[start:end]\n",
    "    chunk.to_csv(out_path, mode=\"w\" if first else \"a\", index=False, header=first)\n",
    "    first = False\n",
    "    written += len(chunk)\n",
    "    print(f\"Wrote rows {start:,}..{end-1:,} -> {len(chunk):,} rows\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", written, \"Columns in output:\", len(merged_initial.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57308ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_initial.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "merged_initial_file = None\n",
    "chunksize = 500_000\n",
    "save_after = False\n",
    "\n",
    "if not chartevents_path.exists():\n",
    "    raise FileNotFoundError(chartevents_path)\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "\n",
    "ditems = pd.read_csv(ditems_path, usecols=['itemid'])\n",
    "keep_itemids = pd.to_numeric(ditems['itemid'], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "keep_itemids_set = set(keep_itemids)\n",
    "print(\"Keep itemids count:\", len(keep_itemids))\n",
    "\n",
    "try:\n",
    "    merged_initial\n",
    "except NameError:\n",
    "    if merged_initial_file is None:\n",
    "        raise NameError(\"merged_initial not in memory. Set merged_initial_file path or load it.\")\n",
    "    print(\"Loading merged_initial from disk...\")\n",
    "    merged_initial = pd.read_csv(merged_initial_file, low_memory=False, parse_dates=['admittime'])\n",
    "    print(\"loaded merged_initial rows:\", len(merged_initial))\n",
    "\n",
    "for iid in keep_itemids:\n",
    "    col = str(iid)\n",
    "    if col not in merged_initial.columns:\n",
    "        merged_initial[col] = pd.Series([pd.NA] * len(merged_initial), dtype=\"object\")\n",
    "\n",
    "admit_map = merged_initial.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "\n",
    "merged_initial_index_map = {}\n",
    "for idx, row in merged_initial[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    key = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "    merged_initial_index_map[key] = idx\n",
    "\n",
    "print(\"Admit map keys:\", len(admit_dict), \"merged rows map size:\", len(merged_initial_index_map))\n",
    "\n",
    "reader = pd.read_csv(chartevents_path, usecols=['subject_id','hadm_id','itemid','charttime','value','valuenum'],\n",
    "                     parse_dates=['charttime'], chunksize=chunksize, low_memory=True)\n",
    "\n",
    "total_assigned = 0\n",
    "chunk_no = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_no += 1\n",
    "    print(f\"\\n--- Processing chunk {chunk_no} (rows: {len(chunk)}) ---\")\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    chunk = chunk[chunk['itemid'].isin(keep_itemids)]\n",
    "    if chunk.empty:\n",
    "        print(\"no relevant itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    chunk['subject_id'] = chunk['subject_id'].astype(int)\n",
    "    chunk['hadm_id'] = chunk['hadm_id'].astype(int)\n",
    "\n",
    "    def lookup_admit_date(s):\n",
    "        return admit_dict.get((int(s.subject_id), int(s.hadm_id)), pd.NaT)\n",
    "    keys = list(zip(chunk['subject_id'].astype(int), chunk['hadm_id'].astype(int)))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    chunk = chunk[chunk['admit_date'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(\"no rows with admit_date in this chunk\")\n",
    "        continue\n",
    "\n",
    "    chunk['chart_date'] = chunk['charttime'].dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    chunk['numeric_val'] = pd.to_numeric(chunk['valuenum'], errors='coerce')\n",
    "    mask_num_missing = chunk['numeric_val'].isna()\n",
    "    if mask_num_missing.any():\n",
    "        parsed = pd.to_numeric(chunk.loc[mask_num_missing, 'value'].astype(str).str.replace(',',''), errors='coerce')\n",
    "        chunk.loc[mask_num_missing, 'numeric_val'] = parsed\n",
    "\n",
    "    chunk['value_raw'] = chunk['value'].astype(str)\n",
    "\n",
    "    grp_keys = ['subject_id','hadm_id','day_index','itemid']\n",
    "\n",
    "    numeric_rows = chunk[chunk['numeric_val'].notna()].copy()\n",
    "    if not numeric_rows.empty:\n",
    "        grp_num = numeric_rows.groupby(grp_keys, as_index=False)['numeric_val'].max()\n",
    "        grp_num = grp_num.rename(columns={'numeric_val':'agg_value_num'})\n",
    "    else:\n",
    "        grp_num = pd.DataFrame(columns=grp_keys + ['agg_value_num'])\n",
    "\n",
    "    chunk_sorted = chunk.sort_values('charttime')\n",
    "    grp_last = chunk_sorted.groupby(grp_keys, as_index=False).last()[grp_keys + ['value_raw','charttime']]\n",
    "    grp_last = grp_last.rename(columns={'value_raw':'agg_value_text', 'charttime':'agg_time_text'})\n",
    "\n",
    "    merged_grps = pd.merge(grp_last, grp_num, on=grp_keys, how='left')\n",
    "\n",
    "    def pick_final_val(row):\n",
    "        if pd.notna(row.get('agg_value_num')):\n",
    "            return row['agg_value_num']\n",
    "        else:\n",
    "            v = row.get('agg_value_text')\n",
    "            if pd.isna(v) or v in (\"nan\",\"None\",\"NoneType\",\"NA\",\"<NA>\"):\n",
    "                return pd.NA\n",
    "            return v\n",
    "\n",
    "    merged_grps['final_value'] = merged_grps.apply(pick_final_val, axis=1)\n",
    "\n",
    "    assigned = 0\n",
    "    for _, r in merged_grps.iterrows():\n",
    "        key = (int(r['subject_id']), int(r['hadm_id']), int(r['day_index']))\n",
    "        row_idx = merged_initial_index_map.get(key)\n",
    "        if row_idx is None:\n",
    "            continue\n",
    "        itemid_col = str(int(r['itemid']))\n",
    "        val = r['final_value']\n",
    "        merged_initial.at[row_idx, itemid_col] = val\n",
    "        assigned += 1\n",
    "\n",
    "    total_assigned += assigned\n",
    "    print(f\"Chunk {chunk_no}: groups aggregated = {len(merged_grps)}, assigned = {assigned}, total_assigned so far = {total_assigned}\")\n",
    "\n",
    "    del chunk, chunk_sorted, numeric_rows, grp_num, grp_last, merged_grps\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- ALL CHUNKS PROCESSED ---\")\n",
    "print(\"Total assigned cells:\", total_assigned)\n",
    "\n",
    "out_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "n_rows = len(merged_initial)\n",
    "write_chunk = 20000\n",
    "first = True\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_initial.iloc[start:end].to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved final to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "merged_with_chartevents_filled = pd.read_csv(merged_with_chartevents_filled_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82151eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"chartevents_missing50_dropped.csv\")\n",
    "output_path = Path(\"chartevents_missing50_dropped_filtered_hadm_id_23282506.csv\")\n",
    "\n",
    "chunksize = 2_000_000\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(input_path, chunksize=chunksize, low_memory=False)):\n",
    "    filtered = chunk[chunk['hadm_id'] == 23282506]\n",
    "    if not filtered.empty:\n",
    "        filtered.to_csv(output_path, mode='w' if first else 'a',\n",
    "                        index=False, header=first)\n",
    "        first = False\n",
    "        total_rows += len(filtered)\n",
    "        print(f\"Chunk {i}: wrote {len(filtered)} rows (total so far: {total_rows})\")\n",
    "\n",
    "print(\"Done! Final rows written:\", total_rows)\n",
    "print(\"Output file:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40055979",
   "metadata": {},
   "outputs": [],
   "source": [
    "ditems_path = Path(\"d_items_chartevents_missing50_dropped.csv\")\n",
    "in_path = Path(\"merged_with_chartevents_filled.csv\")\n",
    "out_path = Path(\"merged_with_chartevents_filled_renamed.csv\")\n",
    "chunksize = 20_000\n",
    "max_name_len = 80\n",
    "\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(ditems_path)\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(in_path)\n",
    "\n",
    "d = pd.read_csv(ditems_path, usecols=['itemid','label','abbreviation'], dtype=str)\n",
    "d['itemid'] = d['itemid'].str.strip()\n",
    "d['label'] = d['label'].fillna('').astype(str).str.strip()\n",
    "d['abbreviation'] = d['abbreviation'].fillna('').astype(str).str.strip()\n",
    "\n",
    "d['chosen'] = d.apply(lambda r: r['abbreviation'] if r['abbreviation']!='' else (r['label'] if r['label']!='' else ''), axis=1)\n",
    "\n",
    "def sanitize_name(s):\n",
    "    if pd.isna(s) or s is None:\n",
    "        return ''\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w\\-]', '', s)\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s[:max_name_len]\n",
    "    return s\n",
    "\n",
    "name_map = {}\n",
    "used = set()\n",
    "\n",
    "for _, row in d.iterrows():\n",
    "    iid = row['itemid']\n",
    "    chosen = row['chosen']\n",
    "    if chosen == '':\n",
    "        base = f\"item_{iid}\"\n",
    "    else:\n",
    "        base = sanitize_name(chosen)\n",
    "        if base == '':\n",
    "            base = f\"item_{iid}\"\n",
    "    name = base\n",
    "    if name in used:\n",
    "        name = f\"{base}__{iid}\"\n",
    "    counter = 1\n",
    "    while name in used:\n",
    "        name = f\"{base}__{iid}_{counter}\"\n",
    "        counter += 1\n",
    "    used.add(name)\n",
    "    name_map[str(iid)] = name\n",
    "\n",
    "orig_header = pd.read_csv(in_path, nrows=0).columns.tolist()\n",
    "new_header = []\n",
    "conflicts = 0\n",
    "for col in orig_header:\n",
    "    new_col = col\n",
    "    col_str = str(col).strip()\n",
    "    if col_str in name_map:\n",
    "        new_col = \"chartevents_\" + name_map[col_str]\n",
    "    else:\n",
    "        try:\n",
    "            icol = str(int(float(col_str)))\n",
    "            if icol in name_map:\n",
    "                new_col = name_map[icol]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if new_col in new_header:\n",
    "        conflicts += 1\n",
    "        new_col = f\"{new_col}__orig_{sanitize_name(col_str)}\"\n",
    "        k = 1\n",
    "        while new_col in new_header:\n",
    "            new_col = f\"{new_col}_{k}\"; k += 1\n",
    "    new_header.append(new_col)\n",
    "\n",
    "print(f\"Prepared header mapping. Total cols: {len(orig_header)}, conflicts resolved: {conflicts}\")\n",
    "\n",
    "sample_map = {k: name_map[k] for k in list(name_map)[:10]}\n",
    "print(\"sample itemid->name (first 10):\", sample_map)\n",
    "\n",
    "first = True\n",
    "rows_written = 0\n",
    "for i, chunk in enumerate(pd.read_csv(in_path, chunksize=chunksize, low_memory=False)):\n",
    "    chunk.columns = new_header\n",
    "    chunk.to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    rows_written += len(chunk)\n",
    "    print(f\"Chunk {i+1}: wrote {len(chunk):,} rows (total {rows_written:,})\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", rows_written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed_path = Path(\"merged_with_chartevents_filled_renamed.csv\")\n",
    "merged_with_chartevents_filled_renamed = pd.read_csv(merged_with_chartevents_filled_renamed_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_chartevents_filled_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents_path = Path(\"datetimeevents.csv\")\n",
    "datetimeevents = pd.read_csv(datetimeevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252369fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents.head(100).to_csv('test_datetimeevents', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents[300:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5685e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyevents_path = Path(\"microbiologyevents.csv\")\n",
    "microbiologyevents = pd.read_csv(microbiologyevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyevents.head(100).to_csv('test_microbiologyevents', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb53da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyevents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "DIAGNOSIS_CSV = Path(\"diagnoses_icd.csv\")\n",
    "DICT_CSV = Path(\"d_icd_diagnoses.csv\")\n",
    "MERGED_INITIAL_CSV = Path(\"admissions_expanded.csv\")   # input (admission x day)\n",
    "OUT_CSV = Path(\"merged_with_diagnoses.csv\")      # output\n",
    "CHUNK_DIAG = 200_000        # chunk size for diagnoses reading (diagnoses table is usually small)\n",
    "CHUNK_WRITE = 20_000        # chunk size for writing merged_initial with diagnoses\n",
    "TOP_K = 5                   # produce diag_1..diag_K columns (set to 0 to disable)\n",
    "\n",
    "# --- Helpers ---\n",
    "def normalize_code(code):\n",
    "    \"\"\"Normalize ICD code strings for matching: str, strip, uppercase, remove dots.\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return \"\"\n",
    "    s = str(code).strip().upper()\n",
    "    s = s.replace(\".\", \"\")\n",
    "    return s\n",
    "\n",
    "def try_lookup_description(code, version, dict_map):\n",
    "    \"\"\"Attempt to find long_title for (code, version) with fallback strategies.\"\"\"\n",
    "    key = (code, str(int(version)) if pd.notna(version) else str(version))\n",
    "    if key in dict_map:\n",
    "        return dict_map[key]\n",
    "    # fallback: remove leading zeros from both sides (e.g., '0010' -> '10')\n",
    "    code_nolead = code.lstrip(\"0\")\n",
    "    key2 = (code_nolead, key[1])\n",
    "    if key2 in dict_map:\n",
    "        return dict_map[key2]\n",
    "    # fallback: if dict keys have leading zeros and code doesn't, try to left-pad to 4 (common for old formats)\n",
    "    if code.isdigit():\n",
    "        for pad in (3,4,5):\n",
    "            kp = (code.zfill(pad), key[1])\n",
    "            if kp in dict_map:\n",
    "                return dict_map[kp]\n",
    "    return pd.NA\n",
    "\n",
    "# --- Step 1: load ICD dictionary into memory (small) ---\n",
    "if not DICT_CSV.exists():\n",
    "    raise FileNotFoundError(f\"{DICT_CSV} not found. Place d_icd_diagnoses.csv next to this script.\")\n",
    "\n",
    "dict_df = pd.read_csv(DICT_CSV, dtype=str)  # icd_code, icd_version, long_title\n",
    "# normalize dict codes:\n",
    "dict_df['icd_code_norm'] = dict_df['icd_code'].astype(str).apply(normalize_code)\n",
    "dict_df['icd_version_norm'] = dict_df['icd_version'].astype(str).str.strip()\n",
    "# build mapping (code_norm, version) -> long_title\n",
    "dict_map = dict(((row.icd_code_norm, row.icd_version_norm), row.long_title) for row in dict_df.itertuples(index=False))\n",
    "\n",
    "print(f\"Loaded ICD dictionary rows: {len(dict_df)}\")\n",
    "\n",
    "# --- Step 2: read diagnoses_icd in chunks and accumulate per-admission lists ---\n",
    "if not DIAGNOSIS_CSV.exists():\n",
    "    raise FileNotFoundError(f\"{DIAGNOSIS_CSV} not found. Place diagnoses_icd.csv next to this script.\")\n",
    "\n",
    "acc = defaultdict(list)   # key -> list of (seq_num (int), icd_code_norm (str), icd_version)\n",
    "rows_seen = 0\n",
    "for chunk in pd.read_csv(DIAGNOSIS_CSV, chunksize=CHUNK_DIAG, dtype=str, low_memory=False):\n",
    "    # ensure required columns exist\n",
    "    for col in (\"subject_id\",\"hadm_id\",\"seq_num\",\"icd_code\",\"icd_version\"):\n",
    "        if col not in chunk.columns:\n",
    "            raise KeyError(f\"Expected column '{col}' in diagnoses_icd.csv but missing.\")\n",
    "    # normalize and iterate\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    chunk['seq_num'] = pd.to_numeric(chunk['seq_num'], errors='coerce').fillna(99999).astype(int)\n",
    "    chunk['icd_code_norm'] = chunk['icd_code'].astype(str).apply(normalize_code)\n",
    "    chunk['icd_version_norm'] = chunk['icd_version'].astype(str).str.strip()\n",
    "\n",
    "    for r in chunk.itertuples(index=False):\n",
    "        # skip if missing hadm or subject\n",
    "        if pd.isna(r.subject_id) or pd.isna(r.hadm_id):\n",
    "            continue\n",
    "        key = (int(r.subject_id), int(r.hadm_id))\n",
    "        acc[key].append((int(r.seq_num), r.icd_code_norm, r.icd_version_norm))\n",
    "        rows_seen += 1\n",
    "    print(f\"Processed diagnoses rows so far: {rows_seen}\", end='\\r')\n",
    "\n",
    "print(f\"\\nTotal diagnosis rows processed: {rows_seen}; unique admissions with diagnoses: {len(acc)}\")\n",
    "\n",
    "# --- Step 3: build per-admission aggregate DataFrame ---\n",
    "agg_rows = []\n",
    "for (subj, hadm), entries in acc.items():\n",
    "    # sort by seq_num ascending\n",
    "    entries_sorted = sorted(entries, key=lambda x: (x[0] if x[0] is not None else 99999))\n",
    "    codes = [e[1] for e in entries_sorted if e[1] != \"\"]\n",
    "    versions = [e[2] for e in entries_sorted]\n",
    "    # lookup descriptions (preserve order)\n",
    "    descs = [ try_lookup_description(c, v, dict_map) if c != \"\" else pd.NA for c,v in zip(codes, versions) ]\n",
    "    n = len(codes)\n",
    "    primary_code = codes[0] if n >= 1 else pd.NA\n",
    "    primary_desc = descs[0] if n >= 1 else pd.NA\n",
    "    # top-K split\n",
    "    top_codes = {}\n",
    "    top_descs = {}\n",
    "    for k in range(1, TOP_K+1):\n",
    "        if n >= k:\n",
    "            top_codes[f\"diag_{k}_code\"] = codes[k-1]\n",
    "            top_descs[f\"diag_{k}_desc\"] = descs[k-1]\n",
    "        else:\n",
    "            top_codes[f\"diag_{k}_code\"] = pd.NA\n",
    "            top_descs[f\"diag_{k}_desc\"] = pd.NA\n",
    "\n",
    "    agg_rows.append({\n",
    "        \"subject_id\": int(subj),\n",
    "        \"hadm_id\": int(hadm),\n",
    "        \"diag_n\": int(n),\n",
    "        \"diag_codes\": \";\".join(codes) if codes else pd.NA,\n",
    "        \"diag_descs\": \";\".join([str(d) for d in descs]) if descs else pd.NA,\n",
    "        \"primary_diag_code\": primary_code,\n",
    "        \"primary_diag_desc\": primary_desc,\n",
    "        **top_codes,\n",
    "        **top_descs\n",
    "    })\n",
    "\n",
    "diag_df = pd.DataFrame(agg_rows)\n",
    "# ensure dtypes\n",
    "if not diag_df.empty:\n",
    "    diag_df['subject_id'] = diag_df['subject_id'].astype('Int64')\n",
    "    diag_df['hadm_id'] = diag_df['hadm_id'].astype('Int64')\n",
    "    diag_df['diag_n'] = diag_df['diag_n'].astype('Int64')\n",
    "\n",
    "print(\"Built diag_df with rows:\", len(diag_df))\n",
    "\n",
    "# --- Step 4: merge diag_df into merged_initial.csv in chunks (so we don't load merged_initial fully) ---\n",
    "if not MERGED_INITIAL_CSV.exists():\n",
    "    raise FileNotFoundError(f\"{MERGED_INITIAL_CSV} not found. Place merged_initial.csv next to this script.\")\n",
    "\n",
    "first_write = True\n",
    "written = 0\n",
    "for chunk in pd.read_csv(MERGED_INITIAL_CSV, chunksize=CHUNK_WRITE, parse_dates=['admittime'], low_memory=False):\n",
    "    # ensure keys have correct dtype\n",
    "    if 'subject_id' in chunk.columns:\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    if 'hadm_id' in chunk.columns:\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "    merged_chunk = chunk.merge(diag_df, on=['subject_id','hadm_id'], how='left')\n",
    "    # if diag_df is empty, the merge will just add nothing; that's okay.\n",
    "\n",
    "    merged_chunk.to_csv(OUT_CSV, mode='w' if first_write else 'a', index=False, header=first_write)\n",
    "    first_write = False\n",
    "    written += len(merged_chunk)\n",
    "    print(f\"Wrote merged rows: {written}\", end='\\r')\n",
    "    del chunk, merged_chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nDone. Output saved to: {OUT_CSV} (rows written: {written})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9258013",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(MERGED_INITIAL_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02496607",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_diagnoses = pd.read_csv(\"merged_with_diagnoses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_diagnoses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_diagnoses.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c838f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a743cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures_icd = pd.read_csv(\"procedures_icd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures_icd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_icd_procedures = pd.read_csv(\"d_icd_procedures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a618761",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_icd_procedures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "procedures_path = Path(\"procedures_icd.csv\")\n",
    "dprocedures_path = Path(\"d_icd_procedures.csv\")\n",
    "merged_initial_path = Path(\"admissions_expanded.csv\")\n",
    "intermediate_chunks_path = Path(\"procedures_daily_chunks.csv\")\n",
    "final_daily_path = Path(\"procedures_daily_final.csv\")\n",
    "out_merged_path = Path(\"merged_with_procedures.csv\")\n",
    "\n",
    "chunksize = 500_000   # tune to your environment\n",
    "write_chunk = 20000\n",
    "\n",
    "# === 0) sanity checks ===\n",
    "for p in (procedures_path, dprocedures_path, merged_initial_path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Required file not found: {p.resolve()}\")\n",
    "\n",
    "# === 1) build admit_date lookup from merged_initial ===\n",
    "print(\"Loading merged_initial admissions (admit_time -> admit_date map)...\")\n",
    "mi_cols = ['subject_id', 'hadm_id', 'admittime']\n",
    "mi = pd.read_csv(merged_initial_path, usecols=mi_cols, parse_dates=['admittime'], low_memory=False)\n",
    "mi['subject_id'] = pd.to_numeric(mi['subject_id'], errors='coerce').astype('Int64')\n",
    "mi['hadm_id'] = pd.to_numeric(mi['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "# take first admittime per (subject_id, hadm_id)\n",
    "admit_map = mi.groupby(['subject_id', 'hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "\n",
    "# make a dict: (int(subject_id), int(hadm_id)) -> admit_date (Timestamp) for fast lookup\n",
    "admit_dict = {}\n",
    "for r in admit_map.itertuples(index=False):\n",
    "    try:\n",
    "        key = (int(r.subject_id), int(r.hadm_id))\n",
    "    except Exception:\n",
    "        continue\n",
    "    admit_dict[key] = r.admit_date\n",
    "print(\"Admit map size:\", len(admit_dict))\n",
    "\n",
    "del mi, admit_map; gc.collect()\n",
    "\n",
    "# === 2) chunked read of procedures_icd -> per-chunk daily aggregates ===\n",
    "print(\"Streaming procedures_icd in chunks and writing per-chunk daily aggregates...\")\n",
    "first_out = True\n",
    "total_skipped_no_admit = 0\n",
    "total_rows_processed = 0\n",
    "reader = pd.read_csv(procedures_path,\n",
    "                     usecols=['subject_id','hadm_id','seq_num','chartdate','icd_code','icd_version'],\n",
    "                     parse_dates=['chartdate'],\n",
    "                     chunksize=chunksize,\n",
    "                     low_memory=True)\n",
    "\n",
    "for chunk_i, chunk in enumerate(reader, start=1):\n",
    "    total_rows_processed += len(chunk)\n",
    "    # normalize ids\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # drop rows with missing ids\n",
    "    chunk = chunk[chunk['subject_id'].notna() & chunk['hadm_id'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(f\"Chunk {chunk_i}: no valid subject/hadm ids, skipping\")\n",
    "        continue\n",
    "\n",
    "    # icd_code as cleaned string\n",
    "    chunk['icd_code'] = chunk['icd_code'].astype(str).str.strip()\n",
    "    # map admit_date quickly using vectorized approach via list comprehension (safe for chunk sizes)\n",
    "    keys = list(zip(chunk['subject_id'].astype(int), chunk['hadm_id'].astype(int)))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    # drop rows without admit_date (no matching admission in merged_initial)\n",
    "    missing_admit_mask = chunk['admit_date'].isna()\n",
    "    n_missing = int(missing_admit_mask.sum())\n",
    "    total_skipped_no_admit += n_missing\n",
    "    if n_missing:\n",
    "        # keep memory low by filtering now\n",
    "        chunk = chunk.loc[~missing_admit_mask]\n",
    "    if chunk.empty:\n",
    "        print(f\"Chunk {chunk_i}: {n_missing} rows had no admit_date; chunk empty after drop -> continue\")\n",
    "        continue\n",
    "\n",
    "    # compute day_index (chart_date normalized minus admit_date), clipped to >= 0\n",
    "    chunk['chart_date'] = pd.to_datetime(chunk['chartdate'], errors='coerce').dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    # group per day and aggregate:\n",
    "    # - proc_count: number of procedure rows that day\n",
    "    # - last_proc_charttime: most recent chartdate (max)\n",
    "    # - proc_codes: unique semicolon-separated icd_code strings (sorted)\n",
    "    def join_unique_codes(series):\n",
    "        s = set([str(x).strip() for x in series.dropna() if str(x).strip() not in (\"\", \"nan\", \"None\")])\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        return \";\".join(sorted(s))\n",
    "\n",
    "    grp = chunk.groupby(['subject_id','hadm_id','day_index'], dropna=False)\n",
    "    df_agg = grp.agg(\n",
    "        proc_count = ('icd_code', 'size'),\n",
    "        last_proc_charttime = ('chartdate', 'max'),\n",
    "        proc_codes = ('icd_code', join_unique_codes)\n",
    "    ).reset_index()\n",
    "\n",
    "    # write per-chunk aggregates (append)\n",
    "    df_agg.to_csv(intermediate_chunks_path, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out = False\n",
    "\n",
    "    print(f\"Chunk {chunk_i}: rows_in={len(chunk):,}, groups_out={len(df_agg):,}, skipped_no_admit={n_missing}\")\n",
    "    del chunk, df_agg, grp\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Streaming done. Total rows processed:\", total_rows_processed)\n",
    "print(\"Total rows skipped because no matching admission:\", total_skipped_no_admit)\n",
    "\n",
    "# === 3) finalize aggregated daily procedures by grouping intermediate file ===\n",
    "print(\"Reading intermediate chunks and final-aggregating...\")\n",
    "if not intermediate_chunks_path.exists():\n",
    "    raise FileNotFoundError(f\"Expected intermediate file {intermediate_chunks_path} not found.\")\n",
    "\n",
    "daily = pd.read_csv(intermediate_chunks_path, parse_dates=['last_proc_charttime'], low_memory=False)\n",
    "\n",
    "# final aggregation: sum counts, max(last_proc_charttime), union of proc_codes across chunked writes\n",
    "def union_semicolon_lists(series):\n",
    "    sset = set()\n",
    "    for val in series.dropna():\n",
    "        if val == \"\":\n",
    "            continue\n",
    "        parts = [p.strip() for p in str(val).split(\";\") if p.strip() != \"\"]\n",
    "        sset.update(parts)\n",
    "    if not sset:\n",
    "        return \"\"\n",
    "    return \";\".join(sorted(sset))\n",
    "\n",
    "final = daily.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "    proc_count = ('proc_count', 'sum'),\n",
    "    last_proc_charttime = ('last_proc_charttime', 'max'),\n",
    "    proc_codes = ('proc_codes', union_semicolon_lists)\n",
    ")\n",
    "\n",
    "final.to_csv(final_daily_path, index=False)\n",
    "print(\"Final per-day procedures saved to:\", final_daily_path)\n",
    "del daily; gc.collect()\n",
    "\n",
    "# === 4) optionally map codes -> titles from d_icd_procedures (if file available) ===\n",
    "print(\"Loading d_icd_procedures to map codes -> titles (if available)...\")\n",
    "dproc = pd.read_csv(dprocedures_path, dtype=str, low_memory=False)\n",
    "dproc['icd_code'] = dproc['icd_code'].astype(str).str.strip()\n",
    "code2title = dict(zip(dproc['icd_code'], dproc['long_title'].fillna(\"\").astype(str)))\n",
    "\n",
    "def map_codes_to_titles(codes_str):\n",
    "    if pd.isna(codes_str) or codes_str == \"\":\n",
    "        return \"\"\n",
    "    codes = [c for c in codes_str.split(\";\") if c.strip() != \"\"]\n",
    "    titles = [code2title.get(c, \"\") for c in codes]\n",
    "    titles = [t for t in titles if t != \"\"]\n",
    "    return \";\".join(titles)\n",
    "\n",
    "final['proc_titles'] = final['proc_codes'].apply(map_codes_to_titles)\n",
    "# Save updated final\n",
    "final.to_csv(final_daily_path, index=False)\n",
    "print(\"Final per-day procedures (with titles) saved to:\", final_daily_path)\n",
    "\n",
    "# === 5) LEFT JOIN final daily procedures into merged_initial and write merged file ===\n",
    "print(\"Merging final daily procedure aggregates into merged_initial master table...\")\n",
    "merged = pd.read_csv(merged_initial_path, low_memory=False, parse_dates=['admittime','dischtime','deathtime'])\n",
    "# ensure types\n",
    "merged['subject_id'] = pd.to_numeric(merged['subject_id'], errors='coerce').astype('Int64')\n",
    "merged['hadm_id'] = pd.to_numeric(merged['hadm_id'], errors='coerce').astype('Int64')\n",
    "merged['day_index'] = pd.to_numeric(merged['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "# load final daily procedures\n",
    "proc_daily = pd.read_csv(final_daily_path, parse_dates=['last_proc_charttime'], low_memory=False)\n",
    "proc_daily['subject_id'] = pd.to_numeric(proc_daily['subject_id'], errors='coerce').astype('Int64')\n",
    "proc_daily['hadm_id'] = pd.to_numeric(proc_daily['hadm_id'], errors='coerce').astype('Int64')\n",
    "proc_daily['day_index'] = pd.to_numeric(proc_daily['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "# left join\n",
    "merged_with_proc = merged.merge(proc_daily, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "# optional: fill NaN counts with 0\n",
    "merged_with_proc['proc_count'] = merged_with_proc['proc_count'].fillna(0).astype('Int64')\n",
    "# keep proc_codes and proc_titles as empty string where missing\n",
    "merged_with_proc['proc_codes'] = merged_with_proc['proc_codes'].fillna(\"\").astype(str)\n",
    "merged_with_proc['proc_titles'] = merged_with_proc['proc_titles'].fillna(\"\").astype(str)\n",
    "\n",
    "# write final merged CSV in chunks (to avoid huge memory spikes)\n",
    "print(\"Writing final merged file (chunked writes)...\")\n",
    "n_rows = len(merged_with_proc)\n",
    "first = True\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_with_proc.iloc[start:end].to_csv(out_merged_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Wrote rows {start}..{end-1}\")\n",
    "print(\"Merged output saved to:\", out_merged_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d72538",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_procedures = pd.read_csv(\"merged_with_procedures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466714ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_procedures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20121989",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_procedures.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1002fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config / paths ---\n",
    "drg_path = Path(\"drgcodes.csv\")\n",
    "merged_initial_path = Path(\"admissions_expanded.csv\")   # or use DataFrame merged_initial if already in memory\n",
    "out_path = Path(\"merged_with_drg.csv\")\n",
    "\n",
    "# --- Load merged_initial (admission x day rows) ---\n",
    "if 'merged_initial' in globals():\n",
    "    merged_initial = globals()['merged_initial']\n",
    "else:\n",
    "    if not merged_initial_path.exists():\n",
    "        raise FileNotFoundError(f\"{merged_initial_path} not found. Provide merged_initial.csv or have merged_initial DataFrame in memory.\")\n",
    "    merged_initial = pd.read_csv(merged_initial_path, parse_dates=['admittime','dischtime','deathtime'], low_memory=False)\n",
    "# ensure keys have consistent types\n",
    "merged_initial['subject_id'] = pd.to_numeric(merged_initial['subject_id'], errors='coerce').astype('Int64')\n",
    "merged_initial['hadm_id'] = pd.to_numeric(merged_initial['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "# --- Load drgcodes (small) ---\n",
    "if not drg_path.exists():\n",
    "    raise FileNotFoundError(f\"{drg_path} not found.\")\n",
    "drg = pd.read_csv(drg_path, low_memory=False)\n",
    "\n",
    "# normalize / coerce types\n",
    "drg['subject_id'] = pd.to_numeric(drg['subject_id'], errors='coerce').astype('Int64')\n",
    "drg['hadm_id'] = pd.to_numeric(drg['hadm_id'], errors='coerce').astype('Int64')\n",
    "# keep textual drg_code as string but strip\n",
    "drg['drg_code'] = drg['drg_code'].astype(str).str.strip()\n",
    "drg['description'] = drg['description'].astype(str).str.strip()\n",
    "drg['drg_type'] = drg['drg_type'].astype(str).str.strip()\n",
    "\n",
    "# numeric columns: coerce to numeric (float), keep NaN when missing\n",
    "drg['drg_severity_num'] = pd.to_numeric(drg['drg_severity'], errors='coerce')\n",
    "drg['drg_mortality_num'] = pd.to_numeric(drg['drg_mortality'], errors='coerce')\n",
    "\n",
    "# --- Aggregation strategy ---\n",
    "# 1) Compute numeric aggregates per admission (max)\n",
    "numeric_aggs = drg.groupby(['subject_id','hadm_id'], dropna=False).agg({\n",
    "    'drg_severity_num': 'max',\n",
    "    'drg_mortality_num': 'max'\n",
    "}).reset_index().rename(columns={\n",
    "    'drg_severity_num': 'drg_severity_max',\n",
    "    'drg_mortality_num': 'drg_mortality_max'\n",
    "})\n",
    "\n",
    "# 2) Choose single representative row per admission for textual fields.\n",
    "# Build a sort key: (drg_severity_num desc, drg_mortality_num desc, drg_type priority desc)\n",
    "type_priority = {'APR': 2, 'HCFA': 1}  # APR preferred over HCFA; others map to 0\n",
    "drg['type_prio'] = drg['drg_type'].map(type_priority).fillna(0).astype(int)\n",
    "\n",
    "# Replace NaN with very small number so numerics that exist always win\n",
    "drg['_sev_for_sort'] = drg['drg_severity_num'].fillna(-9999)\n",
    "drg['_mort_for_sort'] = drg['drg_mortality_num'].fillna(-9999)\n",
    "\n",
    "drg_sorted = drg.sort_values(\n",
    "    by=['subject_id','hadm_id','_sev_for_sort','_mort_for_sort','type_prio'],\n",
    "    ascending=[True, True, False, False, False]\n",
    ")\n",
    "\n",
    "# pick first per group (best by our rule)\n",
    "drg_rep = drg_sorted.groupby(['subject_id','hadm_id'], as_index=False).first()[[\n",
    "    'subject_id','hadm_id','drg_type','drg_code','description','drg_severity','drg_mortality'\n",
    "]].rename(columns={\n",
    "    'drg_type': 'drg_type_chosen',\n",
    "    'drg_code': 'drg_code_chosen',\n",
    "    'description': 'drg_description_chosen',\n",
    "    'drg_severity': 'drg_severity_chosen',\n",
    "    'drg_mortality': 'drg_mortality_chosen'\n",
    "})\n",
    "\n",
    "# 3) merge numeric_aggs and textual representative row into a single drg_summary\n",
    "drg_summary = numeric_aggs.merge(drg_rep, on=['subject_id','hadm_id'], how='left')\n",
    "\n",
    "# Optional: if you want both numeric_max and chosen textual numeric (string),\n",
    "# ensure columns consistent and cast types\n",
    "# (drg_severity_max is float; drg_severity_chosen may be string original - try to coerce)\n",
    "drg_summary['drg_severity_chosen'] = pd.to_numeric(drg_summary['drg_severity_chosen'], errors='coerce')\n",
    "\n",
    "# --- Merge into merged_initial (broadcast to day rows) ---\n",
    "merged_final = merged_initial.merge(\n",
    "    drg_summary,\n",
    "    on=['subject_id','hadm_id'],\n",
    "    how='left',\n",
    "    validate='m:1'  # many merged_initial rows (days) to one drg_summary row\n",
    ")\n",
    "\n",
    "# --- Save output ---\n",
    "merged_final.to_csv(out_path, index=False)\n",
    "print(\"Merged drgcodes -> written to:\", out_path)\n",
    "print(\"Rows with DRG info:\", int(merged_final['drg_code_chosen'].notna().sum()))\n",
    "\n",
    "# End of script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f380174",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_drg = pd.read_csv(\"merged_with_drg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc45ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_drg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_drg.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ed365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config - tune these to your environment\n",
    "MERGED_IN_PATH = Path(\"admissions_expanded.csv\")   # or use Memory DataFrame merged_initial\n",
    "TRANSFERS_PATH   = Path(\"transfers.csv\")\n",
    "SERVICES_PATH    = Path(\"services.csv\")\n",
    "OUT_PATH         = Path(\"merged_with_transfers_services.csv\")\n",
    "\n",
    "TRANS_CHUNKSIZE = 200_000   # transfers often smaller; safe default\n",
    "SERV_CHUNKSIZE  = 200_000\n",
    "\n",
    "# Load or reference merged_initial\n",
    "if 'merged_initial' in globals():\n",
    "    merged = merged_initial.copy()\n",
    "else:\n",
    "    if not MERGED_IN_PATH.exists():\n",
    "        raise FileNotFoundError(f\"{MERGED_IN_PATH} not found. Provide merged_initial DataFrame or file.\")\n",
    "    merged = pd.read_csv(MERGED_IN_PATH, parse_dates=['admittime'], low_memory=False)\n",
    "print(\"Loaded merged rows:\", len(merged))\n",
    "\n",
    "# Prepare new columns (if not present)\n",
    "new_cols = [\n",
    "    'transfers_eventtype', 'transfers_careunit', 'transfers_intime', 'transfers_outtime',\n",
    "    'services_prev_service', 'services_curr_service', 'services_transfertime'\n",
    "]\n",
    "for c in new_cols:\n",
    "    if c not in merged.columns:\n",
    "        merged[c] = pd.NA\n",
    "\n",
    "# Build admit_map: (subject_id, hadm_id) -> admit_date (normalized)\n",
    "admit_map = merged.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "print(\"Admit map size:\", len(admit_dict))\n",
    "\n",
    "# Build index map for merged rows: (subject_id, hadm_id, day_index) -> row index\n",
    "merged_index_map = {}\n",
    "for idx, row in merged[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    try:\n",
    "        k = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "    except Exception:\n",
    "        # skip malformed keys\n",
    "        continue\n",
    "    merged_index_map[k] = idx\n",
    "print(\"Merged rows map size:\", len(merged_index_map))\n",
    "\n",
    "########################\n",
    "# Process transfers.csv\n",
    "########################\n",
    "if not TRANSFERS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"{TRANSFERS_PATH} not found.\")\n",
    "\n",
    "# We'll collect first-match per (row_idx) via assignment - to mimic your ICU first-match semantics,\n",
    "# we process transfers sorted by intime so earliest covering transfer is assigned first.\n",
    "trans_reader = pd.read_csv(TRANSFERS_PATH, parse_dates=['intime','outtime'], chunksize=TRANS_CHUNKSIZE, low_memory=True)\n",
    "\n",
    "total_assigned_transfers = 0\n",
    "chunk_no = 0\n",
    "for chunk in trans_reader:\n",
    "    chunk_no += 1\n",
    "    # keep relevant columns\n",
    "    chunk = chunk[['subject_id','hadm_id','transfer_id','eventtype','careunit','intime','outtime']].copy()\n",
    "    # coerce ids\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce')\n",
    "    # normalize times\n",
    "    chunk['intime'] = pd.to_datetime(chunk['intime'], errors='coerce')\n",
    "    chunk['outtime'] = pd.to_datetime(chunk['outtime'], errors='coerce')\n",
    "    chunk = chunk.dropna(subset=['subject_id','hadm_id','intime'])  # need atleast intime\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # sort by intime ascending so earliest transfers processed first\n",
    "    chunk = chunk.sort_values('intime')\n",
    "\n",
    "    # iterate rows (transfers per admission are usually few; OK to loop)\n",
    "    assigned = 0\n",
    "    for _, tr in chunk.iterrows():\n",
    "        s = int(tr['subject_id'])\n",
    "        h = int(tr['hadm_id'])\n",
    "        admit_date = admit_dict.get((s,h), pd.NaT)\n",
    "        if pd.isna(admit_date):\n",
    "            continue\n",
    "        intime = tr['intime']\n",
    "        outtime = tr['outtime'] if pd.notna(tr['outtime']) else tr['intime']\n",
    "        intime_norm = intime.normalize()\n",
    "        outtime_norm = pd.to_datetime(outtime).normalize()\n",
    "        di_start = int(max(0, (intime_norm - admit_date).days))\n",
    "        di_end   = int(max(0, (outtime_norm - admit_date).days))\n",
    "        for di in range(di_start, di_end + 1):\n",
    "            key = (s, h, di)\n",
    "            row_idx = merged_index_map.get(key)\n",
    "            if row_idx is None:\n",
    "                continue\n",
    "            existing_val = merged.at[row_idx, 'transfers_careunit']\n",
    "            if pd.notna(existing_val):\n",
    "                continue\n",
    "            merged.at[row_idx, 'transfers_eventtype'] = tr.get('eventtype', pd.NA)\n",
    "            merged.at[row_idx, 'transfers_careunit'] = tr.get('careunit', pd.NA)\n",
    "            merged.at[row_idx, 'transfers_intime'] = intime\n",
    "            merged.at[row_idx, 'transfers_outtime'] = outtime if pd.notna(tr.get('outtime')) else pd.NA\n",
    "            assigned += 1\n",
    "    total_assigned_transfers += assigned\n",
    "    print(f\"Transfers chunk {chunk_no}: assigned {assigned} rows (total assigned so far: {total_assigned_transfers})\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Total transfer assignments:\", total_assigned_transfers)\n",
    "\n",
    "########################\n",
    "# Process services.csv (last-per-day semantics)\n",
    "########################\n",
    "if not SERVICES_PATH.exists():\n",
    "    raise FileNotFoundError(f\"{SERVICES_PATH} not found.\")\n",
    "\n",
    "# We'll keep a dict for best (latest) service per (subject,hadm,day_index)\n",
    "service_best = {}  # key -> (transfertime, prev_service, curr_service)\n",
    "\n",
    "serv_reader = pd.read_csv(SERVICES_PATH, parse_dates=['transfertime'], chunksize=SERV_CHUNKSIZE, low_memory=True)\n",
    "chunk_no = 0\n",
    "for chunk in serv_reader:\n",
    "    chunk_no += 1\n",
    "    # ensure columns exist\n",
    "    chunk = chunk[['subject_id','hadm_id','transfertime','prev_service','curr_service']].copy()\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce')\n",
    "    chunk['transfertime'] = pd.to_datetime(chunk['transfertime'], errors='coerce')\n",
    "    chunk = chunk.dropna(subset=['subject_id','hadm_id','transfertime'])\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # iterate rows - services per admission relatively small\n",
    "    for _, srow in chunk.iterrows():\n",
    "        s = int(srow['subject_id'])\n",
    "        h = int(srow['hadm_id'])\n",
    "        key_admit = (s,h)\n",
    "        admit_date = admit_dict.get(key_admit, pd.NaT)\n",
    "        if pd.isna(admit_date):\n",
    "            continue\n",
    "        t = srow['transfertime']\n",
    "        day_idx = int(max(0, (t.normalize() - admit_date).days))\n",
    "        map_key = (s, h, day_idx)\n",
    "        existing = service_best.get(map_key)\n",
    "        # pick the later transfertime on that day\n",
    "        if existing is None or (pd.notna(existing[0]) and t > existing[0]):\n",
    "            service_best[map_key] = (t, srow.get('prev_service', pd.NA), srow.get('curr_service', pd.NA))\n",
    "\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "    print(f\"Services chunk {chunk_no} processed; current cached service keys: {len(service_best)}\")\n",
    "\n",
    "# Apply service_best into merged DataFrame\n",
    "assigned_services = 0\n",
    "for map_key, (t, prev_s, curr_s) in service_best.items():\n",
    "    row_idx = merged_index_map.get(map_key)\n",
    "    if row_idx is None:\n",
    "        continue\n",
    "    merged.at[row_idx, 'services_prev_service'] = prev_s if prev_s is not None else pd.NA\n",
    "    merged.at[row_idx, 'services_curr_service'] = curr_s if curr_s is not None else pd.NA\n",
    "    merged.at[row_idx, 'services_transfertime'] = t\n",
    "    assigned_services += 1\n",
    "\n",
    "print(\"Total service rows applied:\", assigned_services)\n",
    "\n",
    "# Finalize types (optional)\n",
    "for c in ['transfers_intime','transfers_outtime','services_transfertime']:\n",
    "    merged[c] = pd.to_datetime(merged[c], errors='coerce')\n",
    "\n",
    "# Write out in chunks to avoid memory spikes\n",
    "write_chunk = 20_000\n",
    "first = True\n",
    "n_rows = len(merged)\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged.iloc[start:end].to_csv(OUT_PATH, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved merged file to:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add12dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_transfers_services = pd.read_csv(\"merged_with_transfers_services.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cba51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_transfers_services.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3183473",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_transfers_services.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a942b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd002c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions = pd.read_csv(\"prescriptions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b469f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prescriptions.head(5).to_csv('test_prescriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba602e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy = pd.read_csv(\"pharmacy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0982eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacy.head(5).to_csv('test_pharmacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191016da",
   "metadata": {},
   "outputs": [],
   "source": [
    "emar = pd.read_csv(\"emar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "emar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "emar.head(5).to_csv('test_emar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0beb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- config ----------\n",
    "BASE_DIR = Path(\".\")\n",
    "MERGED_INITIAL_PATH = BASE_DIR / \"admissions_expanded.csv\"\n",
    "PRESC_PATH = BASE_DIR / \"prescriptions.csv\"\n",
    "PHARM_PATH = BASE_DIR / \"pharmacy.csv\"\n",
    "EMAR_PATH = BASE_DIR / \"emar.csv\"\n",
    "\n",
    "AGG_PRESC_PATH = BASE_DIR / \"agg_prescriptions_daily.csv\"\n",
    "AGG_PHARM_PATH = BASE_DIR / \"agg_pharmacy_daily.csv\"\n",
    "AGG_EMAR_PATH  = BASE_DIR / \"agg_emar_daily.csv\"\n",
    "\n",
    "# chunk sizes (tune these)\n",
    "SRC_CHUNKSIZE = 200_000\n",
    "WRITE_CHUNK = 20_000\n",
    "MATCH_CHUNK = 200_000\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def nowstr():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def parse_numeric(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    try:\n",
    "        if isinstance(x, (int,float,np.number)): return float(x)\n",
    "        s = str(x).strip().replace(',', '')\n",
    "        if s in (\"\", \"NaN\", \"nan\", \"None\", \"none\", \"___\"): return np.nan\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def read_header(path: Path):\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    return pd.read_csv(path, nrows=0).columns.tolist()\n",
    "\n",
    "def build_admit_map(merged_initial_path):\n",
    "    print(f\"[{nowstr()}] Building admit_map from {merged_initial_path} (light read)...\")\n",
    "    mi = pd.read_csv(merged_initial_path, usecols=['subject_id','hadm_id','admittime'],\n",
    "                     parse_dates=['admittime'], low_memory=False)\n",
    "    mi['subject_id'] = pd.to_numeric(mi['subject_id'], errors='coerce').astype('Int64')\n",
    "    mi['hadm_id'] = pd.to_numeric(mi['hadm_id'], errors='coerce').astype('Int64')\n",
    "    admit = mi.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "    admit['admit_date'] = pd.to_datetime(admit['admit_time']).dt.normalize()\n",
    "    admit_dict = dict(zip(zip(admit['subject_id'].astype(int), admit['hadm_id'].astype(int)), admit['admit_date']))\n",
    "    print(f\"[{nowstr()}] Admit map keys: {len(admit_dict)}\")\n",
    "    del mi, admit\n",
    "    gc.collect()\n",
    "    return admit_dict\n",
    "\n",
    "def consolidate_agg(path: Path, out_path: Path, key_cols=('subject_id','hadm_id','day_index'),\n",
    "                    sum_cols=None, min_cols=None, max_cols=None, pick_by_max=None, pick_cols=None):\n",
    "    \"\"\"\n",
    "    Read a per-chunk AGG file (which may have duplicates for the same (s,h,d)),\n",
    "    and consolidate so result has one row per (s,h,d).\n",
    "    - sum_cols: list of columns to sum across duplicates (e.g., counts)\n",
    "    - min_cols: list of cols to take min\n",
    "    - max_cols: list of cols to take max\n",
    "    - pick_by_max: a timestamp column name — for pick_cols choose the row with max(pick_by_max) and take those values\n",
    "    \"\"\"\n",
    "    sum_cols = sum_cols or []\n",
    "    min_cols = min_cols or []\n",
    "    max_cols = max_cols or []\n",
    "    pick_cols = pick_cols or []\n",
    "\n",
    "    if not path.exists():\n",
    "        # nothing to do\n",
    "        print(f\"[{nowstr()}] consolidate_agg: {path} not found, skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[{nowstr()}] Consolidating {path} -> {out_path} ...\")\n",
    "    # read whole AGG file (these AGG files are usually much smaller than raw sources)\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    # ensure key types\n",
    "    for c in key_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    # convert candidate datetime columns if present\n",
    "    candidate_dt = []\n",
    "    if pick_by_max:\n",
    "        candidate_dt.append(pick_by_max)\n",
    "    for col in (min_cols + max_cols + candidate_dt):\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    # Build aggregation dict for groupby. Use named aggregations for readability.\n",
    "    agg_dict = {}\n",
    "    for c in sum_cols:\n",
    "        if c in df.columns:\n",
    "            agg_dict[c] = 'sum'\n",
    "    for c in min_cols:\n",
    "        if c in df.columns:\n",
    "            agg_dict[c] = 'min'\n",
    "    for c in max_cols:\n",
    "        if c in df.columns:\n",
    "            agg_dict[c] = 'max'\n",
    "\n",
    "    # if no aggregation columns present (edge), just drop duplicates using first\n",
    "    if not agg_dict and not pick_cols:\n",
    "        out = df.drop_duplicates(subset=list(key_cols)).reset_index(drop=True)\n",
    "        out.to_csv(out_path, index=False)\n",
    "        print(f\"[{nowstr()}] Consolidation done (simple dedupe). rows: {len(out)}\")\n",
    "        return\n",
    "\n",
    "    grouped = df.groupby(list(key_cols), dropna=False, as_index=False).agg(agg_dict) if agg_dict else df.groupby(list(key_cols), dropna=False, as_index=False).first()\n",
    "\n",
    "    # handle pick_by_max for pick_cols\n",
    "    if pick_by_max and any(pc in df.columns for pc in pick_cols):\n",
    "        # For each group, pick the row index in df with max(pick_by_max)\n",
    "        # handle groups where pick_by_max is NaT by falling back to first occurrence\n",
    "        idx = df.groupby(list(key_cols))[pick_by_max].idxmax().dropna()\n",
    "        # idx is a Series mapping group -> row index (may miss groups where pick_by_max all NaT)\n",
    "        picked = df.loc[idx].reset_index(drop=True)\n",
    "        # reduce picked to key_cols + pick_cols (and pick_by_max if exists)\n",
    "        take_cols = list(key_cols) + [c for c in pick_cols if c in picked.columns]\n",
    "        picked = picked[take_cols]\n",
    "        # merge picked into grouped (picked values overwrite grouped's columns if same names)\n",
    "        grouped = grouped.merge(picked, on=list(key_cols), how='left')\n",
    "\n",
    "        # for groups not present in picked (all NaT), try to fill pick_cols from first row per group\n",
    "        missing_mask = grouped[pick_cols[0]].isna() if pick_cols and pick_cols[0] in grouped.columns else None\n",
    "        if missing_mask is not None and missing_mask.any():\n",
    "            # get first row per group\n",
    "            first_rows = df.groupby(list(key_cols), as_index=False).first()[list(key_cols)+[c for c in pick_cols if c in df.columns]]\n",
    "            grouped = grouped.merge(first_rows, on=list(key_cols), how='left', suffixes=('','__first'))\n",
    "            # fill NaNs in pick_cols with __first\n",
    "            for c in pick_cols:\n",
    "                if c in grouped.columns and (c + \"__first\") in grouped.columns:\n",
    "                    grouped[c] = grouped[c].combine_first(grouped[c + \"__first\"])\n",
    "                    grouped = grouped.drop(columns=[c + \"__first\"])\n",
    "    # Ensure types for keys are Int64 where possible\n",
    "    for c in key_cols:\n",
    "        if c in grouped.columns:\n",
    "            grouped[c] = pd.to_numeric(grouped[c], errors='coerce').astype('Int64')\n",
    "\n",
    "    grouped.to_csv(out_path, index=False)\n",
    "    print(f\"[{nowstr()}] Consolidation done. rows: {len(grouped)} (written to {out_path})\")\n",
    "    del df, grouped\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def collect_matching_rows(agg_path: Path, keys_set, usecols=None, parse_dates=None, chunksize=MATCH_CHUNK):\n",
    "    \"\"\"Scan agg_path in chunks and return rows matching keys_set (keys_set of (s,h,d) ints).\"\"\"\n",
    "    if not agg_path.exists():\n",
    "        return pd.DataFrame(columns=(usecols or []))\n",
    "    if not keys_set:\n",
    "        return pd.DataFrame(columns=(usecols or []))\n",
    "    keys_str = set(f\"{int(s)}|{int(h)}|{int(d)}\" for (s,h,d) in keys_set)\n",
    "    matches = []\n",
    "    usecols = usecols or None\n",
    "    for c_i, chunk in enumerate(pd.read_csv(agg_path, usecols=usecols, parse_dates=parse_dates or [], chunksize=chunksize, low_memory=True)):\n",
    "        # ensure key columns exist\n",
    "        if not {'subject_id','hadm_id','day_index'}.issubset(set(chunk.columns)):\n",
    "            continue\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "        chunk['day_index'] = pd.to_numeric(chunk['day_index'], errors='coerce').astype('Int64')\n",
    "        keys = chunk['subject_id'].astype(str) + '|' + chunk['hadm_id'].astype(str) + '|' + chunk['day_index'].astype(str)\n",
    "        mask = keys.isin(keys_str)\n",
    "        sel = chunk[mask]\n",
    "        if not sel.empty:\n",
    "            matches.append(sel)\n",
    "        del chunk, keys, mask, sel\n",
    "        gc.collect()\n",
    "    if matches:\n",
    "        return pd.concat(matches, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=(usecols or []))\n",
    "\n",
    "\n",
    "# -------------------- run --------------------\n",
    "import gc\n",
    "admit_dict = build_admit_map(MERGED_INITIAL_PATH)\n",
    "\n",
    "# ---------- Phase 1: aggregate prescriptions ----------\n",
    "print(f\"[{nowstr()}] Phase 1 — aggregate prescriptions -> {AGG_PRESC_PATH}\")\n",
    "if AGG_PRESC_PATH.exists(): AGG_PRESC_PATH.unlink()\n",
    "\n",
    "presc_header = read_header(PRESC_PATH)\n",
    "presc_usecols_want = ['subject_id','hadm_id','starttime','stoptime','drug','formulary_drug_cd','ndc','prod_strength','dose_val_rx','dose_unit_rx','route','doses_per_24_hrs','drug_type','poe_id','poe_seq','order_provider_id']\n",
    "presc_usecols = [c for c in presc_usecols_want if c in presc_header]\n",
    "presc_parse_dates = [c for c in ['starttime','stoptime'] if c in presc_header]\n",
    "\n",
    "first_out = True\n",
    "total_out = 0\n",
    "for i, chunk in enumerate(pd.read_csv(PRESC_PATH, usecols=presc_usecols, parse_dates=presc_parse_dates, chunksize=SRC_CHUNKSIZE, low_memory=True), start=1):\n",
    "    t0 = time.time()\n",
    "    print(f\"[{nowstr()}] Presc chunk {i} rows={len(chunk):,}\")\n",
    "    # cast ids\n",
    "    if 'subject_id' in chunk.columns:\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    keys = list(zip(chunk['subject_id'].astype('Int64').astype(object), chunk['hadm_id'].astype('Int64').astype(object)))\n",
    "    chunk['admit_date'] = [admit_dict.get((int(s), int(h)), pd.NaT) if not (pd.isna(s) or pd.isna(h)) else pd.NaT for s,h in keys]\n",
    "    before = len(chunk)\n",
    "    chunk = chunk[chunk['admit_date'].notna()].copy()\n",
    "    dropped = before - len(chunk)\n",
    "    if dropped:\n",
    "        print(f\"  -> dropped {dropped:,} presc rows with no admit mapping\")\n",
    "    if chunk.empty:\n",
    "        del chunk; gc.collect(); continue\n",
    "    chunk['event_time'] = pd.to_datetime(chunk['starttime'], errors='coerce').fillna(pd.to_datetime(chunk.get('stoptime', pd.NaT), errors='coerce'))\n",
    "    chunk['chart_date'] = chunk['event_time'].dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "    chunk['dose_val_num'] = chunk['dose_val_rx'].apply(parse_numeric) if 'dose_val_rx' in chunk.columns else np.nan\n",
    "    chunk['prod_strength_num'] = chunk['prod_strength'].apply(parse_numeric) if 'prod_strength' in chunk.columns else np.nan\n",
    "    chunk['doses_per_24h_num'] = chunk['doses_per_24_hrs'].apply(parse_numeric) if 'doses_per_24_hrs' in chunk.columns else np.nan\n",
    "    chunk['drug_text'] = chunk['drug'].astype(str).str.strip().replace({'nan':None})\n",
    "    chunk['route_text'] = chunk['route'].astype(str).str.strip().replace({'nan':None})\n",
    "    agg = chunk.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "        presc_orders_count = ('drug_text','count'),\n",
    "        presc_first_start = ('starttime','min') if 'starttime' in chunk.columns else pd.NamedAgg(column='event_time', aggfunc='min'),\n",
    "        presc_last_stop  = ('stoptime','max') if 'stoptime' in chunk.columns else pd.NamedAgg(column='event_time', aggfunc='max'),\n",
    "        presc_last_drug  = ('drug_text', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        presc_last_route = ('route_text', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        presc_max_dose   = ('dose_val_num','max'),\n",
    "        presc_max_strength = ('prod_strength_num','max'),\n",
    "        presc_doses_per_24h = ('doses_per_24h_num','max')\n",
    "    )\n",
    "    agg['subject_id'] = agg['subject_id'].astype('Int64')\n",
    "    agg['hadm_id'] = agg['hadm_id'].astype('Int64')\n",
    "    agg['day_index'] = agg['day_index'].astype('Int64')\n",
    "    agg.to_csv(AGG_PRESC_PATH, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out = False\n",
    "    total_out += len(agg)\n",
    "    print(f\"[{nowstr()}] Presc chunk {i} -> wrote {len(agg):,} agg rows (total {total_out:,})  took {time.time()-t0:.1f}s\")\n",
    "    del chunk, agg\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"[{nowstr()}] Prescriptions aggregation done. total agg rows (unconsolidated): {total_out}\")\n",
    "\n",
    "# consolidate the AGG presc into unique keys\n",
    "consolidate_agg(AGG_PRESC_PATH, AGG_PRESC_PATH,\n",
    "                key_cols=('subject_id','hadm_id','day_index'),\n",
    "                sum_cols=['presc_orders_count'],\n",
    "                min_cols=['presc_first_start'],\n",
    "                max_cols=['presc_last_stop','presc_max_dose','presc_max_strength','presc_doses_per_24h'],\n",
    "                pick_by_max='presc_last_stop',\n",
    "                pick_cols=['presc_last_drug','presc_last_route'])\n",
    "\n",
    "# ---------- Phase 2: aggregate pharmacy ----------\n",
    "print(f\"[{nowstr()}] Phase 2 — aggregate pharmacy -> {AGG_PHARM_PATH}\")\n",
    "if AGG_PHARM_PATH.exists(): AGG_PHARM_PATH.unlink()\n",
    "\n",
    "pharm_header = read_header(PHARM_PATH)\n",
    "pharm_usecols_want = ['subject_id','hadm_id','starttime','stoptime','medication','route','frequency','dispensation','fill_quantity','entertime','verifiedtime','expirationdate']\n",
    "pharm_usecols = [c for c in pharm_usecols_want if c in pharm_header]\n",
    "pharm_parse_dates = [c for c in ['starttime','stoptime','entertime','verifiedtime','expirationdate'] if c in pharm_header]\n",
    "\n",
    "first_out = True\n",
    "total_out = 0\n",
    "for i, chunk in enumerate(pd.read_csv(PHARM_PATH, usecols=pharm_usecols, parse_dates=pharm_parse_dates, chunksize=SRC_CHUNKSIZE, low_memory=True), start=1):\n",
    "    t0 = time.time()\n",
    "    print(f\"[{nowstr()}] Pharm chunk {i} rows={len(chunk):,}\")\n",
    "    if 'subject_id' in chunk.columns:\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    keys = list(zip(chunk['subject_id'].astype('Int64').astype(object), chunk['hadm_id'].astype('Int64').astype(object)))\n",
    "    chunk['admit_date'] = [admit_dict.get((int(s), int(h)), pd.NaT) if not (pd.isna(s) or pd.isna(h)) else pd.NaT for s,h in keys]\n",
    "    before = len(chunk)\n",
    "    chunk = chunk[chunk['admit_date'].notna()].copy()\n",
    "    dropped = before - len(chunk)\n",
    "    if dropped:\n",
    "        print(f\"  -> dropped {dropped:,} pharm rows with no admit mapping\")\n",
    "    if chunk.empty:\n",
    "        del chunk; gc.collect(); continue\n",
    "    chunk['event_time'] = pd.to_datetime(chunk['starttime'], errors='coerce').fillna(pd.to_datetime(chunk.get('verifiedtime', pd.NaT), errors='coerce')).fillna(pd.to_datetime(chunk.get('entertime', pd.NaT), errors='coerce'))\n",
    "    chunk['chart_date'] = chunk['event_time'].dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "    chunk['fill_qty_num'] = chunk['fill_quantity'].apply(parse_numeric) if 'fill_quantity' in chunk.columns else np.nan\n",
    "    chunk['med_text'] = chunk['medication'].astype(str).str.strip().replace({'nan':None})\n",
    "    chunk['route_text'] = chunk['route'].astype(str).str.strip().replace({'nan':None})\n",
    "    agg = chunk.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "        pharm_dispense_count = ('med_text','count'),\n",
    "        pharm_first_start = ('starttime','min') if 'starttime' in chunk.columns else pd.NamedAgg(column='event_time', aggfunc='min'),\n",
    "        pharm_last_stop  = ('stoptime','max') if 'stoptime' in chunk.columns else pd.NamedAgg(column='event_time', aggfunc='max'),\n",
    "        pharm_last_med   = ('med_text', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        pharm_last_route = ('route_text', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        pharm_max_fill_qty = ('fill_qty_num','max')\n",
    "    )\n",
    "    agg['subject_id'] = agg['subject_id'].astype('Int64')\n",
    "    agg['hadm_id'] = agg['hadm_id'].astype('Int64')\n",
    "    agg['day_index'] = agg['day_index'].astype('Int64')\n",
    "    agg.to_csv(AGG_PHARM_PATH, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out = False\n",
    "    total_out += len(agg)\n",
    "    print(f\"[{nowstr()}] Pharm chunk {i} -> wrote {len(agg):,} agg rows (total {total_out:,})  took {time.time()-t0:.1f}s\")\n",
    "    del chunk, agg\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"[{nowstr()}] Pharmacy aggregation done. total agg rows (unconsolidated): {total_out}\")\n",
    "\n",
    "consolidate_agg(AGG_PHARM_PATH, AGG_PHARM_PATH,\n",
    "                key_cols=('subject_id','hadm_id','day_index'),\n",
    "                sum_cols=['pharm_dispense_count'],\n",
    "                min_cols=['pharm_first_start'],\n",
    "                max_cols=['pharm_last_stop','pharm_max_fill_qty'],\n",
    "                pick_by_max='pharm_last_stop',\n",
    "                pick_cols=['pharm_last_med','pharm_last_route'])\n",
    "\n",
    "\n",
    "# ---------- Phase 3: aggregate emar ----------\n",
    "print(f\"[{nowstr()}] Phase 3 — aggregate emar -> {AGG_EMAR_PATH}\")\n",
    "if AGG_EMAR_PATH.exists(): AGG_EMAR_PATH.unlink()\n",
    "\n",
    "emar_header = read_header(EMAR_PATH)\n",
    "emar_usecols_want = ['subject_id','hadm_id','charttime','medication','event_txt','pharmacy_id']\n",
    "emar_usecols = [c for c in emar_usecols_want if c in emar_header]\n",
    "emar_parse_dates = [c for c in ['charttime'] if c in emar_header]\n",
    "\n",
    "first_out = True\n",
    "total_out = 0\n",
    "for i, chunk in enumerate(pd.read_csv(EMAR_PATH, usecols=emar_usecols, parse_dates=emar_parse_dates, chunksize=SRC_CHUNKSIZE, low_memory=True), start=1):\n",
    "    t0 = time.time()\n",
    "    print(f\"[{nowstr()}] Emar chunk {i} rows={len(chunk):,}\")\n",
    "    if 'subject_id' in chunk.columns:\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    keys = list(zip(chunk['subject_id'].astype('Int64').astype(object), chunk['hadm_id'].astype('Int64').astype(object)))\n",
    "    chunk['admit_date'] = [admit_dict.get((int(s), int(h)), pd.NaT) if not (pd.isna(s) or pd.isna(h)) else pd.NaT for s,h in keys]\n",
    "    before = len(chunk)\n",
    "    chunk = chunk[chunk['admit_date'].notna()].copy()\n",
    "    dropped = before - len(chunk)\n",
    "    if dropped:\n",
    "        print(f\"  -> dropped {dropped:,} emar rows with no admit mapping\")\n",
    "    if chunk.empty:\n",
    "        del chunk; gc.collect(); continue\n",
    "    chunk['chart_date'] = pd.to_datetime(chunk['charttime'], errors='coerce').dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "    chunk['med_text'] = chunk['medication'].astype(str).str.strip().replace({'nan':None})\n",
    "    chunk['event_txt'] = chunk['event_txt'].astype(str).str.strip().replace({'nan':None})\n",
    "    chunk['admin_flag'] = chunk['event_txt'].str.contains('Administered|Given|Given by|Admin|Dose|Flushed', case=False, na=False).astype(int)\n",
    "    agg = chunk.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "        emar_events_count = ('event_txt','count'),\n",
    "        emar_admin_count  = ('admin_flag','sum'),\n",
    "        emar_last_event   = ('event_txt', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        emar_last_med     = ('med_text', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        emar_last_charttime = ('charttime','max')\n",
    "    )\n",
    "    agg['subject_id'] = agg['subject_id'].astype('Int64')\n",
    "    agg['hadm_id'] = agg['hadm_id'].astype('Int64')\n",
    "    agg['day_index'] = agg['day_index'].astype('Int64')\n",
    "    agg.to_csv(AGG_EMAR_PATH, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out = False\n",
    "    total_out += len(agg)\n",
    "    print(f\"[{nowstr()}] Emar chunk {i} -> wrote {len(agg):,} agg rows (total {total_out:,})  took {time.time()-t0:.1f}s\")\n",
    "    del chunk, agg\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"[{nowstr()}] Emar aggregation done. total agg rows (unconsolidated): {total_out}\")\n",
    "\n",
    "consolidate_agg(AGG_EMAR_PATH, AGG_EMAR_PATH,\n",
    "                key_cols=('subject_id','hadm_id','day_index'),\n",
    "                sum_cols=['emar_events_count','emar_admin_count'],\n",
    "                min_cols=[],\n",
    "                max_cols=['emar_last_charttime'],\n",
    "                pick_by_max='emar_last_charttime',\n",
    "                pick_cols=['emar_last_event','emar_last_med'])\n",
    "\n",
    "\n",
    "# -------------------- Phase 4: merge aggregated into merged_initial in chunks --------------------\n",
    "OUT_PATH = BASE_DIR / \"merged_with_medications.csv\"\n",
    "if OUT_PATH.exists(): OUT_PATH.unlink()\n",
    "\n",
    "print(f\"[{nowstr()}] Phase 4 — merge aggregated files into {OUT_PATH} in chunks (write_chunk={WRITE_CHUNK})\")\n",
    "first_write = True\n",
    "mchunk_no = 0\n",
    "for mchunk in pd.read_csv(MERGED_INITIAL_PATH, chunksize=WRITE_CHUNK, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'], low_memory=False):\n",
    "    mchunk_no += 1\n",
    "    t0 = time.time()\n",
    "    print(f\"[{nowstr()}] Merged chunk {mchunk_no}: rows={len(mchunk):,}\")\n",
    "    mchunk['subject_id'] = pd.to_numeric(mchunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    mchunk['hadm_id'] = pd.to_numeric(mchunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    mchunk['day_index'] = pd.to_numeric(mchunk['day_index'], errors='coerce').astype('Int64')\n",
    "    # build key set\n",
    "    keys = set((int(r['subject_id']), int(r['hadm_id']), int(r['day_index'])) for _, r in mchunk[['subject_id','hadm_id','day_index']].iterrows())\n",
    "    # collect matching rows (each consolidated AGG is now unique per key)\n",
    "    presc_match = collect_matching_rows(AGG_PRESC_PATH, keys) if AGG_PRESC_PATH.exists() else pd.DataFrame()\n",
    "    pharm_match = collect_matching_rows(AGG_PHARM_PATH, keys) if AGG_PHARM_PATH.exists() else pd.DataFrame()\n",
    "    emar_match  = collect_matching_rows(AGG_EMAR_PATH, keys)  if AGG_EMAR_PATH.exists() else pd.DataFrame()\n",
    "    # merge (left) - since AGG files were consolidated, no key will duplicate\n",
    "    if not presc_match.empty:\n",
    "        mchunk = mchunk.merge(presc_match, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "    if not pharm_match.empty:\n",
    "        mchunk = mchunk.merge(pharm_match, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "    if not emar_match.empty:\n",
    "        mchunk = mchunk.merge(emar_match, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "    # write\n",
    "    mchunk.to_csv(OUT_PATH, mode='w' if first_write else 'a', index=False, header=first_write)\n",
    "    first_write = False\n",
    "    print(f\"[{nowstr()}] Written merged chunk {mchunk_no} (took {time.time()-t0:.1f}s).\")\n",
    "    del mchunk, presc_match, pharm_match, emar_match\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"[{nowstr()}] All done. Output file: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8930b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial[-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24096c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f51835",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_medications = pd.read_csv(\"merged_with_medications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83532e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_medications.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a721b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_medications.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_medications[-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_medications[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6869cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredientevents = pd.read_csv(\"ingredientevents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61733d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredientevents.head(5).to_csv('test_ingredientevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputevents = pd.read_csv(\"inputevents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca084ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputevents.head(5).to_csv('test_inputevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedureevents = pd.read_csv(\"procedureevents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be22e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedureevents.head(5).to_csv('test_procedureevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_merge_inputs_procs.py\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------- CONFIG ----------\n",
    "BASE = Path(\".\")\n",
    "MERGED_INITIAL_PATH = BASE / \"admissions_expanded.csv\"   # existing admission-day base\n",
    "ING_PATH  = BASE / \"ingredientevents.csv\"\n",
    "INP_PATH  = BASE / \"inputevents.csv\"\n",
    "PROC_PATH = BASE / \"procedureevents.csv\"\n",
    "\n",
    "AGG_ING_PATH  = BASE / \"agg_ingredient_daily.csv\"\n",
    "AGG_INP_PATH  = BASE / \"agg_input_daily.csv\"\n",
    "AGG_PROC_PATH = BASE / \"agg_procedure_daily.csv\"\n",
    "\n",
    "SRC_CHUNKSIZE = 200_000   # reduce if memory pressure\n",
    "WRITE_CHUNK    = 20_000   # rows of merged_initial to process per write\n",
    "MATCH_CHUNK    = 200_000   # chunk size when scanning agg csvs for matches\n",
    "# ---------------------------\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def nowstr(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def parse_numeric(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    try:\n",
    "        if isinstance(x,(int,float,np.number)): return float(x)\n",
    "        s = str(x).strip().replace(',','')\n",
    "        if s in (\"\",\"NaN\",\"nan\",\"None\",\"none\",\"___\"): return np.nan\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def read_header(path: Path):\n",
    "    if not path.exists(): return []\n",
    "    return pd.read_csv(path, nrows=0).columns.tolist()\n",
    "\n",
    "def build_admit_map(merged_initial_path):\n",
    "    print(f\"[{nowstr()}] Build admit_map (light read)...\")\n",
    "    mi = pd.read_csv(merged_initial_path, usecols=['subject_id','hadm_id','admittime'], parse_dates=['admittime'], low_memory=False)\n",
    "    mi['subject_id'] = pd.to_numeric(mi['subject_id'], errors='coerce').astype('Int64')\n",
    "    mi['hadm_id'] = pd.to_numeric(mi['hadm_id'], errors='coerce').astype('Int64')\n",
    "    adm = mi.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "    adm['admit_date'] = pd.to_datetime(adm['admit_time']).dt.normalize()\n",
    "    d = dict(zip(zip(adm['subject_id'].astype(int), adm['hadm_id'].astype(int)), adm['admit_date']))\n",
    "    print(f\"[{nowstr()}] admit_map keys: {len(d)}\")\n",
    "    del mi, adm; gc.collect()\n",
    "    return d\n",
    "\n",
    "def key_str(s,h,d): return f\"{int(s)}|{int(h)}|{int(d)}\"\n",
    "\n",
    "def key_series_from_df(df):\n",
    "    s = pd.to_numeric(df['subject_id'], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    h = pd.to_numeric(df['hadm_id'], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    d = pd.to_numeric(df['day_index'], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    return s + '|' + h + '|' + d\n",
    "\n",
    "def _choose_agg_for_col(col_name):\n",
    "    \"\"\"\n",
    "    Heuristic for collapsing duplicates in agg files.\n",
    "    - counts/totals -> sum\n",
    "    - '*_max' or 'max' in name -> max\n",
    "    - numeric-like names containing 'amount' or 'rate' -> sum (for amounts) or max (for rate) depending on keywords\n",
    "    - otherwise -> take last non-null\n",
    "    \"\"\"\n",
    "    name = col_name.lower()\n",
    "    if name in ('subject_id','hadm_id','day_index'):\n",
    "        return 'first'\n",
    "    if 'count' in name or 'total' in name or name.endswith('_sum'):\n",
    "        return 'sum'\n",
    "    if 'max' in name or name.endswith('_max'):\n",
    "        return 'max'\n",
    "    # amounts -> sum\n",
    "    if 'amount' in name or 'total' in name:\n",
    "        return 'sum'\n",
    "    # rate or val or _num -> prefer max (for numeric measures we often want max)\n",
    "    if 'rate' in name or name.endswith('_num') or 'val' in name or 'value' in name:\n",
    "        return 'max'\n",
    "    # fallback: last non-null string\n",
    "    return lambda s: s.dropna().iloc[-1] if s.dropna().shape[0] > 0 else pd.NA\n",
    "\n",
    "def collect_matching_rows(agg_path, keys_set, usecols=None, parse_dates=None, chunksize=MATCH_CHUNK):\n",
    "    \"\"\"\n",
    "    Read agg_path in chunks and collect rows matching keys_set.\n",
    "    Then collapse duplicate keys by grouping and applying heuristics for aggregation.\n",
    "    Returns a DataFrame with unique keys.\n",
    "    \"\"\"\n",
    "    if not agg_path.exists() or not keys_set:\n",
    "        return pd.DataFrame(columns=(usecols or []))\n",
    "\n",
    "    # prepare set of string keys for fast isin\n",
    "    keys_s = set(key_str(s,h,d) for (s,h,d) in keys_set)\n",
    "\n",
    "    matches = []\n",
    "    cols_seen = None\n",
    "    for chunk in pd.read_csv(agg_path, usecols=usecols, parse_dates=parse_dates, chunksize=chunksize, low_memory=True):\n",
    "        # ensure expected key columns exist in this chunk\n",
    "        if not {'subject_id','hadm_id','day_index'}.issubset(set(chunk.columns)):\n",
    "            continue\n",
    "        # normalize types\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "        chunk['day_index'] = pd.to_numeric(chunk['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "        ks = key_series_from_df(chunk)\n",
    "        mask = ks.isin(keys_s)\n",
    "        sel = chunk.loc[mask]\n",
    "        if not sel.empty:\n",
    "            matches.append(sel)\n",
    "            if cols_seen is None:\n",
    "                cols_seen = sel.columns.tolist()\n",
    "        del chunk, ks, mask; gc.collect()\n",
    "\n",
    "    if not matches:\n",
    "        return pd.DataFrame(columns=(usecols or []))\n",
    "\n",
    "    df = pd.concat(matches, ignore_index=True)\n",
    "\n",
    "    # ensure keys are typed properly\n",
    "    df['subject_id'] = pd.to_numeric(df['subject_id'], errors='coerce').astype('Int64')\n",
    "    df['hadm_id'] = pd.to_numeric(df['hadm_id'], errors='coerce').astype('Int64')\n",
    "    df['day_index'] = pd.to_numeric(df['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Build aggregation dict using heuristics\n",
    "    agg_dict = {}\n",
    "    for col in df.columns:\n",
    "        agg_dict[col] = _choose_agg_for_col(col)\n",
    "\n",
    "    # Perform groupby aggregation to collapse duplicates\n",
    "    grouped = df.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(agg_dict)\n",
    "\n",
    "    # Post-process: ensure columns for keys are Int64 again\n",
    "    grouped['subject_id'] = pd.to_numeric(grouped['subject_id'], errors='coerce').astype('Int64')\n",
    "    grouped['hadm_id'] = pd.to_numeric(grouped['hadm_id'], errors='coerce').astype('Int64')\n",
    "    grouped['day_index'] = pd.to_numeric(grouped['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# ---------- prep ----------\n",
    "admit_dict = build_admit_map(MERGED_INITIAL_PATH)\n",
    "\n",
    "# ---------- Phase A: agg ingredientevents ----------\n",
    "print(f\"[{nowstr()}] Phase A: aggregate ingredientevents -> {AGG_ING_PATH}\")\n",
    "if AGG_ING_PATH.exists(): AGG_ING_PATH.unlink()\n",
    "hdr = read_header(ING_PATH)\n",
    "use_want = ['subject_id','hadm_id','starttime','endtime','storetime','itemid','amount','amountuom','rate','rateuom','orderid','statusdescription','originalamount','originalrate']\n",
    "usecols = [c for c in use_want if c in hdr]\n",
    "parse_dates = [c for c in ['starttime','endtime','storetime'] if c in hdr]\n",
    "first_out=True; total=0\n",
    "for i, chunk in enumerate(pd.read_csv(ING_PATH, usecols=usecols, parse_dates=parse_dates, chunksize=SRC_CHUNKSIZE, low_memory=True), start=1):\n",
    "    t0=time.time(); print(f\"[{nowstr()}] ing chunk {i} rows={len(chunk):,}\")\n",
    "    chunk['subject_id']=pd.to_numeric(chunk['subject_id'],errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id']=pd.to_numeric(chunk['hadm_id'],errors='coerce').astype('Int64')\n",
    "    keys=list(zip(chunk['subject_id'].astype('Int64').astype(object), chunk['hadm_id'].astype('Int64').astype(object)))\n",
    "    chunk['admit_date']=[admit_dict.get((int(s),int(h)), pd.NaT) if not (pd.isna(s) or pd.isna(h)) else pd.NaT for s,h in keys]\n",
    "    before=len(chunk); chunk=chunk[chunk['admit_date'].notna()].copy(); dropped=before-len(chunk)\n",
    "    if dropped: print(f\"  -> dropped {dropped:,} ing rows w/o admit\")\n",
    "    if chunk.empty: del chunk; gc.collect(); continue\n",
    "    # compute day_index by starttime (fallback storetime/endtime)\n",
    "    chunk['event_time']=pd.to_datetime(chunk.get('starttime', pd.NaT),errors='coerce').fillna(pd.to_datetime(chunk.get('storetime', pd.NaT),errors='coerce')).fillna(pd.to_datetime(chunk.get('endtime', pd.NaT),errors='coerce'))\n",
    "    chunk['chart_date']=chunk['event_time'].dt.normalize()\n",
    "    chunk['day_index']= (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int); chunk.loc[chunk['day_index']<0,'day_index']=0\n",
    "    # numeric parsing\n",
    "    chunk['amount_num']=chunk['amount'].apply(parse_numeric) if 'amount' in chunk.columns else np.nan\n",
    "    chunk['rate_num']=chunk['rate'].apply(parse_numeric) if 'rate' in chunk.columns else np.nan\n",
    "    # text\n",
    "    chunk['item_text']=chunk['itemid'].astype(str).str.strip()\n",
    "    # aggregate: sum amounts (ingredients sum often makes sense), max rate, last status/text, count\n",
    "    agg = chunk.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "        ing_events_count = ('item_text','count'),\n",
    "        ing_total_amount = ('amount_num','sum'),\n",
    "        ing_max_rate = ('rate_num','max'),\n",
    "        ing_last_itemid = ('item_text', lambda s: s.dropna().astype(str).iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        ing_last_status = ('statusdescription', lambda s: s.dropna().iloc[-1] if 'statusdescription' in chunk.columns and len(s.dropna())>0 else pd.NA)\n",
    "    )\n",
    "    agg['subject_id']=agg['subject_id'].astype('Int64'); agg['hadm_id']=agg['hadm_id'].astype('Int64'); agg['day_index']=agg['day_index'].astype('Int64')\n",
    "    agg.to_csv(AGG_ING_PATH, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out=False; total+=len(agg)\n",
    "    print(f\"[{nowstr()}] ing chunk {i} -> wrote {len(agg):,} agg rows (total {total:,}) took {time.time()-t0:.1f}s\")\n",
    "    del chunk, agg; gc.collect()\n",
    "print(f\"[{nowstr()}] ingredient aggregation done. total agg rows (raw appended): {total}\")\n",
    "\n",
    "# ---------- Phase B: agg inputevents ----------\n",
    "print(f\"[{nowstr()}] Phase B: aggregate inputevents -> {AGG_INP_PATH}\")\n",
    "if AGG_INP_PATH.exists(): AGG_INP_PATH.unlink()\n",
    "hdr = read_header(INP_PATH)\n",
    "use_want = ['subject_id','hadm_id','starttime','endtime','storetime','itemid','amount','amountuom','rate','rateuom','orderid','ordercategoryname','ordercomponenttypedescription','totalamount','totalamountuom','isopenbag','statusdescription']\n",
    "usecols = [c for c in use_want if c in hdr]\n",
    "parse_dates = [c for c in ['starttime','endtime','storetime'] if c in hdr]\n",
    "first_out=True; total=0\n",
    "for i, chunk in enumerate(pd.read_csv(INP_PATH, usecols=usecols, parse_dates=parse_dates, chunksize=SRC_CHUNKSIZE, low_memory=True), start=1):\n",
    "    t0=time.time(); print(f\"[{nowstr()}] inp chunk {i} rows={len(chunk):,}\")\n",
    "    chunk['subject_id']=pd.to_numeric(chunk['subject_id'],errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id']=pd.to_numeric(chunk['hadm_id'],errors='coerce').astype('Int64')\n",
    "    keys=list(zip(chunk['subject_id'].astype('Int64').astype(object), chunk['hadm_id'].astype('Int64').astype(object)))\n",
    "    chunk['admit_date']=[admit_dict.get((int(s),int(h)), pd.NaT) if not (pd.isna(s) or pd.isna(h)) else pd.NaT for s,h in keys]\n",
    "    before=len(chunk); chunk=chunk[chunk['admit_date'].notna()].copy(); dropped=before-len(chunk)\n",
    "    if dropped: print(f\"  -> dropped {dropped:,} input rows w/o admit\")\n",
    "    if chunk.empty: del chunk; gc.collect(); continue\n",
    "    chunk['event_time']=pd.to_datetime(chunk.get('starttime', pd.NaT),errors='coerce').fillna(pd.to_datetime(chunk.get('storetime', pd.NaT),errors='coerce')).fillna(pd.to_datetime(chunk.get('endtime', pd.NaT),errors='coerce'))\n",
    "    chunk['chart_date']=chunk['event_time'].dt.normalize()\n",
    "    chunk['day_index']=(chunk['chart_date']-chunk['admit_date']).dt.days.fillna(0).astype(int); chunk.loc[chunk['day_index']<0,'day_index']=0\n",
    "    chunk['amount_num']=chunk['amount'].apply(parse_numeric) if 'amount' in chunk.columns else np.nan\n",
    "    chunk['rate_num']=chunk['rate'].apply(parse_numeric) if 'rate' in chunk.columns else np.nan\n",
    "    chunk['totalamount_num']=chunk['totalamount'].apply(parse_numeric) if 'totalamount' in chunk.columns else np.nan\n",
    "    chunk['item_text']=chunk['itemid'].astype(str)\n",
    "    chunk['ordercat_text']=chunk['ordercategoryname'].astype(str) if 'ordercategoryname' in chunk.columns else pd.NA\n",
    "    # aggregations: sum amounts (for fluid totals), max rate, counts, last textual descriptors\n",
    "    agg = chunk.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "        input_events_count = ('item_text','count'),\n",
    "        input_total_amount = ('amount_num','sum'),\n",
    "        input_totalamount_field = ('totalamount_num','sum'),\n",
    "        input_max_rate = ('rate_num','max'),\n",
    "        input_last_ordercat = ('ordercat_text', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        input_last_status = ('statusdescription', lambda s: s.dropna().iloc[-1] if 'statusdescription' in chunk.columns and len(s.dropna())>0 else pd.NA)\n",
    "    )\n",
    "    agg['subject_id']=agg['subject_id'].astype('Int64'); agg['hadm_id']=agg['hadm_id'].astype('Int64'); agg['day_index']=agg['day_index'].astype('Int64')\n",
    "    agg.to_csv(AGG_INP_PATH, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out=False; total+=len(agg)\n",
    "    print(f\"[{nowstr()}] inp chunk {i} -> wrote {len(agg):,} agg rows (total {total:,}) took {time.time()-t0:.1f}s\")\n",
    "    del chunk, agg; gc.collect()\n",
    "print(f\"[{nowstr()}] inputevents aggregation done. total agg rows (raw appended): {total}\")\n",
    "\n",
    "# ---------- Phase C: agg procedureevents ----------\n",
    "print(f\"[{nowstr()}] Phase C: aggregate procedureevents -> {AGG_PROC_PATH}\")\n",
    "if AGG_PROC_PATH.exists(): AGG_PROC_PATH.unlink()\n",
    "hdr = read_header(PROC_PATH)\n",
    "use_want = ['subject_id','hadm_id','starttime','endtime','storetime','itemid','value','valueuom','location','locationcategory','ordercategoryname','statusdescription']\n",
    "usecols = [c for c in use_want if c in hdr]\n",
    "parse_dates = [c for c in ['starttime','endtime','storetime'] if c in hdr]\n",
    "first_out=True; total=0\n",
    "for i, chunk in enumerate(pd.read_csv(PROC_PATH, usecols=usecols, parse_dates=parse_dates, chunksize=SRC_CHUNKSIZE, low_memory=True), start=1):\n",
    "    t0=time.time(); print(f\"[{nowstr()}] proc chunk {i} rows={len(chunk):,}\")\n",
    "    chunk['subject_id']=pd.to_numeric(chunk['subject_id'],errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id']=pd.to_numeric(chunk['hadm_id'],errors='coerce').astype('Int64')\n",
    "    keys=list(zip(chunk['subject_id'].astype('Int64').astype(object), chunk['hadm_id'].astype('Int64').astype(object)))\n",
    "    chunk['admit_date']=[admit_dict.get((int(s),int(h)), pd.NaT) if not (pd.isna(s) or pd.isna(h)) else pd.NaT for s,h in keys]\n",
    "    before=len(chunk); chunk=chunk[chunk['admit_date'].notna()].copy(); dropped=before-len(chunk)\n",
    "    if dropped: print(f\"  -> dropped {dropped:,} proc rows w/o admit\")\n",
    "    if chunk.empty: del chunk; gc.collect(); continue\n",
    "    chunk['event_time']=pd.to_datetime(chunk.get('starttime', pd.NaT),errors='coerce').fillna(pd.to_datetime(chunk.get('storetime', pd.NaT),errors='coerce')).fillna(pd.to_datetime(chunk.get('endtime', pd.NaT),errors='coerce'))\n",
    "    chunk['chart_date']=chunk['event_time'].dt.normalize()\n",
    "    chunk['day_index']=(chunk['chart_date']-chunk['admit_date']).dt.days.fillna(0).astype(int); chunk.loc[chunk['day_index']<0,'day_index']=0\n",
    "    chunk['val_num']=chunk['value'].apply(parse_numeric) if 'value' in chunk.columns else np.nan\n",
    "    chunk['item_text']=chunk['itemid'].astype(str)\n",
    "    chunk['loc_text']=chunk['location'].astype(str) if 'location' in chunk.columns else pd.NA\n",
    "    agg = chunk.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "        proc_events_count = ('item_text','count'),\n",
    "        proc_max_value = ('val_num','max'),\n",
    "        proc_last_valueuom = ('valueuom', lambda s: s.dropna().iloc[-1] if 'valueuom' in chunk.columns and len(s.dropna())>0 else pd.NA),\n",
    "        proc_last_location = ('loc_text', lambda s: s.dropna().iloc[-1] if len(s.dropna())>0 else pd.NA),\n",
    "        proc_last_category = ('ordercategoryname', lambda s: s.dropna().iloc[-1] if 'ordercategoryname' in chunk.columns and len(s.dropna())>0 else pd.NA)\n",
    "    )\n",
    "    agg['subject_id']=agg['subject_id'].astype('Int64'); agg['hadm_id']=agg['hadm_id'].astype('Int64'); agg['day_index']=agg['day_index'].astype('Int64')\n",
    "    agg.to_csv(AGG_PROC_PATH, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out=False; total+=len(agg)\n",
    "    print(f\"[{nowstr()}] proc chunk {i} -> wrote {len(agg):,} agg rows (total {total:,}) took {time.time()-t0:.1f}s\")\n",
    "    del chunk, agg; gc.collect()\n",
    "print(f\"[{nowstr()}] procedure aggregation done. total agg rows (raw appended): {total}\")\n",
    "\n",
    "# ---------- Phase D: merge aggregated files into merged_initial in chunks ----------\n",
    "OUT = BASE / \"merged_with_inputs_procs.csv\"\n",
    "if OUT.exists(): OUT.unlink()\n",
    "print(f\"[{nowstr()}] Phase D: merge aggregated files into {OUT} in chunks (write_chunk={WRITE_CHUNK})\")\n",
    "first_write=True; mchunk_no=0\n",
    "for mchunk in pd.read_csv(MERGED_INITIAL_PATH, chunksize=WRITE_CHUNK, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'], low_memory=False):\n",
    "    mchunk_no+=1; t0=time.time()\n",
    "    print(f\"[{nowstr()}] merged chunk {mchunk_no} rows={len(mchunk):,}\")\n",
    "    mchunk['subject_id']=pd.to_numeric(mchunk['subject_id'],errors='coerce').astype('Int64')\n",
    "    mchunk['hadm_id']=pd.to_numeric(mchunk['hadm_id'],errors='coerce').astype('Int64')\n",
    "    mchunk['day_index']=pd.to_numeric(mchunk['day_index'],errors='coerce').astype('Int64')\n",
    "\n",
    "    # build keys set for this chunk\n",
    "    keys = set((int(r['subject_id']), int(r['hadm_id']), int(r['day_index'])) for _,r in mchunk[['subject_id','hadm_id','day_index']].iterrows())\n",
    "\n",
    "    # collect matches and collapse duplicates internally\n",
    "    ing_match = collect_matching_rows(AGG_ING_PATH, keys, usecols=None) if AGG_ING_PATH.exists() else pd.DataFrame()\n",
    "    inp_match = collect_matching_rows(AGG_INP_PATH, keys, usecols=None) if AGG_INP_PATH.exists() else pd.DataFrame()\n",
    "    proc_match = collect_matching_rows(AGG_PROC_PATH, keys, usecols=None) if AGG_PROC_PATH.exists() else pd.DataFrame()\n",
    "\n",
    "    # Now merge - since collected matches are already deduplicated per key, no Cartesian duplicates will occur\n",
    "    if not ing_match.empty:\n",
    "        mchunk = mchunk.merge(ing_match, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "    if not inp_match.empty:\n",
    "        mchunk = mchunk.merge(inp_match, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "    if not proc_match.empty:\n",
    "        mchunk = mchunk.merge(proc_match, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "    mchunk.to_csv(OUT, mode='w' if first_write else 'a', index=False, header=first_write)\n",
    "    first_write=False\n",
    "    print(f\"[{nowstr()}] Written merged chunk {mchunk_no} (took {time.time()-t0:.1f}s)\")\n",
    "    del mchunk, ing_match, inp_match, proc_match; gc.collect()\n",
    "\n",
    "print(f\"[{nowstr()}] All done. Output: {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477289bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e220580",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_inputs_procs = pd.read_csv(\"merged_with_inputs_procs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_inputs_procs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_inputs_procs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fbd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c81ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyevents = pd.read_csv(\"microbiologyevents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiologyevents.head(50).to_csv('test_microbiologyevents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "# Paths - adjust if needed\n",
    "merged_initial_path = Path(\"admissions_expanded.csv\")   # created earlier by your notebook\n",
    "micro_path = Path(\"microbiologyevents.csv\")\n",
    "out_merged_path = Path(\"merged_with_microbiologyevents.csv\")\n",
    "\n",
    "if not merged_initial_path.exists():\n",
    "    raise FileNotFoundError(f\"{merged_initial_path} not found. Run admissions expansion first.\")\n",
    "\n",
    "if not micro_path.exists():\n",
    "    raise FileNotFoundError(f\"{micro_path} not found. Put microbiologyevents.csv next to admissions.csv\")\n",
    "\n",
    "# 1) Load merged_initial (admission-day base)\n",
    "merged_initial = pd.read_csv(merged_initial_path, low_memory=False,\n",
    "                             parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'])\n",
    "# ensure day_index is integer\n",
    "merged_initial['day_index'] = merged_initial['day_index'].astype('Int64')\n",
    "\n",
    "# 2) Build admit_map (admit_date per (subject_id,hadm_id)) and index map\n",
    "admit_map = merged_initial.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "\n",
    "# merged_initial index map to assign quickly\n",
    "merged_initial_index_map = {}\n",
    "for idx, row in merged_initial[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    # ensure ints\n",
    "    try:\n",
    "        key = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "        merged_initial_index_map[key] = idx\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(\"Loaded merged_initial rows:\", len(merged_initial))\n",
    "print(\"Admit map keys:\", len(admit_dict), \"merged rows map size:\", len(merged_initial_index_map))\n",
    "\n",
    "# 3) Read microbiologyevents header to get columns\n",
    "micro_head = pd.read_csv(micro_path, nrows=0)\n",
    "micro_cols = micro_head.columns.tolist()\n",
    "print(\"microbiologyevents columns:\", micro_cols)\n",
    "\n",
    "# We'll create merged_initial columns for each original micro column (prefixed with micro_)\n",
    "prefix = \"micro_\"\n",
    "new_cols = []\n",
    "for c in micro_cols:\n",
    "    if c in ('subject_id','hadm_id'):\n",
    "        continue\n",
    "    new_c = prefix + c\n",
    "    new_cols.append(new_c)\n",
    "    if new_c not in merged_initial.columns:\n",
    "        merged_initial[new_c] = pd.Series([pd.NA] * len(merged_initial), dtype=\"object\")\n",
    "\n",
    "print(\"Added new micro columns to merged_initial (if missing). Count:\", len(new_cols))\n",
    "\n",
    "# 4) Decide which columns to treat as numeric (attempt a conservative list; we'll coerce in-chunk)\n",
    "# Common numeric-like fields in microbiologyevents: isolate_num, quantity, dilution_value, ab_itemid, test_seq, microevent_id, spec_itemid\n",
    "candidate_numeric = ['isolate_num','quantity','dilution_value','ab_itemid','test_seq','microevent_id','spec_itemid']\n",
    "\n",
    "# 5) Chunked processing\n",
    "chunksize = 200_000   # adjust if you want larger/smaller chunks\n",
    "reader = pd.read_csv(micro_path, parse_dates=['charttime','chartdate','storedate'], chunksize=chunksize, low_memory=True)\n",
    "\n",
    "total_assigned = 0\n",
    "chunk_no = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_no += 1\n",
    "    print(f\"\\n--- Processing chunk {chunk_no} (rows: {len(chunk):,}) ---\")\n",
    "    # Ensure subject_id/hadm_id are numeric ints\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce')\n",
    "    chunk = chunk.dropna(subset=['subject_id','hadm_id'])\n",
    "    chunk['subject_id'] = chunk['subject_id'].astype(int)\n",
    "    chunk['hadm_id'] = chunk['hadm_id'].astype(int)\n",
    "\n",
    "    # Resolve chart_date: prefer charttime (if available) else chartdate\n",
    "    if 'charttime' in chunk.columns and chunk['charttime'].notna().any():\n",
    "        chunk['chartref'] = pd.to_datetime(chunk['charttime'], errors='coerce')\n",
    "    else:\n",
    "        chunk['chartref'] = pd.to_datetime(chunk.get('chartdate', pd.NaT), errors='coerce')\n",
    "\n",
    "    chunk['chart_date'] = chunk['chartref'].dt.normalize()\n",
    "\n",
    "    # Attach admit_date using admit_dict\n",
    "    keys = list(zip(chunk['subject_id'].astype(int), chunk['hadm_id'].astype(int)))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    # Drop rows with no matching admission\n",
    "    before_drop = len(chunk)\n",
    "    chunk = chunk[chunk['admit_date'].notna()].copy()\n",
    "    after_drop = len(chunk)\n",
    "    if after_drop == 0:\n",
    "        print(\"no matching admissions in this chunk -> skipping\")\n",
    "        continue\n",
    "    if after_drop < before_drop:\n",
    "        print(f\"Dropped {before_drop-after_drop} rows with no admit_date\")\n",
    "\n",
    "    # Compute day_index relative to admit_date (normalized)\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    # Coerce numeric-like candidate columns to numeric where possible\n",
    "    numeric_cols_present = [c for c in candidate_numeric if c in chunk.columns]\n",
    "    coerced_numeric = []\n",
    "    for nc in numeric_cols_present:\n",
    "        coerced = pd.to_numeric(chunk[nc], errors='coerce')\n",
    "        # treat as numeric if a non-trivial fraction parse as numeric (>=1%)\n",
    "        frac_num = coerced.notna().sum() / max(1, len(chunk))\n",
    "        if frac_num > 0.01:\n",
    "            chunk[nc + \"_num\"] = coerced\n",
    "            coerced_numeric.append(nc)\n",
    "        # else leave as text (we'll treat as text)\n",
    "\n",
    "    # Group keys: day-level per admission\n",
    "    grp_keys = ['subject_id','hadm_id','day_index']\n",
    "\n",
    "    # For textual fields (and all fields as fallback) pick the last row by chartref (charttime)\n",
    "    chunk_sorted = chunk.sort_values('chartref')\n",
    "    grp_last = chunk_sorted.groupby(grp_keys, as_index=False).last()  # last row per field\n",
    "\n",
    "    # For numeric columns we created \"<col>_num\" - take max per day\n",
    "    if coerced_numeric:\n",
    "        agg_spec = {nc + \"_num\": \"max\" for nc in coerced_numeric}\n",
    "        grp_num = chunk.groupby(grp_keys, as_index=False).agg(agg_spec)\n",
    "    else:\n",
    "        grp_num = pd.DataFrame(columns=grp_keys)  # empty\n",
    "\n",
    "    # Merge last/text and numeric max results\n",
    "    if not grp_num.empty:\n",
    "        merged_grps = pd.merge(grp_last, grp_num, on=grp_keys, how='left', suffixes=('','_nummax'))\n",
    "    else:\n",
    "        merged_grps = grp_last\n",
    "\n",
    "    # Now determine final value per original column:\n",
    "    # - if original column in coerced_numeric -> use the max from \"<col>_num\" (after merging)\n",
    "    # - else use the last observed value (grp_last[col])\n",
    "    assigned = 0\n",
    "    for _, r in merged_grps.iterrows():\n",
    "        key = (int(r['subject_id']), int(r['hadm_id']), int(r['day_index']))\n",
    "        row_idx = merged_initial_index_map.get(key)\n",
    "        if row_idx is None:\n",
    "            continue\n",
    "        # assign each original column value into merged_initial under prefix 'micro_<col>'\n",
    "        for col in micro_cols:\n",
    "            if col in ('subject_id','hadm_id'):\n",
    "                continue\n",
    "            out_col = prefix + col\n",
    "            if col in coerced_numeric:\n",
    "                # numeric stored as \"<col>_num\" in merged_grps (or \"<col>_num\" in chunk)\n",
    "                numcol = col + \"_num\"\n",
    "                val = r.get(numcol, pd.NA)\n",
    "                # fallback: if no numeric, try textual column value\n",
    "                if pd.isna(val):\n",
    "                    val = r.get(col, pd.NA)\n",
    "            else:\n",
    "                val = r.get(col, pd.NA)\n",
    "            # assign as-is (preserve strings/dates). Convert numpy types to python native where needed\n",
    "            merged_initial.at[row_idx, out_col] = val\n",
    "        assigned += 1\n",
    "\n",
    "    total_assigned += assigned\n",
    "    print(f\"Chunk {chunk_no}: groups aggregated = {len(merged_grps)}, assigned rows = {assigned}, total_assigned so far = {total_assigned}\")\n",
    "\n",
    "    # cleanup\n",
    "    del chunk, chunk_sorted, grp_last, grp_num, merged_grps\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- ALL CHUNKS PROCESSED ---\")\n",
    "print(\"Total assigned day-rows updated:\", total_assigned)\n",
    "\n",
    "# 6) Save out (chunked write to avoid memory surge)\n",
    "n_rows = len(merged_initial)\n",
    "write_chunk = 20000\n",
    "first_out = True\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_initial.iloc[start:end].to_csv(out_merged_path, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved final merged file with microbiologyevents to:\", out_merged_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355356d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ef0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_microbiologyevents = pd.read_csv(\"merged_with_microbiologyevents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e899d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_microbiologyevents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_microbiologyevents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c195c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents_path = Path(\"datetimeevents.csv\")\n",
    "datetimeevents = pd.read_csv(datetimeevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83714d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents.head(100).to_csv('test_datetimeevents', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimeevents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_items_path = Path(\"d_items.csv\")\n",
    "d_items = pd.read_csv(d_items_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_items.head(10).to_csv('test_d_items', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "# فایل‌ها\n",
    "merged_initial_file = Path(\"admissions_expanded.csv\")   # یا admissions_expanded.csv طبق جریان قبلی\n",
    "ditems_file = Path(\"d_items.csv\")\n",
    "datetimeevents_file = Path(\"datetimeevents.csv\")\n",
    "\n",
    "# پارامترها\n",
    "chunksize = 500_000   # مثل کد قبلی، می‌توانی تغییر دهی\n",
    "\n",
    "# بررسی وجود فایل‌ها\n",
    "if not merged_initial_file.exists():\n",
    "    raise FileNotFoundError(f\"{merged_initial_file} not found. load or generate merged_initial first.\")\n",
    "if not ditems_file.exists():\n",
    "    raise FileNotFoundError(f\"{ditems_file} not found.\")\n",
    "if not datetimeevents_file.exists():\n",
    "    raise FileNotFoundError(f\"{datetimeevents_file} not found.\")\n",
    "\n",
    "# 1) بارگذاری merged_initial (ممکن است بزرگ باشد — اما ما برای افزودن ستون‌ها آن را در حافظه فرض می‌کنیم همانند نوت‌بوک تو)\n",
    "print(\"Loading merged_initial (this may use substantial memory)...\")\n",
    "merged_initial = pd.read_csv(merged_initial_file, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'], low_memory=False)\n",
    "n_rows = len(merged_initial)\n",
    "print(\"merged_initial rows:\", n_rows)\n",
    "\n",
    "# 2) دریافت itemidهای مربوط به datetimeevents از d_items\n",
    "print(\"Loading d_items and selecting itemids where linksto == 'datetimeevents' ...\")\n",
    "d = pd.read_csv(ditems_file, dtype=str, usecols=['itemid','linksto','label','abbreviation'])\n",
    "d['linksto'] = d['linksto'].fillna('').astype(str)\n",
    "dt_items = d.loc[d['linksto'].str.lower() == 'datetimeevents', 'itemid'].dropna().unique().tolist()\n",
    "# convert to ints (where possible)\n",
    "dt_itemids = []\n",
    "for iid in dt_items:\n",
    "    try:\n",
    "        dt_itemids.append(int(iid))\n",
    "    except:\n",
    "        pass\n",
    "dt_itemids = sorted(set(dt_itemids))\n",
    "print(f\"Found {len(dt_itemids)} datetimeevents itemids (sample):\", dt_itemids[:20])\n",
    "\n",
    "# 3) اضافه کردن ستون‌های itemid به merged_initial (نام ستون‌ها همان id به صورت رشته)\n",
    "added = 0\n",
    "for iid in dt_itemids:\n",
    "    col = str(iid)\n",
    "    if col not in merged_initial.columns:\n",
    "        merged_initial[col] = pd.Series([pd.NA] * n_rows, dtype=\"object\")\n",
    "        added += 1\n",
    "print(f\"Added {added} new columns to merged_initial for datetimeitems. Total columns now: {len(merged_initial.columns)}\")\n",
    "\n",
    "# 4) ساختن admit_map: (subject_id,hadm_id) -> admit_date (normalized)\n",
    "print(\"Building admit_date map from merged_initial ...\")\n",
    "admit_map = merged_initial.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "print(\"Admit map entries:\", len(admit_dict))\n",
    "\n",
    "# 5) ساختن index map برای merged_initial: (subject_id,hadm_id,day_index) -> row index\n",
    "print(\"Building merged_initial index map (for direct assignment) ...\")\n",
    "merged_initial_index_map = {}\n",
    "for idx, row in merged_initial[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    try:\n",
    "        key = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "        merged_initial_index_map[key] = idx\n",
    "    except Exception:\n",
    "        # skip rows with missing keys\n",
    "        continue\n",
    "print(\"Merged rows map size:\", len(merged_initial_index_map))\n",
    "\n",
    "# 6) خواندن datetimeevents چانک‌به‌چانک و نگاشت به day_index و aggregate\n",
    "usecols = ['subject_id','hadm_id','stay_id','caregiver_id','charttime','storetime','itemid','value','valueuom','warning']\n",
    "reader = pd.read_csv(datetimeevents_file, usecols=usecols, parse_dates=['charttime','storetime'], chunksize=chunksize, low_memory=True)\n",
    "\n",
    "total_assigned = 0\n",
    "chunk_no = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_no += 1\n",
    "    print(f\"\\n--- Processing datetimeevents chunk {chunk_no} (rows: {len(chunk)}) ---\")\n",
    "    # itemid numeric\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(\"no itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # فیلتر فقط itemidهای مربوط به datetimeevents (از d_items)\n",
    "    chunk = chunk[chunk['itemid'].isin(dt_itemids)]\n",
    "    if chunk.empty:\n",
    "        print(\"no relevant datetime itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # تبدیل شناسه‌ها به int برای lookup\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    # lookup admit_date\n",
    "    keys = list(zip(chunk['subject_id'].astype('Int64'), chunk['hadm_id'].astype('Int64')))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    # حذف ردیف‌هایی که admission match ندارند\n",
    "    chunk = chunk[chunk['admit_date'].notna()].copy()\n",
    "    if chunk.empty:\n",
    "        print(\"no rows with admit_date in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # محاسبه day_index (بر اساس charttime normalize)\n",
    "    chunk['chart_date'] = pd.to_datetime(chunk['charttime'], errors='coerce').dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    # تلاش برای پارس کردن مقدار value به datetime (اگر ممکن باشد)\n",
    "    chunk['value_dt'] = pd.to_datetime(chunk['value'], errors='coerce')\n",
    "\n",
    "    # اگر value_dt خالی است، می‌توانیم به عنوان fallback از charttime استفاده کنیم (اختیاری)\n",
    "    # اینجا ما ترجیح می‌دهیم مقدار value_dt را اگر موجود باشد استفاده کنیم؛ در غیر اینصورت مقدار متنی را نگه می‌داریم.\n",
    "    chunk['value_raw'] = chunk['value'].astype(str)\n",
    "\n",
    "    # گروه‌بندی: برای هر (subject_id,hadm_id,day_index,itemid) آخرین مقدار بر اساس charttime را می‌گیریم\n",
    "    grp_keys = ['subject_id','hadm_id','day_index','itemid']\n",
    "    chunk_sorted = chunk.sort_values('charttime')\n",
    "    grp_last = chunk_sorted.groupby(grp_keys, as_index=False).last()[grp_keys + ['value_dt','value_raw','charttime']]\n",
    "    # فرمت نهایی: اگر value_dt موجود است آن را به رشتهٔ ISO ذخیره کن، وگرنه value_raw\n",
    "    def make_final_val(r):\n",
    "        if pd.notna(r.get('value_dt')):\n",
    "            # تبدیل به ISO (بدون timezone)\n",
    "            return pd.to_datetime(r['value_dt']).isoformat()\n",
    "        else:\n",
    "            v = r.get('value_raw')\n",
    "            if pd.isna(v) or v in (\"nan\",\"None\",\"NoneType\",\"NA\",\"<NA>\"):\n",
    "                return pd.NA\n",
    "            return v\n",
    "\n",
    "    grp_last['final_value'] = grp_last.apply(make_final_val, axis=1)\n",
    "\n",
    "    # اکنون مقدارها را به merged_initial اختصاص می‌دهیم (همان روش قبلی: find row_idx و .at assignment)\n",
    "    assigned = 0\n",
    "    for _, r in grp_last.iterrows():\n",
    "        try:\n",
    "            key = (int(r['subject_id']), int(r['hadm_id']), int(r['day_index']))\n",
    "        except Exception:\n",
    "            continue\n",
    "        row_idx = merged_initial_index_map.get(key)\n",
    "        if row_idx is None:\n",
    "            continue\n",
    "        itemid_col = str(int(r['itemid']))\n",
    "        val = r['final_value']\n",
    "        merged_initial.at[row_idx, itemid_col] = val\n",
    "        assigned += 1\n",
    "\n",
    "    total_assigned += assigned\n",
    "    print(f\"Chunk {chunk_no}: groups aggregated = {len(grp_last)}, assigned = {assigned}, total_assigned so far = {total_assigned}\")\n",
    "\n",
    "    # پاکسازی\n",
    "    del chunk, chunk_sorted, grp_last\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- ALL datetimeevents CHUNKS PROCESSED ---\")\n",
    "print(\"Total assigned datetime cells:\", total_assigned)\n",
    "\n",
    "# 7) ذخیرهٔ خروجی نهایی (chunked write برای حافظه دوستانه)\n",
    "out_path = Path(\"merged_with_datetimeevents_filled.csv\")\n",
    "write_chunk = 20000\n",
    "first = True\n",
    "n_rows = len(merged_initial)\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_initial.iloc[start:end].to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved final to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114eeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de33dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95695c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_datetimeevents_filled = pd.read_csv(\"merged_with_datetimeevents_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_datetimeevents_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_datetimeevents_filled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename_datetimeevents_columns.py\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "# فایل‌ها\n",
    "ditems_path = Path(\"d_items.csv\")\n",
    "in_path = Path(\"merged_with_datetimeevents_filled.csv\")\n",
    "out_path = Path(\"merged_with_datetimeevents_filled_renamed.csv\")\n",
    "\n",
    "# پارامترها\n",
    "chunksize = 20000\n",
    "max_name_len = 80\n",
    "\n",
    "# بررسی وجود فایل‌ها\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(f\"{ditems_path} not found.\")\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(f\"{in_path} not found.\")\n",
    "\n",
    "# بارگذاری دیکشنری آیتم‌ها (فقط datetimeevents)\n",
    "d = pd.read_csv(ditems_path, usecols=['itemid','label','abbreviation','linksto'], dtype=str)\n",
    "d['linksto'] = d['linksto'].fillna('').astype(str)\n",
    "d_dt = d.loc[d['linksto'].str.lower() == 'datetimeevents'].copy()\n",
    "d_dt['itemid'] = d_dt['itemid'].astype(str).str.strip()\n",
    "d_dt['label'] = d_dt['label'].fillna('').astype(str).str.strip()\n",
    "d_dt['abbreviation'] = d_dt['abbreviation'].fillna('').astype(str).str.strip()\n",
    "\n",
    "# انتخاب نام نهایی (abbreviation اگر هست، وگرنه label، وگرنه fallback)\n",
    "d_dt['chosen'] = d_dt.apply(lambda r: r['abbreviation'] if r['abbreviation']!='' else (r['label'] if r['label']!='' else ''), axis=1)\n",
    "\n",
    "def sanitize_name(s):\n",
    "    if pd.isna(s) or s is None:\n",
    "        return ''\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w\\-]', '', s)   # اجازه حروف/اعداد/underscore/dash\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s[:max_name_len]\n",
    "    return s\n",
    "\n",
    "# ساخت map itemid -> chosen name (با تضمین یکتایی)\n",
    "name_map = {}\n",
    "used = set()\n",
    "for _, row in d_dt.iterrows():\n",
    "    iid = row['itemid']\n",
    "    chosen = row['chosen']\n",
    "    if chosen == '':\n",
    "        base = f\"item_{iid}\"\n",
    "    else:\n",
    "        base = sanitize_name(chosen)\n",
    "        if base == '':\n",
    "            base = f\"item_{iid}\"\n",
    "    name = base\n",
    "    if name in used:\n",
    "        name = f\"{base}__{iid}\"\n",
    "    counter = 1\n",
    "    while name in used:\n",
    "        name = f\"{base}__{iid}_{counter}\"\n",
    "        counter += 1\n",
    "    used.add(name)\n",
    "    name_map[iid] = name\n",
    "\n",
    "# آماده‌سازی header جدید\n",
    "orig_header = pd.read_csv(in_path, nrows=0).columns.tolist()\n",
    "new_header = []\n",
    "conflicts = 0\n",
    "\n",
    "for col in orig_header:\n",
    "    new_col = col\n",
    "    col_str = str(col).strip()\n",
    "    # اگر خودِ ستون دقیقا یک itemid از datetimeevents باشد، نامگذاری کنیم\n",
    "    if col_str in name_map:\n",
    "        new_col = \"datetimeevents_\" + name_map[col_str]\n",
    "    else:\n",
    "        # بعضی هدرها ممکنه به صورت عدد/float ذخیره شده باشند؛ تلاش کن به int تبدیل کنی و بررسی کنی\n",
    "        try:\n",
    "            icol = str(int(float(col_str)))\n",
    "            if icol in name_map:\n",
    "                new_col = \"datetimeevents_\" + name_map[icol]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # جلوگیری از تضاد نام‌ها در هدر جدید\n",
    "    if new_col in new_header:\n",
    "        conflicts += 1\n",
    "        new_col = f\"{new_col}__orig_{sanitize_name(col_str)}\"\n",
    "        k = 1\n",
    "        while new_col in new_header:\n",
    "            new_col = f\"{new_col}_{k}\"; k += 1\n",
    "    new_header.append(new_col)\n",
    "\n",
    "print(f\"Prepared header mapping. Total cols: {len(orig_header)}, conflicts resolved: {conflicts}\")\n",
    "sample_map = {k: name_map[k] for k in list(name_map)[:10]}\n",
    "print(\"sample itemid->name (first 10):\", sample_map)\n",
    "\n",
    "# نوشتن فایل با هدر جدید به صورت chunked\n",
    "first = True\n",
    "rows_written = 0\n",
    "for i, chunk in enumerate(pd.read_csv(in_path, chunksize=chunksize, low_memory=False)):\n",
    "    chunk.columns = new_header\n",
    "    chunk.to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    rows_written += len(chunk)\n",
    "    print(f\"Chunk {i+1}: wrote {len(chunk):,} rows (total {rows_written:,})\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", rows_written)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_datetimeevents_filled_renamed = pd.read_csv(\"merged_with_datetimeevents_filled_renamed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c963f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_datetimeevents_filled_renamed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputevents_path = Path(\"outputevents.csv\")\n",
    "outputevents = pd.read_csv(outputevents_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputevents.head(10).to_csv('test_outputevents', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputevents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_outputevents_to_merged_initial.py\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "# فایل‌ها\n",
    "merged_initial_file = Path(\"admissions_expanded.csv\")   # یا مسیر دیگری که merged_initial تو ذخیره کردی\n",
    "ditems_file = Path(\"d_items.csv\")\n",
    "outputevents_file = Path(\"outputevents.csv\")\n",
    "\n",
    "# پارامترها\n",
    "chunksize = 500_000   # مثل کدهای قبلی، قابل تغییر\n",
    "\n",
    "# بررسی وجود فایل‌ها\n",
    "if not merged_initial_file.exists():\n",
    "    raise FileNotFoundError(f\"{merged_initial_file} not found. Load or generate merged_initial first.\")\n",
    "if not ditems_file.exists():\n",
    "    raise FileNotFoundError(f\"{ditems_file} not found.\")\n",
    "if not outputevents_file.exists():\n",
    "    raise FileNotFoundError(f\"{outputevents_file} not found.\")\n",
    "\n",
    "# 1) بارگذاری merged_initial (همانند نوت‌بوک تو در حافظه نگه داشته می‌شود)\n",
    "print(\"Loading merged_initial (may use substantial memory)...\")\n",
    "merged_initial = pd.read_csv(merged_initial_file, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'], low_memory=False)\n",
    "n_rows = len(merged_initial)\n",
    "print(\"merged_initial rows:\", n_rows)\n",
    "\n",
    "# 2) گرفتن itemidهای مربوط به outputevents از d_items\n",
    "print(\"Loading d_items and selecting itemids where linksto == 'outputevents' ...\")\n",
    "d = pd.read_csv(ditems_file, dtype=str, usecols=['itemid','linksto','label','abbreviation'])\n",
    "d['linksto'] = d['linksto'].fillna('').astype(str)\n",
    "out_items = d.loc[d['linksto'].str.lower() == 'outputevents', 'itemid'].dropna().unique().tolist()\n",
    "out_itemids = []\n",
    "for iid in out_items:\n",
    "    try:\n",
    "        out_itemids.append(int(iid))\n",
    "    except:\n",
    "        pass\n",
    "out_itemids = sorted(set(out_itemids))\n",
    "print(f\"Found {len(out_itemids)} outputevents itemids (sample):\", out_itemids[:20])\n",
    "\n",
    "# 3) اضافه کردن ستون‌های itemid به merged_initial در صورت نبودن\n",
    "added = 0\n",
    "for iid in out_itemids:\n",
    "    col = str(iid)\n",
    "    if col not in merged_initial.columns:\n",
    "        merged_initial[col] = pd.Series([pd.NA] * n_rows, dtype=\"object\")\n",
    "        added += 1\n",
    "print(f\"Added {added} new columns to merged_initial for outputitems. Total columns now: {len(merged_initial.columns)}\")\n",
    "\n",
    "# 4) ساختن admit_map: (subject_id,hadm_id) -> admit_date (normalized)\n",
    "print(\"Building admit_date map from merged_initial ...\")\n",
    "admit_map = merged_initial.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "admit_map['admit_date'] = pd.to_datetime(admit_map['admit_time'], errors='coerce').dt.normalize()\n",
    "admit_map['key'] = list(zip(admit_map['subject_id'].astype('Int64'), admit_map['hadm_id'].astype('Int64')))\n",
    "admit_dict = dict(zip(admit_map['key'], admit_map['admit_date']))\n",
    "print(\"Admit map entries:\", len(admit_dict))\n",
    "\n",
    "# 5) ساختن index map برای merged_initial: (subject_id,hadm_id,day_index) -> row index\n",
    "print(\"Building merged_initial index map (for direct assignment) ...\")\n",
    "merged_initial_index_map = {}\n",
    "for idx, row in merged_initial[['subject_id','hadm_id','day_index']].iterrows():\n",
    "    try:\n",
    "        key = (int(row['subject_id']), int(row['hadm_id']), int(row['day_index']))\n",
    "        merged_initial_index_map[key] = idx\n",
    "    except Exception:\n",
    "        continue\n",
    "print(\"Merged rows map size:\", len(merged_initial_index_map))\n",
    "\n",
    "# 6) خواندن outputevents چانک‌به‌چانک و نگاشت به day_index و aggregate\n",
    "usecols = ['subject_id','hadm_id','stay_id','caregiver_id','charttime','storetime','itemid','value','valueuom']\n",
    "reader = pd.read_csv(outputevents_file, usecols=usecols, parse_dates=['charttime','storetime'], chunksize=chunksize, low_memory=True)\n",
    "\n",
    "total_assigned = 0\n",
    "chunk_no = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_no += 1\n",
    "    print(f\"\\n--- Processing outputevents chunk {chunk_no} (rows: {len(chunk)}) ---\")\n",
    "    # itemid numeric\n",
    "    chunk['itemid'] = pd.to_numeric(chunk['itemid'], errors='coerce').astype('Int64')\n",
    "    chunk = chunk[chunk['itemid'].notna()]\n",
    "    if chunk.empty:\n",
    "        print(\"no itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # صرفاً itemidهای مربوط به outputevents (بر اساس d_items)\n",
    "    chunk = chunk[chunk['itemid'].isin(out_itemids)]\n",
    "    if chunk.empty:\n",
    "        print(\"no relevant output itemids in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # تبدیل شناسه‌ها به int برای lookup\n",
    "    chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # lookup admit_date\n",
    "    keys = list(zip(chunk['subject_id'].astype('Int64'), chunk['hadm_id'].astype('Int64')))\n",
    "    chunk['admit_date'] = [admit_dict.get(k, pd.NaT) for k in keys]\n",
    "\n",
    "    # حذف ردیف‌هایی که admission match ندارند\n",
    "    chunk = chunk[chunk['admit_date'].notna()].copy()\n",
    "    if chunk.empty:\n",
    "        print(\"no rows with admit_date in this chunk\")\n",
    "        continue\n",
    "\n",
    "    # محاسبه day_index (بر اساس charttime normalize)\n",
    "    chunk['chart_date'] = pd.to_datetime(chunk['charttime'], errors='coerce').dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    # numeric parse: تلاش برای ساخت valuenum از value (ممکنه در فایل outputevents ستون valuenum نباشه)\n",
    "    chunk['numeric_val'] = pd.to_numeric(chunk['value'], errors='coerce')\n",
    "\n",
    "    # گروه‌بندی: برای هر (subject_id,hadm_id,day_index,itemid) \n",
    "    grp_keys = ['subject_id','hadm_id','day_index','itemid']\n",
    "\n",
    "    # 6a) برای مقادیر عددی: جمع روزانه\n",
    "    numeric_rows = chunk[chunk['numeric_val'].notna()].copy()\n",
    "    if not numeric_rows.empty:\n",
    "        grp_num = numeric_rows.groupby(grp_keys, as_index=False)['numeric_val'].sum().rename(columns={'numeric_val':'agg_value_num_sum'})\n",
    "    else:\n",
    "        grp_num = pd.DataFrame(columns=grp_keys + ['agg_value_num_sum'])\n",
    "\n",
    "    # 6b) برای مقادیر غیرعددی یا برای مرجع متن آخرین مقدار متن/زمان\n",
    "    chunk_sorted = chunk.sort_values('charttime')\n",
    "    grp_last = chunk_sorted.groupby(grp_keys, as_index=False).last()[grp_keys + ['value','charttime']]\n",
    "    grp_last = grp_last.rename(columns={'value':'agg_value_text','charttime':'agg_time_text'})\n",
    "\n",
    "    # 6c) ادغام نتایج عددی و متنی\n",
    "    merged_grps = pd.merge(grp_last, grp_num, on=grp_keys, how='left')\n",
    "\n",
    "    # 6d) انتخاب مقدار نهایی: اگر جمع عددی موجود است از آن استفاده کن، وگرنه متن آخر\n",
    "    def pick_final_val(row):\n",
    "        if pd.notna(row.get('agg_value_num_sum')):\n",
    "            return row['agg_value_num_sum']\n",
    "        else:\n",
    "            v = row.get('agg_value_text')\n",
    "            if pd.isna(v) or str(v).strip() in (\"nan\",\"None\",\"NoneType\",\"NA\",\"<NA>\",\"\"):\n",
    "                return pd.NA\n",
    "            return v\n",
    "\n",
    "    merged_grps['final_value'] = merged_grps.apply(pick_final_val, axis=1)\n",
    "\n",
    "    # 6e) تخصیص به merged_initial با استفاده از merged_initial_index_map و .at\n",
    "    assigned = 0\n",
    "    for _, r in merged_grps.iterrows():\n",
    "        try:\n",
    "            key = (int(r['subject_id']), int(r['hadm_id']), int(r['day_index']))\n",
    "        except Exception:\n",
    "            continue\n",
    "        row_idx = merged_initial_index_map.get(key)\n",
    "        if row_idx is None:\n",
    "            continue\n",
    "        itemid_col = str(int(r['itemid']))\n",
    "        val = r['final_value']\n",
    "        merged_initial.at[row_idx, itemid_col] = val\n",
    "        assigned += 1\n",
    "\n",
    "    total_assigned += assigned\n",
    "    print(f\"Chunk {chunk_no}: groups aggregated = {len(merged_grps)}, assigned = {assigned}, total_assigned so far = {total_assigned}\")\n",
    "\n",
    "    # پاکسازی\n",
    "    del chunk, chunk_sorted, grp_last, grp_num, merged_grps, numeric_rows\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- ALL outputevents CHUNKS PROCESSED ---\")\n",
    "print(\"Total assigned outputevents cells:\", total_assigned)\n",
    "\n",
    "# 7) ذخیرهٔ خروجی نهایی (chunked write)\n",
    "out_path = Path(\"merged_with_outputevents_filled.csv\")\n",
    "write_chunk = 20000\n",
    "first = True\n",
    "n_rows = len(merged_initial)\n",
    "for start in range(0, n_rows, write_chunk):\n",
    "    end = min(start + write_chunk, n_rows)\n",
    "    merged_initial.iloc[start:end].to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    print(f\"Saved rows {start}-{end-1}\")\n",
    "print(\"Saved final to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename_outputevents_columns.py\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "# فایل‌ها\n",
    "ditems_path = Path(\"d_items.csv\")\n",
    "in_path = Path(\"merged_with_outputevents_filled.csv\")\n",
    "out_path = Path(\"merged_with_outputevents_filled_renamed.csv\")\n",
    "\n",
    "# پارامترها\n",
    "chunksize = 20000\n",
    "max_name_len = 80\n",
    "\n",
    "# بررسی وجود فایل‌ها\n",
    "if not ditems_path.exists():\n",
    "    raise FileNotFoundError(f\"{ditems_path} not found.\")\n",
    "if not in_path.exists():\n",
    "    raise FileNotFoundError(f\"{in_path} not found.\")\n",
    "\n",
    "# بارگذاری دیکشنری آیتم‌ها (فقط outputevents)\n",
    "d = pd.read_csv(ditems_path, usecols=['itemid','label','abbreviation','linksto'], dtype=str)\n",
    "d['linksto'] = d['linksto'].fillna('').astype(str)\n",
    "d_out = d.loc[d['linksto'].str.lower() == 'outputevents'].copy()\n",
    "d_out['itemid'] = d_out['itemid'].astype(str).str.strip()\n",
    "d_out['label'] = d_out['label'].fillna('').astype(str).str.strip()\n",
    "d_out['abbreviation'] = d_out['abbreviation'].fillna('').astype(str).str.strip()\n",
    "\n",
    "# انتخاب نام نهایی (abbreviation اگر هست، وگرنه label، وگرنه fallback)\n",
    "d_out['chosen'] = d_out.apply(lambda r: r['abbreviation'] if r['abbreviation']!='' else (r['label'] if r['label']!='' else ''), axis=1)\n",
    "\n",
    "def sanitize_name(s):\n",
    "    if pd.isna(s) or s is None:\n",
    "        return ''\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^\\w\\-]', '', s)\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    s = s[:max_name_len]\n",
    "    return s\n",
    "\n",
    "# ساخت map itemid -> chosen name (با تضمین یکتایی)\n",
    "name_map = {}\n",
    "used = set()\n",
    "for _, row in d_out.iterrows():\n",
    "    iid = row['itemid']\n",
    "    chosen = row['chosen']\n",
    "    if chosen == '':\n",
    "        base = f\"item_{iid}\"\n",
    "    else:\n",
    "        base = sanitize_name(chosen)\n",
    "        if base == '':\n",
    "            base = f\"item_{iid}\"\n",
    "    name = base\n",
    "    if name in used:\n",
    "        name = f\"{base}__{iid}\"\n",
    "    counter = 1\n",
    "    while name in used:\n",
    "        name = f\"{base}__{iid}_{counter}\"\n",
    "        counter += 1\n",
    "    used.add(name)\n",
    "    name_map[iid] = name\n",
    "\n",
    "# آماده‌سازی header جدید\n",
    "orig_header = pd.read_csv(in_path, nrows=0).columns.tolist()\n",
    "new_header = []\n",
    "conflicts = 0\n",
    "\n",
    "for col in orig_header:\n",
    "    new_col = col\n",
    "    col_str = str(col).strip()\n",
    "    if col_str in name_map:\n",
    "        new_col = \"outputevents_\" + name_map[col_str]\n",
    "    else:\n",
    "        try:\n",
    "            icol = str(int(float(col_str)))\n",
    "            if icol in name_map:\n",
    "                new_col = \"outputevents_\" + name_map[icol]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if new_col in new_header:\n",
    "        conflicts += 1\n",
    "        new_col = f\"{new_col}__orig_{sanitize_name(col_str)}\"\n",
    "        k = 1\n",
    "        while new_col in new_header:\n",
    "            new_col = f\"{new_col}_{k}\"; k += 1\n",
    "    new_header.append(new_col)\n",
    "\n",
    "print(f\"Prepared header mapping. Total cols: {len(orig_header)}, conflicts resolved: {conflicts}\")\n",
    "sample_map = {k: name_map[k] for k in list(name_map)[:10]}\n",
    "print(\"sample itemid->name (first 10):\", sample_map)\n",
    "\n",
    "# نوشتن فایل با هدر جدید به صورت chunked\n",
    "first = True\n",
    "rows_written = 0\n",
    "for i, chunk in enumerate(pd.read_csv(in_path, chunksize=chunksize, low_memory=False)):\n",
    "    chunk.columns = new_header\n",
    "    chunk.to_csv(out_path, mode='w' if first else 'a', index=False, header=first)\n",
    "    first = False\n",
    "    rows_written += len(chunk)\n",
    "    print(f\"Chunk {i+1}: wrote {len(chunk):,} rows (total {rows_written:,})\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Done. Output saved to:\", out_path)\n",
    "print(\"Rows written:\", rows_written)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_outputevents_filled_renamed = pd.read_csv(\"merged_with_outputevents_filled_renamed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_outputevents_filled_renamed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4584ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_outputevents_filled_renamed.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "poe = pd.read_csv(\"poe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac70671",
   "metadata": {},
   "outputs": [],
   "source": [
    "poe.head(10).to_csv('test_poe', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e255962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_name(s):\n",
    "    # make small safe filename-like token\n",
    "    s = re.sub(r'[^0-9A-Za-z]+', '_', s)\n",
    "    return s.strip('_')[:40] or 'file'\n",
    "\n",
    "def side_by_side_merge(base_path, other_paths, out_path, chunksize=5000,\n",
    "                       base_cols_keep=None, prefix_conflicts=True, verbose=True):\n",
    "    base_path = Path(base_path)\n",
    "    other_paths = [Path(p) for p in other_paths]\n",
    "    out_path = Path(out_path)\n",
    "\n",
    "    # read base header\n",
    "    base_header = pd.read_csv(base_path, nrows=0).columns.tolist()\n",
    "    if base_cols_keep is None:\n",
    "        base_cols_keep = base_header  # keep them as canonical\n",
    "\n",
    "    base_cols_keep = list(base_cols_keep)\n",
    "\n",
    "    # Pre-open readers for all files\n",
    "    base_reader = pd.read_csv(base_path, chunksize=chunksize, dtype=str, low_memory=True)\n",
    "    other_readers = {}\n",
    "    other_headers = {}\n",
    "    for p in other_paths:\n",
    "        other_readers[p] = pd.read_csv(p, chunksize=chunksize, dtype=str, low_memory=True)\n",
    "        other_headers[p] = pd.read_csv(p, nrows=0).columns.tolist()\n",
    "\n",
    "    first_out = True\n",
    "    chunk_i = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            chunk_i += 1\n",
    "            base_chunk = next(base_reader)  # may raise StopIteration -> finished\n",
    "            # keep only canonical base columns (in case file has extra)\n",
    "            base_chunk = base_chunk.loc[:, [c for c in base_cols_keep if c in base_chunk.columns]]\n",
    "\n",
    "            out_chunk = base_chunk.copy()\n",
    "\n",
    "            # drop typical unnamed index columns if present\n",
    "            drop_unnamed = [c for c in out_chunk.columns if str(c).startswith('Unnamed:') or str(c).strip()=='' ]\n",
    "            if drop_unnamed:\n",
    "                out_chunk = out_chunk.drop(columns=drop_unnamed, errors='ignore')\n",
    "\n",
    "            for p, reader in other_readers.items():\n",
    "                try:\n",
    "                    other_chunk = next(reader)\n",
    "                except StopIteration:\n",
    "                    raise RuntimeError(f\"File {p} ended earlier than base file at chunk {chunk_i}. Row counts differ!\")\n",
    "\n",
    "                # drop unnamed index columns\n",
    "                other_chunk = other_chunk.loc[:, [c for c in other_chunk.columns if not (str(c).startswith('Unnamed:') or str(c).strip()=='')]]\n",
    "\n",
    "                # remove base columns duplicated in this other file\n",
    "                overlap = [c for c in base_cols_keep if c in other_chunk.columns]\n",
    "                if overlap:\n",
    "                    other_chunk = other_chunk.drop(columns=overlap, errors='ignore')\n",
    "\n",
    "                # If any remaining columns collide with out_chunk columns, rename them\n",
    "                collisions = [c for c in other_chunk.columns if c in out_chunk.columns]\n",
    "                if collisions:\n",
    "                    safe = safe_name(p.name)\n",
    "                    new_names = {}\n",
    "                    for c in collisions:\n",
    "                        if prefix_conflicts:\n",
    "                            new_names[c] = f\"{c}__from_{safe}\"\n",
    "                        else:\n",
    "                            # fallback: append file suffix to make unique\n",
    "                            new_names[c] = f\"{c}__{safe}\"\n",
    "                    other_chunk = other_chunk.rename(columns=new_names)\n",
    "\n",
    "                # As a final safety, if other_chunk has more/less rows than base_chunk, check lengths\n",
    "                if len(other_chunk) != len(out_chunk):\n",
    "                    # If lengths mismatch within chunk, that's fatal for exact alignment\n",
    "                    raise RuntimeError(\n",
    "                        f\"Row-count mismatch in chunk {chunk_i} for file {p.name}: \"\n",
    "                        f\"base_chunk rows={len(out_chunk)}, other_chunk rows={len(other_chunk)}. \"\n",
    "                        \"Ensure files are aligned and have same row order/count.\"\n",
    "                    )\n",
    "\n",
    "                # concatenate horizontally (preserve column order: base then others incrementally)\n",
    "                out_chunk = pd.concat([out_chunk.reset_index(drop=True), other_chunk.reset_index(drop=True)], axis=1)\n",
    "\n",
    "            # write out_chunk to CSV (append after first)\n",
    "            if first_out:\n",
    "                out_chunk.to_csv(out_path, index=False, mode='w', header=True)\n",
    "                first_out = False\n",
    "            else:\n",
    "                out_chunk.to_csv(out_path, index=False, mode='a', header=False)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Chunk {chunk_i}: wrote {len(out_chunk)} rows, total cols now: {len(out_chunk.columns)}\")\n",
    "\n",
    "    except StopIteration:\n",
    "        # base finished normally; ensure all other_readers are also finished\n",
    "        pass\n",
    "\n",
    "    # verify other readers exhausted\n",
    "    for p, reader in other_readers.items():\n",
    "        try:\n",
    "            next(reader)\n",
    "            raise RuntimeError(f\"File {p} has MORE rows than base file — row counts differ.\")\n",
    "        except StopIteration:\n",
    "            # good, finished\n",
    "            if verbose:\n",
    "                print(f\"Verified {p.name} finished (same length as base).\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Merge complete. Output saved to:\", out_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ----- CONFIGURE THESE -----\n",
    "    base = \"admissions_expanded.csv\"   # canonical base\n",
    "    others = [\n",
    "        \"merged_with_icu.csv\",\n",
    "        \"merged_with_vanco.csv\",\n",
    "        \"merged_with_chartevents_filled_renamed.csv\", # تا این چک شد \n",
    "        \"merged_with_diagnoses.csv\",\n",
    "        \"merged_with_procedures.csv\",\n",
    "        \"merged_with_drg.csv\",\n",
    "        \"merged_with_transfers_services.csv\",\n",
    "        \"merged_with_medications.csv\",\n",
    "        \"merged_with_inputs_procs.csv\",\n",
    "        \"merged_with_microbiologyevents.csv\",\n",
    "        \"merged_with_datetimeevents_filled_renamed.csv\",\n",
    "        \"merged_with_outputevents_filled_renamed.csv\",\n",
    "        \n",
    "        # add other merged_*.csv files here in the order you want columns appended\n",
    "    ]\n",
    "    out = \"final_merged_admission_day.csv\"\n",
    "    CHUNK = 5000   # tune down/up depending on memory\n",
    "    PREF_CONFLICT = True\n",
    "    # --------------------------\n",
    "\n",
    "    # you can also call side_by_side_merge() from another script and pass different args\n",
    "    side_by_side_merge(base, others, out, chunksize=CHUNK, prefix_conflicts=PREF_CONFLICT, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031459f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_admission_day = pd.read_csv(\"final_merged_admission_day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_admission_day.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58805448",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61af8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "poe = pd.read_csv(\"poe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa17d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "poe.head(10).to_csv('test_poe', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "BASE = Path(\".\")\n",
    "MERGED_INITIAL_PATH = BASE / \"admissions_expanded.csv\"\n",
    "POE_PATH = BASE / \"poe.csv\"\n",
    "\n",
    "AGG_POE_PATH = BASE / \"agg_poe_daily.csv\"\n",
    "OUT_MERGED_POE = BASE / \"merged_with_poe.csv\"\n",
    "\n",
    "SRC_CHUNKSIZE = 200_000   # read size for poe.csv\n",
    "WRITE_CHUNK = 20_000      # rows of merged_initial processed per write\n",
    "MATCH_CHUNK = 200_000     # chunk size when scanning agg csvs for matches\n",
    "# ----------------------------------------\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def nowstr(): return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def parse_numeric(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    try:\n",
    "        if isinstance(x,(int,float,np.number)): return float(x)\n",
    "        s = str(x).strip().replace(',','')\n",
    "        if s in (\"\",\"NaN\",\"nan\",\"None\",\"none\",\"___\"): return np.nan\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def read_header(path: Path):\n",
    "    if not path.exists(): return []\n",
    "    return pd.read_csv(path, nrows=0).columns.tolist()\n",
    "\n",
    "def build_admit_map(merged_initial_path):\n",
    "    print(f\"[{nowstr()}] Build admit_map (light read)...\")\n",
    "    mi = pd.read_csv(merged_initial_path, usecols=['subject_id','hadm_id','admittime'], parse_dates=['admittime'], low_memory=False)\n",
    "    mi['subject_id'] = pd.to_numeric(mi['subject_id'], errors='coerce').astype('Int64')\n",
    "    mi['hadm_id'] = pd.to_numeric(mi['hadm_id'], errors='coerce').astype('Int64')\n",
    "    adm = mi.groupby(['subject_id','hadm_id'], dropna=False)['admittime'].first().reset_index().rename(columns={'admittime':'admit_time'})\n",
    "    adm['admit_date'] = pd.to_datetime(adm['admit_time']).dt.normalize()\n",
    "    d = dict(zip(zip(adm['subject_id'].astype(int), adm['hadm_id'].astype(int)), adm['admit_date']))\n",
    "    print(f\"[{nowstr()}] admit_map keys: {len(d)}\")\n",
    "    del mi, adm; gc.collect()\n",
    "    return d\n",
    "\n",
    "def key_str(s,h,d): return f\"{int(s)}|{int(h)}|{int(d)}\"\n",
    "def key_series_from_df(df):\n",
    "    s = pd.to_numeric(df['subject_id'], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    h = pd.to_numeric(df['hadm_id'], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    d = pd.to_numeric(df['day_index'], errors='coerce').fillna(-1).astype(int).astype(str)\n",
    "    return s + '|' + h + '|' + d\n",
    "\n",
    "def _choose_agg_for_col(col_name):\n",
    "    name = col_name.lower()\n",
    "    if name in ('subject_id','hadm_id','day_index'):\n",
    "        return 'first'\n",
    "    if 'count' in name or 'total' in name or name.endswith('_sum'):\n",
    "        return 'sum'\n",
    "    if 'max' in name or name.endswith('_max'):\n",
    "        return 'max'\n",
    "    if 'amount' in name:\n",
    "        return 'sum'\n",
    "    if 'rate' in name or name.endswith('_num') or 'val' in name or 'value' in name or 'ordertime' in name:\n",
    "        # for ordertime we want min/max, but groupby agg requires consistent functions:\n",
    "        # ordetime we'll handle separately; here default to 'max' for numeric-like\n",
    "        return 'max'\n",
    "    return lambda s: s.dropna().iloc[-1] if s.dropna().shape[0] > 0 else pd.NA\n",
    "\n",
    "def collect_matching_rows(agg_path, keys_set, usecols=None, parse_dates=None, chunksize=MATCH_CHUNK):\n",
    "    \"\"\"\n",
    "    Read agg_path in chunks and collect rows matching keys_set.\n",
    "    Then collapse duplicate keys via heuristics and return unique-key dataframe.\n",
    "    \"\"\"\n",
    "    if not agg_path.exists() or not keys_set:\n",
    "        return pd.DataFrame(columns=(usecols or []))\n",
    "\n",
    "    keys_s = set(key_str(s,h,d) for (s,h,d) in keys_set)\n",
    "    matches = []\n",
    "    cols_seen = None\n",
    "    for chunk in pd.read_csv(agg_path, usecols=usecols, parse_dates=parse_dates, chunksize=chunksize, low_memory=True):\n",
    "        if not {'subject_id','hadm_id','day_index'}.issubset(set(chunk.columns)):\n",
    "            continue\n",
    "        chunk['subject_id'] = pd.to_numeric(chunk['subject_id'], errors='coerce').astype('Int64')\n",
    "        chunk['hadm_id'] = pd.to_numeric(chunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "        chunk['day_index'] = pd.to_numeric(chunk['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "        ks = key_series_from_df(chunk)\n",
    "        mask = ks.isin(keys_s)\n",
    "        sel = chunk.loc[mask]\n",
    "        if not sel.empty:\n",
    "            matches.append(sel)\n",
    "            if cols_seen is None:\n",
    "                cols_seen = sel.columns.tolist()\n",
    "        del chunk, ks, mask; gc.collect()\n",
    "\n",
    "    if not matches:\n",
    "        return pd.DataFrame(columns=(usecols or []))\n",
    "\n",
    "    df = pd.concat(matches, ignore_index=True)\n",
    "    # ensure types\n",
    "    df['subject_id'] = pd.to_numeric(df['subject_id'], errors='coerce').astype('Int64')\n",
    "    df['hadm_id'] = pd.to_numeric(df['hadm_id'], errors='coerce').astype('Int64')\n",
    "    df['day_index'] = pd.to_numeric(df['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # choose agg per column\n",
    "    agg_dict = {}\n",
    "    for col in df.columns:\n",
    "        agg_dict[col] = _choose_agg_for_col(col)\n",
    "\n",
    "    # group and aggregate\n",
    "    grouped = df.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(agg_dict)\n",
    "\n",
    "    # post-process ordertime columns if present: compute min/max from original matches if needed\n",
    "    # (In our AGG_POE we included first/last ordertime already; if not, this block can be extended.)\n",
    "\n",
    "    grouped['subject_id'] = pd.to_numeric(grouped['subject_id'], errors='coerce').astype('Int64')\n",
    "    grouped['hadm_id'] = pd.to_numeric(grouped['hadm_id'], errors='coerce').astype('Int64')\n",
    "    grouped['day_index'] = pd.to_numeric(grouped['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# ---------- prepare admit_map ----------\n",
    "admit_dict = build_admit_map(MERGED_INITIAL_PATH)\n",
    "\n",
    "# ---------- Phase A: aggregate poe.csv -> AGG_POE_PATH ----------\n",
    "print(f\"[{nowstr()}] Phase A: aggregate poe -> {AGG_POE_PATH}\")\n",
    "if AGG_POE_PATH.exists(): AGG_POE_PATH.unlink()\n",
    "hdr = read_header(POE_PATH)\n",
    "# choose columns we expect; safe to include extras if present\n",
    "use_want = ['poe_id','poe_seq','subject_id','hadm_id','ordertime','order_type','order_subtype','transaction_type','discontinue_of_poe_id','discontinued_by_poe_id','order_provider_id','order_status']\n",
    "usecols = [c for c in use_want if c in hdr]\n",
    "parse_dates = ['ordertime'] if 'ordertime' in usecols else []\n",
    "\n",
    "first_out=True; total=0\n",
    "for i, chunk in enumerate(pd.read_csv(POE_PATH, usecols=usecols, parse_dates=parse_dates, chunksize=SRC_CHUNKSIZE, low_memory=True), start=1):\n",
    "    t0=time.time(); print(f\"[{nowstr()}] poe chunk {i} rows={len(chunk):,}\")\n",
    "    # normalize ids\n",
    "    if 'subject_id' not in chunk.columns or 'hadm_id' not in chunk.columns:\n",
    "        print(\"ERROR: poe missing subject_id or hadm_id columns. Aborting.\")\n",
    "        raise SystemExit(1)\n",
    "    chunk['subject_id']=pd.to_numeric(chunk['subject_id'],errors='coerce').astype('Int64')\n",
    "    chunk['hadm_id']=pd.to_numeric(chunk['hadm_id'],errors='coerce').astype('Int64')\n",
    "\n",
    "    # map admit_date\n",
    "    keys = list(zip(chunk['subject_id'].astype('Int64').astype(object), chunk['hadm_id'].astype('Int64').astype(object)))\n",
    "    chunk['admit_date'] = [admit_dict.get((int(s), int(h)), pd.NaT) if not (pd.isna(s) or pd.isna(h)) else pd.NaT for s,h in keys]\n",
    "\n",
    "    before = len(chunk)\n",
    "    chunk = chunk[chunk['admit_date'].notna()].copy()\n",
    "    dropped = before - len(chunk)\n",
    "    if dropped: print(f\"  -> dropped {dropped:,} poe rows w/o admit\")\n",
    "\n",
    "    if chunk.empty:\n",
    "        del chunk; gc.collect(); continue\n",
    "\n",
    "    # event_time and day_index: use ordertime\n",
    "    chunk['event_time'] = pd.to_datetime(chunk.get('ordertime', pd.NaT), errors='coerce')\n",
    "    chunk['chart_date'] = chunk['event_time'].dt.normalize()\n",
    "    chunk['day_index'] = (chunk['chart_date'] - chunk['admit_date']).dt.days.fillna(0).astype(int)\n",
    "    chunk.loc[chunk['day_index'] < 0, 'day_index'] = 0\n",
    "\n",
    "    # ensure provider and status columns exist\n",
    "    if 'order_provider_id' not in chunk.columns:\n",
    "        chunk['order_provider_id'] = pd.NA\n",
    "    if 'order_status' not in chunk.columns:\n",
    "        chunk['order_status'] = pd.NA\n",
    "    if 'order_type' not in chunk.columns:\n",
    "        chunk['order_type'] = pd.NA\n",
    "    if 'order_subtype' not in chunk.columns:\n",
    "        chunk['order_subtype'] = pd.NA\n",
    "    if 'transaction_type' not in chunk.columns:\n",
    "        chunk['transaction_type'] = pd.NA\n",
    "\n",
    "    # string/text normalization\n",
    "    chunk['order_provider_id'] = chunk['order_provider_id'].astype(str).replace('nan','').replace('None','').replace('NoneType','').fillna(pd.NA)\n",
    "    chunk['order_status'] = chunk['order_status'].astype(str).replace('nan','').replace('None','').fillna(pd.NA)\n",
    "    chunk['order_type'] = chunk['order_type'].astype(str).replace('nan','').replace('None','').fillna(pd.NA)\n",
    "    chunk['order_subtype'] = chunk['order_subtype'].astype(str).replace('nan','').replace('None','').fillna(pd.NA)\n",
    "    chunk['transaction_type'] = chunk['transaction_type'].astype(str).replace('nan','').replace('None','').fillna(pd.NA)\n",
    "\n",
    "    # Aggregations per chunk (per day)\n",
    "    agg = chunk.groupby(['subject_id','hadm_id','day_index'], as_index=False).agg(\n",
    "        poe_events_count = ('poe_id','count'),\n",
    "        poe_first_ordertime = ('event_time','min'),\n",
    "        poe_last_ordertime = ('event_time','max'),\n",
    "        poe_first_order_provider = ('order_provider_id', lambda s: s.dropna().iloc[0] if s.dropna().shape[0]>0 else pd.NA),\n",
    "        poe_last_order_provider = ('order_provider_id', lambda s: s.dropna().iloc[-1] if s.dropna().shape[0]>0 else pd.NA),\n",
    "        poe_last_order_type = ('order_type', lambda s: s.dropna().astype(str).iloc[-1] if s.dropna().shape[0]>0 else pd.NA),\n",
    "        poe_last_order_subtype = ('order_subtype', lambda s: s.dropna().astype(str).iloc[-1] if s.dropna().shape[0]>0 else pd.NA),\n",
    "        poe_last_transaction_type = ('transaction_type', lambda s: s.dropna().astype(str).iloc[-1] if s.dropna().shape[0]>0 else pd.NA),\n",
    "        poe_last_order_status = ('order_status', lambda s: s.dropna().astype(str).iloc[-1] if s.dropna().shape[0]>0 else pd.NA)\n",
    "    )\n",
    "\n",
    "    # ensure key dtypes\n",
    "    agg['subject_id']=agg['subject_id'].astype('Int64')\n",
    "    agg['hadm_id']=agg['hadm_id'].astype('Int64')\n",
    "    agg['day_index']=agg['day_index'].astype('Int64')\n",
    "\n",
    "    # write chunked aggregated rows (may produce multiple rows per key across chunks; will collapse on merge)\n",
    "    agg.to_csv(AGG_POE_PATH, mode='w' if first_out else 'a', index=False, header=first_out)\n",
    "    first_out=False\n",
    "    total += len(agg)\n",
    "    print(f\"[{nowstr()}] poe chunk {i} -> wrote {len(agg):,} agg rows (total appended {total:,}) took {time.time()-t0:.1f}s\")\n",
    "    del chunk, agg; gc.collect()\n",
    "\n",
    "print(f\"[{nowstr()}] POE aggregation done. raw agg rows appended: {total}\")\n",
    "\n",
    "# ---------- Phase B: merge aggregated poe into merged_initial in chunks ----------\n",
    "print(f\"[{nowstr()}] Phase B: merge aggregated poe into {OUT_MERGED_POE} in chunks (write_chunk={WRITE_CHUNK})\")\n",
    "if OUT_MERGED_POE.exists(): OUT_MERGED_POE.unlink()\n",
    "first_write = True\n",
    "mchunk_no = 0\n",
    "\n",
    "for mchunk in pd.read_csv(MERGED_INITIAL_PATH, chunksize=WRITE_CHUNK, parse_dates=['admittime','dischtime','deathtime','edregtime','edouttime'], low_memory=False):\n",
    "    mchunk_no += 1\n",
    "    t0 = time.time()\n",
    "    print(f\"[{nowstr()}] merged chunk {mchunk_no} rows={len(mchunk):,}\")\n",
    "    mchunk['subject_id'] = pd.to_numeric(mchunk['subject_id'], errors='coerce').astype('Int64')\n",
    "    mchunk['hadm_id'] = pd.to_numeric(mchunk['hadm_id'], errors='coerce').astype('Int64')\n",
    "    mchunk['day_index'] = pd.to_numeric(mchunk['day_index'], errors='coerce').astype('Int64')\n",
    "\n",
    "    keys = set((int(r['subject_id']), int(r['hadm_id']), int(r['day_index'])) for _,r in mchunk[['subject_id','hadm_id','day_index']].iterrows())\n",
    "\n",
    "    # collect matching rows from AGG_POE_PATH and collapse duplicates per key\n",
    "    poe_match = collect_matching_rows(AGG_POE_PATH, keys, usecols=None, parse_dates=['poe_first_ordertime','poe_last_ordertime']) if AGG_POE_PATH.exists() else pd.DataFrame()\n",
    "\n",
    "    if not poe_match.empty:\n",
    "        # note: parse_dates argument above may or may not have effect since agg csv stores ISO timestamps;\n",
    "        # ensure datetime dtype for ordertime cols\n",
    "        if 'poe_first_ordertime' in poe_match.columns:\n",
    "            poe_match['poe_first_ordertime'] = pd.to_datetime(poe_match['poe_first_ordertime'], errors='coerce')\n",
    "        if 'poe_last_ordertime' in poe_match.columns:\n",
    "            poe_match['poe_last_ordertime'] = pd.to_datetime(poe_match['poe_last_ordertime'], errors='coerce')\n",
    "\n",
    "        mchunk = mchunk.merge(poe_match, on=['subject_id','hadm_id','day_index'], how='left')\n",
    "\n",
    "    # write chunk\n",
    "    mchunk.to_csv(OUT_MERGED_POE, mode='w' if first_write else 'a', index=False, header=first_write)\n",
    "    first_write = False\n",
    "    print(f\"[{nowstr()}] Written merged chunk {mchunk_no} (took {time.time()-t0:.1f}s)\")\n",
    "    del mchunk, poe_match; gc.collect()\n",
    "\n",
    "print(f\"[{nowstr()}] Done. Output: {OUT_MERGED_POE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial = pd.read_csv(\"admissions_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_poe = pd.read_csv(\"merged_with_poe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_poe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_poe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c956c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_poe.head(5).to_csv('merged_with_poe_test', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
